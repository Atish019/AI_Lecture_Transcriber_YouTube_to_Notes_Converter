{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube-transcript-api in c:\\anaconda\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\anaconda\\lib\\site-packages (from youtube-transcript-api) (0.7.1)\n",
      "Requirement already satisfied: requests in c:\\anaconda\\lib\\site-packages (from youtube-transcript-api) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\lib\\site-packages (from requests->youtube-transcript-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\lib\\site-packages (from requests->youtube-transcript-api) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\lib\\site-packages (from requests->youtube-transcript-api) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\lib\\site-packages (from requests->youtube-transcript-api) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'fetch', 'list']\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "print(dir(YouTubeTranscriptApi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_video_url = \"https://www.youtube.com/watch?v=341Rb8fJxY0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'341Rb8fJxY0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id = youtube_video_url.split(\"=\")[1]\n",
    "video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/341Rb8fJxY0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x1d817bd1d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = YouTubeTranscriptApi().fetch(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FetchedTranscript(snippets=[FetchedTranscriptSnippet(text='yeah hi and welcome back everyone to the', start=1.48, duration=4.919), FetchedTranscriptSnippet(text='supplementary coding Along video series', start=3.68, duration=4.76), FetchedTranscriptSnippet(text='for the build a large language model', start=6.399, duration=4.24), FetchedTranscriptSnippet(text='from scratchbook here we are now going', start=8.44, duration=4.92), FetchedTranscriptSnippet(text='to talk about chapter 2 working with', start=10.639, duration=4.801), FetchedTranscriptSnippet(text='Text data which means essentially that', start=13.36, duration=4.56), FetchedTranscriptSnippet(text='we are going to prepare the data that we', start=15.44, duration=5.56), FetchedTranscriptSnippet(text='are going to use to train the llm yeah', start=17.92, duration=6.279), FetchedTranscriptSnippet(text='just to um go to the chapter itself', start=21.0, duration=6.039), FetchedTranscriptSnippet(text='chapter 2 you can see there are lots of', start=24.199, duration=5.4), FetchedTranscriptSnippet(text='sections starting at the top the the', start=27.039, duration=5.2), FetchedTranscriptSnippet(text='goal really here is to prepare the data', start=29.599, duration=4.96), FetchedTranscriptSnippet(text='set for the llm so there is actually a', start=32.239, duration=4.16), FetchedTranscriptSnippet(text='nice figure here that shows you this', start=34.559, duration=4.761), FetchedTranscriptSnippet(text='step in the grand scheme of the whole', start=36.399, duration=6.16), FetchedTranscriptSnippet(text='llm building process so here building an', start=39.32, duration=6.48), FetchedTranscriptSnippet(text='llm means setting up the data set coding', start=42.559, duration=5.52), FetchedTranscriptSnippet(text='the attention mechanism coding the llm', start=45.8, duration=4.72), FetchedTranscriptSnippet(text='architecture and so forth this is really', start=48.079, duration=4.8), FetchedTranscriptSnippet(text='the first stage of you know training the', start=50.52, duration=4.8), FetchedTranscriptSnippet(text='llm and then the pre-training comes in', start=52.879, duration=4.721), FetchedTranscriptSnippet(text='stage two and then later on in stage', start=55.32, duration=4.68), FetchedTranscriptSnippet(text='three we will be fine-tuning the llm but', start=57.6, duration=4.84), FetchedTranscriptSnippet(text='you know just to keep things focused', start=60.0, duration=4.28), FetchedTranscriptSnippet(text='here we are going to talk about', start=62.44, duration=5.039), FetchedTranscriptSnippet(text='preparing the data set and yeah to give', start=64.28, duration=6.839), FetchedTranscriptSnippet(text='you another view here um essentially', start=67.479, duration=6.121), FetchedTranscriptSnippet(text='what we are going to do is we are going', start=71.119, duration=6.36), FetchedTranscriptSnippet(text='to take text and tokenize it and then', start=73.6, duration=5.6), FetchedTranscriptSnippet(text='convert these', start=77.479, duration=4.921), FetchedTranscriptSnippet(text='tokenized uh subp parts of the text into', start=79.2, duration=6.0), FetchedTranscriptSnippet(text='token IDs which we will be then encoding', start=82.4, duration=5.399), FetchedTranscriptSnippet(text='into vectors I know this is maybe a bit', start=85.2, duration=4.52), FetchedTranscriptSnippet(text='FAS here in this video so I mentioned', start=87.799, duration=3.96), FetchedTranscriptSnippet(text='before for the videos are not um yeah as', start=89.72, duration=3.96), FetchedTranscriptSnippet(text=\"detailed as the book itself and they're\", start=91.759, duration=4.241), FetchedTranscriptSnippet(text='more focused as supplementary resource', start=93.68, duration=4.439), FetchedTranscriptSnippet(text='for the coding examples but yeah just to', start=96.0, duration=4.079), FetchedTranscriptSnippet(text='give you the big picture here so the', start=98.119, duration=5.521), FetchedTranscriptSnippet(text='goal is really to yeah prepare raw Text', start=100.079, duration=7.08), FetchedTranscriptSnippet(text='data so that the llm can then process it', start=103.64, duration=6.119), FetchedTranscriptSnippet(text=\"it's really that the llm works with text\", start=107.159, duration=4.441), FetchedTranscriptSnippet(text='but you know we have to first convert it', start=109.759, duration=5.241), FetchedTranscriptSnippet(text='into a format that it can be um yeah', start=111.6, duration=5.76), FetchedTranscriptSnippet(text=\"that can be read let's say in a form\", start=115.0, duration=4.6), FetchedTranscriptSnippet(text='that the llm understands which in that', start=117.36, duration=4.88), FetchedTranscriptSnippet(text='sense here is actually vectors like', start=119.6, duration=5.72), FetchedTranscriptSnippet(text='mathematical vectors and if I scroll up', start=122.24, duration=6.079), FetchedTranscriptSnippet(text=\"a bit it's really like the idea also\", start=125.32, duration=5.639), FetchedTranscriptSnippet(text='that yeah vectors are numeric', start=128.319, duration=5.041), FetchedTranscriptSnippet(text='representations here of this text so so', start=130.959, duration=5.521), FetchedTranscriptSnippet(text='that is our goal going from letters and', start=133.36, duration=5.72), FetchedTranscriptSnippet(text='words into a more numeric um yeah', start=136.48, duration=4.64), FetchedTranscriptSnippet(text='representation which can then be used', start=139.08, duration=4.92), FetchedTranscriptSnippet(text='later on during the um yeah pre-training', start=141.12, duration=5.24), FetchedTranscriptSnippet(text='to optimize the weight parameters of', start=144.0, duration=5.44), FetchedTranscriptSnippet(text=\"that llm but yeah um I don't want to go\", start=146.36, duration=4.64), FetchedTranscriptSnippet(text='too far ahead and talk about', start=149.44, duration=4.64), FetchedTranscriptSnippet(text='pre-training the focus here really is on', start=151.0, duration=6.48), FetchedTranscriptSnippet(text='preparing here the input data for the', start=154.08, duration=6.36), FetchedTranscriptSnippet(text='llm and specifically in section two', start=157.48, duration=6.2), FetchedTranscriptSnippet(text='tokenizing text the focus is really also', start=160.44, duration=6.12), FetchedTranscriptSnippet(text='only on this subar here at the bottom', start=163.68, duration=5.36), FetchedTranscriptSnippet(text='where we take some input text and we', start=166.56, duration=4.92), FetchedTranscriptSnippet(text='yeah we break it down into individual', start=169.04, duration=6.0), FetchedTranscriptSnippet(text='smaller chunks and um yeah and this is a', start=171.48, duration=6.16), FetchedTranscriptSnippet(text='process known as um yeah tokenization', start=175.04, duration=5.0), FetchedTranscriptSnippet(text='just to show you an example', start=177.64, duration=5.2), FetchedTranscriptSnippet(text='so uh we will be using a library uh', start=180.04, duration=6.04), FetchedTranscriptSnippet(text='called tick token later and um there is', start=182.84, duration=5.8), FetchedTranscriptSnippet(text='I think a', start=186.08, duration=6.519), FetchedTranscriptSnippet(text='nice nice um yeah virtual visualization', start=188.64, duration=7.84), FetchedTranscriptSnippet(text='um tick tokenizer uh when we go here and', start=192.599, duration=7.161), FetchedTranscriptSnippet(text='they do have different llms for example', start=196.48, duration=7.08), FetchedTranscriptSnippet(text='there is this um gpd2 llm if we type', start=199.76, duration=8.199), FetchedTranscriptSnippet(text=\"some um text let's say um hello world\", start=203.56, duration=7.28), FetchedTranscriptSnippet(text='something like this um this', start=207.959, duration=6.36), FetchedTranscriptSnippet(text='is a test so here you can really see um', start=210.84, duration=5.88), FetchedTranscriptSnippet(text=\"how it's broken down into individual\", start=214.319, duration=4.64), FetchedTranscriptSnippet(text='tokens so you can see Hello is a token', start=216.72, duration=4.599), FetchedTranscriptSnippet(text='the comma is a token world is a token', start=218.959, duration=5.121), FetchedTranscriptSnippet(text='dot is a token and these are the token', start=221.319, duration=5.241), FetchedTranscriptSnippet(text='IDs we will get to the Token IDs later', start=224.08, duration=4.799), FetchedTranscriptSnippet(text='on how they are created for now the', start=226.56, duration=4.44), FetchedTranscriptSnippet(text='focus is really like to just break it', start=228.879, duration=4.2), FetchedTranscriptSnippet(text='down into individual parts and we are', start=231.0, duration=3.76), FetchedTranscriptSnippet(text='not going to implement actually the', start=233.079, duration=4.561), FetchedTranscriptSnippet(text='exact same algorithm that is used here I', start=234.76, duration=4.6), FetchedTranscriptSnippet(text='do have some bonus material on doing', start=237.64, duration=4.92), FetchedTranscriptSnippet(text='that at will share later um here first', start=239.36, duration=5.239), FetchedTranscriptSnippet(text='we want to do the big picture level of', start=242.56, duration=4.599), FetchedTranscriptSnippet(text='how we can actually break down text into', start=244.599, duration=5.321), FetchedTranscriptSnippet(text='individual chunks so one way would be', start=247.159, duration=4.961), FetchedTranscriptSnippet(text='essentially to yeah use just regular', start=249.92, duration=4.879), FetchedTranscriptSnippet(text='Expressions but um yeah starting', start=252.12, duration=5.28), FetchedTranscriptSnippet(text='starting sequentially like how how I did', start=254.799, duration=5.881), FetchedTranscriptSnippet(text=\"in the book Let's actually download the\", start=257.4, duration=5.96), FetchedTranscriptSnippet(text='data set first and we will be using the', start=260.68, duration=6.079), FetchedTranscriptSnippet(text='data set later in this chapter um to', start=263.36, duration=5.839), FetchedTranscriptSnippet(text='actually yeah implement the data lus in', start=266.759, duration=6.121), FetchedTranscriptSnippet(text=\"pytor so let's um let's start with a\", start=269.199, duration=6.241), FetchedTranscriptSnippet(text='data set here before I go too much ahead', start=272.88, duration=5.039), FetchedTranscriptSnippet(text='and talk about all the other things so', start=275.44, duration=4.68), FetchedTranscriptSnippet(text='we are going to use a library um OS', start=277.919, duration=4.681), FetchedTranscriptSnippet(text='which is just a standard python library', start=280.12, duration=6.48), FetchedTranscriptSnippet(text='and U URL lip request which is also a', start=282.6, duration=5.76), FetchedTranscriptSnippet(text='standard python Library this is usually', start=286.6, duration=4.28), FetchedTranscriptSnippet(text='for working with files and this is for', start=288.36, duration=4.72), FetchedTranscriptSnippet(text='downloading things from the internet so', start=290.88, duration=4.28), FetchedTranscriptSnippet(text='the data set we are going to work with', start=293.08, duration=6.04), FetchedTranscriptSnippet(text=\"here um it's called um the verdict so\", start=295.16, duration=9.28), FetchedTranscriptSnippet(text='let me just go here and new tab um and', start=299.12, duration=13.079), FetchedTranscriptSnippet(text='the verdict is a short story by edit won', start=304.44, duration=7.759), FetchedTranscriptSnippet(text='Wharton so', start=312.32, duration=5.319), FetchedTranscriptSnippet(text=\"it's available as public domain\", start=314.199, duration=5.84), FetchedTranscriptSnippet(text=\"data um I think there's a Wikipedia\", start=317.639, duration=5.56), FetchedTranscriptSnippet(text='version that is actually quite', start=320.039, duration=6.841), FetchedTranscriptSnippet(text='nice here so this is a very short story', start=323.199, duration=5.641), FetchedTranscriptSnippet(text='here and I picked this one as the data', start=326.88, duration=4.08), FetchedTranscriptSnippet(text='set because yeah we want to keep things', start=328.84, duration=4.079), FetchedTranscriptSnippet(text=\"simple that it doesn't take too long\", start=330.96, duration=3.6), FetchedTranscriptSnippet(text='when we actually pre-training the llm', start=332.919, duration=3.081), FetchedTranscriptSnippet(text=\"because it's more for educational\", start=334.56, duration=4.16), FetchedTranscriptSnippet(text=\"purposes and it's also you know pretty\", start=336.0, duration=6.12), FetchedTranscriptSnippet(text='simple readable and most importantly', start=338.72, duration=6.96), FetchedTranscriptSnippet(text=\"it's a public domain um data set or book\", start=342.12, duration=5.28), FetchedTranscriptSnippet(text='so that means we can use it without', start=345.68, duration=3.84), FetchedTranscriptSnippet(text='having to worry about um copyright', start=347.4, duration=4.2), FetchedTranscriptSnippet(text='restrictions for example in the United', start=349.52, duration=3.959), FetchedTranscriptSnippet(text='States at least so in that case this is', start=351.6, duration=4.64), FetchedTranscriptSnippet(text='an open public domain data set there is', start=353.479, duration=5.641), FetchedTranscriptSnippet(text='no yeah concern using this for example', start=356.24, duration=4.92), FetchedTranscriptSnippet(text='for M', start=359.12, duration=5.56), FetchedTranscriptSnippet(text='training um okay so we would download', start=361.16, duration=5.4), FetchedTranscriptSnippet(text='this data set so we could um what we', start=364.68, duration=4.48), FetchedTranscriptSnippet(text='could do is we could go here just copy', start=366.56, duration=4.359), FetchedTranscriptSnippet(text='and paste', start=369.16, duration=4.96), FetchedTranscriptSnippet(text='this and create a new file a text file', start=370.919, duration=5.801), FetchedTranscriptSnippet(text='insert this and save it and so forth um', start=374.12, duration=4.16), FetchedTranscriptSnippet(text='I have a little bit of a quicker way', start=376.72, duration=3.479), FetchedTranscriptSnippet(text='here so I actually let me just copy and', start=378.28, duration=4.16), FetchedTranscriptSnippet(text='paste this I have it also in the GitHub', start=380.199, duration=4.84), FetchedTranscriptSnippet(text='repository for the book and so this link', start=382.44, duration=6.24), FetchedTranscriptSnippet(text='is a URL link to the place on GitHub', start=385.039, duration=6.241), FetchedTranscriptSnippet(text='where I provided this data set just in', start=388.68, duration=5.48), FetchedTranscriptSnippet(text='case you know in the future the data set', start=391.28, duration=4.72), FetchedTranscriptSnippet(text='might become unavailable from Wiki', start=394.16, duration=4.64), FetchedTranscriptSnippet(text='umedia or something here you can find a', start=396.0, duration=7.199), FetchedTranscriptSnippet(text='version in the books GitHub repository R', start=398.8, duration=10.119), FetchedTranscriptSnippet(text='asbt llms Das from- scratch and in', start=403.199, duration=8.881), FetchedTranscriptSnippet(text='chapter one so sorry chapter two in the', start=408.919, duration=5.761), FetchedTranscriptSnippet(text='first folder main chapter code I', start=412.08, duration=6.36), FetchedTranscriptSnippet(text='included the verdict here and if you', start=414.68, duration=6.84), FetchedTranscriptSnippet(text='want to copy something from uh yeah', start=418.44, duration=5.68), FetchedTranscriptSnippet(text='GitHub is raw text you have to click on', start=421.52, duration=5.32), FetchedTranscriptSnippet(text='this raw here and then yeah you get the', start=424.12, duration=5.199), FetchedTranscriptSnippet(text='the raw text which in this case um yeah', start=426.84, duration=4.919), FetchedTranscriptSnippet(text='as you can see is', start=429.319, duration=8.041), FetchedTranscriptSnippet(text='similar to what you can see on this page', start=431.759, duration=10.921), FetchedTranscriptSnippet(text='so if I copy this URL now insert it here', start=437.36, duration=6.76), FetchedTranscriptSnippet(text=\"it's really the same URL that you have\", start=442.68, duration=5.4), FetchedTranscriptSnippet(text='seen earlier without the line breaks', start=444.12, duration=5.799), FetchedTranscriptSnippet(text='when I execute this code it will', start=448.08, duration=4.799), FetchedTranscriptSnippet(text='download this um data set and save it to', start=449.919, duration=8.521), FetchedTranscriptSnippet(text='a file the- word. text let me do that um', start=452.879, duration=7.561), FetchedTranscriptSnippet(text='and then maybe refresh here and yeah you', start=458.44, duration=5.08), FetchedTranscriptSnippet(text=\"can see it's now in this folder so it's\", start=460.44, duration=5.52), FetchedTranscriptSnippet(text='locally now on our computer The Next', start=463.52, duration=4.88), FetchedTranscriptSnippet(text='Step would be opening this file in', start=465.96, duration=4.16), FetchedTranscriptSnippet(text=\"Python so right now it's just sitting in\", start=468.4, duration=4.56), FetchedTranscriptSnippet(text='our computer and the next step would be', start=470.12, duration=5.519), FetchedTranscriptSnippet(text='um yeah opening this so let me', start=472.96, duration=4.72), FetchedTranscriptSnippet(text='just copy', start=475.639, duration=5.801), FetchedTranscriptSnippet(text='this and we are just reading it so the r', start=477.68, duration=6.359), FetchedTranscriptSnippet(text=\"here and then on Windows it's I think\", start=481.44, duration=5.52), FetchedTranscriptSnippet(text='required to specifically say which', start=484.039, duration=5.28), FetchedTranscriptSnippet(text='encoding we are using so utf8 the', start=486.96, duration=3.919), FetchedTranscriptSnippet(text='standard', start=489.319, duration=4.84), FetchedTranscriptSnippet(text='encoding um yeah and then we can', start=490.879, duration=6.521), FetchedTranscriptSnippet(text='just read the raw text', start=494.159, duration=6.081), FetchedTranscriptSnippet(text=\"here and then let's double check so we\", start=497.4, duration=4.6), FetchedTranscriptSnippet(text='can double check that the text looks', start=500.24, duration=5.079), FetchedTranscriptSnippet(text='okay so we can see y the python', start=502.0, duration=6.039), FetchedTranscriptSnippet(text='interpreter has successfully loaded the', start=505.319, duration=4.84), FetchedTranscriptSnippet(text='text here', start=508.039, duration=3.761), FetchedTranscriptSnippet(text='and we can also just double check you', start=510.159, duration=4.24), FetchedTranscriptSnippet(text='know how long it is', start=511.8, duration=5.119), FetchedTranscriptSnippet(text='so I maybe recommend running this if you', start=514.399, duration=3.961), FetchedTranscriptSnippet(text='are running the code to check that you', start=516.919, duration=3.92), FetchedTranscriptSnippet(text='get approximately the same number here', start=518.36, duration=4.239), FetchedTranscriptSnippet(text=\"which means you're not missing any\", start=520.839, duration=4.12), FetchedTranscriptSnippet(text='character so it should be about', start=522.599, duration=4.881), FetchedTranscriptSnippet(text='2,479 characters if you loaded the data', start=524.959, duration=6.88), FetchedTranscriptSnippet(text=\"set correctly so let's now um briefly\", start=527.48, duration=7.0), FetchedTranscriptSnippet(text='introduce the concept of tokenization', start=531.839, duration=4.801), FetchedTranscriptSnippet(text='with regular Expressions which is just', start=534.48, duration=5.44), FetchedTranscriptSnippet(text='like a warm up um before weuse use the', start=536.64, duration=5.92), FetchedTranscriptSnippet(text=\"real let's say tick token library for\", start=539.92, duration=4.76), FetchedTranscriptSnippet(text='tokenization so we are going to use a', start=542.56, duration=4.719), FetchedTranscriptSnippet(text=\"python regular expression and let's\", start=544.68, duration=5.0), FetchedTranscriptSnippet(text=\"actually use some simpler text let's for\", start=547.279, duration=4.441), FetchedTranscriptSnippet(text=\"the warmup here let's use the text I\", start=549.68, duration=4.52), FetchedTranscriptSnippet(text='just typed in', start=551.72, duration=7.88), FetchedTranscriptSnippet(text='here and um provide it here then we can', start=554.2, duration=7.72), FetchedTranscriptSnippet(text='type result and then the regular', start=559.6, duration=5.479), FetchedTranscriptSnippet(text='expression and I I must say that regular', start=561.92, duration=5.919), FetchedTranscriptSnippet(text=\"expressions are my weak spot so I'm\", start=565.079, duration=4.561), FetchedTranscriptSnippet(text='really not that good at them but', start=567.839, duration=4.921), FetchedTranscriptSnippet(text='honestly um nowadays with llms like chat', start=569.64, duration=5.319), FetchedTranscriptSnippet(text=\"GPT it's actually pretty easy to figure\", start=572.76, duration=4.319), FetchedTranscriptSnippet(text='it out so this is essentially just a', start=574.959, duration=5.081), FetchedTranscriptSnippet(text='regular expression that splits on whes', start=577.079, duration=4.801), FetchedTranscriptSnippet(text='Space characters and by the way you', start=580.04, duration=3.52), FetchedTranscriptSnippet(text=\"don't have to really know what regular\", start=581.88, duration=3.56), FetchedTranscriptSnippet(text='expressions are for the bigger context', start=583.56, duration=5.04), FetchedTranscriptSnippet(text='of training llms because we are later um', start=585.44, duration=5.72), FetchedTranscriptSnippet(text='using a tokenizer based on bite pair', start=588.6, duration=4.919), FetchedTranscriptSnippet(text='encoding and I also have a from scratch', start=591.16, duration=4.28), FetchedTranscriptSnippet(text=\"implementation it's not in the book but\", start=593.519, duration=4.201), FetchedTranscriptSnippet(text=\"it's in the supplementary materials that\", start=595.44, duration=4.44), FetchedTranscriptSnippet(text='I can also maybe show you later', start=597.72, duration=4.84), FetchedTranscriptSnippet(text='but yeah for Simplicity our goal here if', start=599.88, duration=5.639), FetchedTranscriptSnippet(text='you remember what I mentioned earlier', start=602.56, duration=5.12), FetchedTranscriptSnippet(text='our goal here is first to split up this', start=605.519, duration=3.88), FetchedTranscriptSnippet(text='text into individual', start=607.68, duration=4.68), FetchedTranscriptSnippet(text=\"tokens so let's do that with this\", start=609.399, duration=5.281), FetchedTranscriptSnippet(text='regular expression and then maybe U', start=612.36, duration=4.8), FetchedTranscriptSnippet(text='print the result and see how it looks', start=614.68, duration=4.76), FetchedTranscriptSnippet(text='like and so yeah we can see we have now', start=617.16, duration=4.72), FetchedTranscriptSnippet(text='the individual words and we have whites', start=619.44, duration=5.12), FetchedTranscriptSnippet(text='space characters and so forth um one', start=621.88, duration=5.28), FetchedTranscriptSnippet(text='thing um we might want to do is also to', start=624.56, duration=4.92), FetchedTranscriptSnippet(text='have the punctuation', start=627.16, duration=5.76), FetchedTranscriptSnippet(text='as um separate characters so for that we', start=629.48, duration=5.08), FetchedTranscriptSnippet(text='would have to make a little bit of a', start=632.92, duration=4.4), FetchedTranscriptSnippet(text='more sophisticated regular expression', start=634.56, duration=4.839), FetchedTranscriptSnippet(text=\"and like I told you I'm not very good at\", start=637.32, duration=3.88), FetchedTranscriptSnippet(text='regular Expressions so let me just um', start=639.399, duration=4.161), FetchedTranscriptSnippet(text='copy and paste it here so this is now a', start=641.2, duration=4.72), FetchedTranscriptSnippet(text='regular expression that is slightly more', start=643.56, duration=6.44), FetchedTranscriptSnippet(text='sophisticated um so this one will also', start=645.92, duration=7.44), FetchedTranscriptSnippet(text='include the punctuation as separate', start=650.0, duration=6.44), FetchedTranscriptSnippet(text='tokens where before they were part of', start=653.36, duration=6.599), FetchedTranscriptSnippet(text='the word itself okay so this is our', start=656.44, duration=7.079), FetchedTranscriptSnippet(text='simplest um way of you know tokenizing', start=659.959, duration=6.56), FetchedTranscriptSnippet(text='now as you have seen here if I go here', start=663.519, duration=4.801), FetchedTranscriptSnippet(text='there are actually no whes space', start=666.519, duration=3.601), FetchedTranscriptSnippet(text='characters', start=668.32, duration=5.84), FetchedTranscriptSnippet(text='um in in the output here so one thing we', start=670.12, duration=6.519), FetchedTranscriptSnippet(text='to mimic this could be for example that', start=674.16, duration=4.479), FetchedTranscriptSnippet(text='we are also going to strip the whites', start=676.639, duration=3.681), FetchedTranscriptSnippet(text='space characters this is just like an', start=678.639, duration=3.721), FetchedTranscriptSnippet(text='optional thing we could do we can just', start=680.32, duration=4.0), FetchedTranscriptSnippet(text='get rid of the whites space characters', start=682.36, duration=4.32), FetchedTranscriptSnippet(text='so here I just have a simple list', start=684.32, duration=5.28), FetchedTranscriptSnippet(text='comprehension that will strip out yeah', start=686.68, duration=5.44), FetchedTranscriptSnippet(text='the whites space characters so you can', start=689.6, duration=5.08), FetchedTranscriptSnippet(text='see now that is the result um yeah', start=692.12, duration=4.2), FetchedTranscriptSnippet(text=\"without whites space characters it's a\", start=694.68, duration=5.279), FetchedTranscriptSnippet(text='bit different from what you see here', start=696.32, duration=5.959), FetchedTranscriptSnippet(text='because this is a sophisticated bite', start=699.959, duration=4.0), FetchedTranscriptSnippet(text='pair encoding algorithm and I will talk', start=702.279, duration=4.56), FetchedTranscriptSnippet(text='more about this later I will share some', start=703.959, duration=4.88), FetchedTranscriptSnippet(text='uh we have a dedicated section on this', start=706.839, duration=5.161), FetchedTranscriptSnippet(text=\"actually but so here for now we're just\", start=708.839, duration=5.0), FetchedTranscriptSnippet(text='building a very simple version of that', start=712.0, duration=4.24), FetchedTranscriptSnippet(text='to understand roughly yeah what we are', start=713.839, duration=5.12), FetchedTranscriptSnippet(text='doing here like um how we are tokenizing', start=716.24, duration=3.64), FetchedTranscriptSnippet(text='text', start=718.959, duration=3.961), FetchedTranscriptSnippet(text='um doing it a bit more advanced now I', start=719.88, duration=6.6), FetchedTranscriptSnippet(text='mean this is a nice way to um yeah', start=722.92, duration=6.44), FetchedTranscriptSnippet(text='handle simple cases but now imagine we', start=726.48, duration=5.24), FetchedTranscriptSnippet(text='have a more complicated case like that', start=729.36, duration=4.479), FetchedTranscriptSnippet(text='so this this will actually fail really', start=731.72, duration=4.919), FetchedTranscriptSnippet(text=\"it doesn't really take care of special\", start=733.839, duration=4.881), FetchedTranscriptSnippet(text='characters like you know the Double Dash', start=736.639, duration=3.921), FetchedTranscriptSnippet(text='and everything so we can actually make', start=738.72, duration=5.32), FetchedTranscriptSnippet(text='it more sophisticated so I have now like', start=740.56, duration=6.68), FetchedTranscriptSnippet(text='um more sophisticated regular expression', start=744.04, duration=5.0), FetchedTranscriptSnippet(text='and also again the white space stripping', start=747.24, duration=3.92), FetchedTranscriptSnippet(text='and that does a good job of you know', start=749.04, duration=4.52), FetchedTranscriptSnippet(text='separating words and special tokens so', start=751.16, duration=4.84), FetchedTranscriptSnippet(text='we did a lot of work here now to yeah', start=753.56, duration=6.92), FetchedTranscriptSnippet(text='prepare our our text um by split', start=756.0, duration=6.68), FetchedTranscriptSnippet(text='splitting it into individual tokens and', start=760.48, duration=4.56), FetchedTranscriptSnippet(text='punctuation characters so we tested it', start=762.68, duration=4.399), FetchedTranscriptSnippet(text=\"on this simple example but let's\", start=765.04, duration=5.64), FetchedTranscriptSnippet(text='actually use this on our um yeah on our', start=767.079, duration=6.721), FetchedTranscriptSnippet(text='real um text so for that what we have to', start=770.68, duration=5.76), FetchedTranscriptSnippet(text='do now is we have to actually use our', start=773.8, duration=5.52), FetchedTranscriptSnippet(text='raw text where raw text is what we have', start=776.44, duration=6.12), FetchedTranscriptSnippet(text=\"have here at the top so that's our raw\", start=779.32, duration=6.639), FetchedTranscriptSnippet(text=\"text let's actually see what happens\", start=782.56, duration=7.56), FetchedTranscriptSnippet(text='when we do that so yeah so we tokenized', start=785.959, duration=8.44), FetchedTranscriptSnippet(text=\"our raw text and now let's see how many\", start=790.12, duration=8.36), FetchedTranscriptSnippet(text='tokens we have so we have uh length of', start=794.399, duration=5.921), FetchedTranscriptSnippet(text='let me actually save this as', start=798.48, duration=6.12), FetchedTranscriptSnippet(text='a variable because this is a lot of code', start=800.32, duration=8.639), FetchedTranscriptSnippet(text=\"um let's just call it preprocessed\", start=804.6, duration=4.359), FetchedTranscriptSnippet(text='and', start=817.68, duration=2.24), FetchedTranscriptSnippet(text='then so yeah we have here', start=821.32, duration=6.36), FetchedTranscriptSnippet(text='4,690 tokens so what we have done so far', start=824.44, duration=7.36), FetchedTranscriptSnippet(text='in section 2.2 is we took raw text and', start=827.68, duration=7.32), FetchedTranscriptSnippet(text='we tokenized it and now in the next um', start=831.8, duration=5.44), FetchedTranscriptSnippet(text='yeah section we will be talking about', start=835.0, duration=5.16), FetchedTranscriptSnippet(text='converting this raw text into these', start=837.24, duration=6.12), FetchedTranscriptSnippet(text='so-called token', start=840.16, duration=3.2), FetchedTranscriptSnippet(text=\"IDs yeah let's now talk about converting\", start=845.0, duration=6.199), FetchedTranscriptSnippet(text='the tokens into token IDs so previously', start=847.8, duration=5.36), FetchedTranscriptSnippet(text='we talked about breaking down the text', start=851.199, duration=4.64), FetchedTranscriptSnippet(text='into individual tokens here and now in', start=853.16, duration=4.72), FetchedTranscriptSnippet(text='this video we are going to talk about', start=855.839, duration=4.761), FetchedTranscriptSnippet(text='converting the tokens here into token', start=857.88, duration=5.12), FetchedTranscriptSnippet(text='IDs which are so-called yeah uh token', start=860.6, duration=5.64), FetchedTranscriptSnippet(text='IDs um and they are essentially unique', start=863.0, duration=6.12), FetchedTranscriptSnippet(text='integers so how do we do that the first', start=866.24, duration=5.599), FetchedTranscriptSnippet(text=\"First Step let's maybe go to section 2.3\", start=869.12, duration=4.92), FetchedTranscriptSnippet(text='here the first step would be building a', start=871.839, duration=5.081), FetchedTranscriptSnippet(text='so-called vocabulary so the vocabulary', start=874.04, duration=5.2), FetchedTranscriptSnippet(text='is a unique mapping between yeah each', start=876.92, duration=6.0), FetchedTranscriptSnippet(text='word that can occur and a unique integer', start=879.24, duration=5.76), FetchedTranscriptSnippet(text='so for example for Simplicity if we have', start=882.92, duration=4.719), FetchedTranscriptSnippet(text='a very small data set here the quick', start=885.0, duration=5.48), FetchedTranscriptSnippet(text='brown fox jumps over the lazy dog the', start=887.639, duration=4.921), FetchedTranscriptSnippet(text='first step would be yeah breaking down', start=890.48, duration=3.96), FetchedTranscriptSnippet(text='the text into tokens which we have done', start=892.56, duration=4.519), FetchedTranscriptSnippet(text='before and now we are going to yeah sort', start=894.44, duration=4.92), FetchedTranscriptSnippet(text='them alphabetically making sure sure we', start=897.079, duration=4.241), FetchedTranscriptSnippet(text=\"don't have duplicates in there and then\", start=899.36, duration=4.36), FetchedTranscriptSnippet(text='we are going to assign a unique mapping', start=901.32, duration=5.319), FetchedTranscriptSnippet(text='here to integers so how do we do that in', start=903.72, duration=5.72), FetchedTranscriptSnippet(text='Python um so we already have the', start=906.639, duration=5.401), FetchedTranscriptSnippet(text='pre-processed um from the previous', start=909.44, duration=4.68), FetchedTranscriptSnippet(text='section so pre-processed is all the', start=912.04, duration=5.359), FetchedTranscriptSnippet(text='unique tokens in in the data set and so', start=914.12, duration=5.639), FetchedTranscriptSnippet(text='what we can do now is we can get rid of', start=917.399, duration=6.041), FetchedTranscriptSnippet(text='the duplicates first so we can do um set', start=919.759, duration=6.041), FetchedTranscriptSnippet(text='pre-processed which will get rid of the', start=923.44, duration=5.199), FetchedTranscriptSnippet(text=\"duplicates and then um let's also sort\", start=925.8, duration=5.0), FetchedTranscriptSnippet(text='them with a sorted function in Python', start=928.639, duration=4.961), FetchedTranscriptSnippet(text='and we can yeah assign all words a', start=930.8, duration=5.68), FetchedTranscriptSnippet(text=\"variable to hold them so let's just make\", start=933.6, duration=4.56), FetchedTranscriptSnippet(text='sure it worked so we can see these are', start=936.48, duration=4.32), FetchedTranscriptSnippet(text=\"all the unique words it's a lot of\", start=938.16, duration=5.52), FetchedTranscriptSnippet(text='words we can actually also look at how', start=940.8, duration=6.2), FetchedTranscriptSnippet(text='how many we have how long this list is', start=943.68, duration=5.279), FetchedTranscriptSnippet(text=\"so that's\", start=947.0, duration=5.44), FetchedTranscriptSnippet(text=\"about, 130 words so it's yeah four times\", start=948.959, duration=5.721), FetchedTranscriptSnippet(text='smaller than pre-processed so we had a', start=952.44, duration=4.759), FetchedTranscriptSnippet(text=\"bunch of duplicates in there um let's\", start=954.68, duration=4.639), FetchedTranscriptSnippet(text='actually save that for later we call', start=957.199, duration=4.361), FetchedTranscriptSnippet(text='that the vocabulary size so how many', start=959.319, duration=5.801), FetchedTranscriptSnippet(text='unique words we have and um yeah so the', start=961.56, duration=5.92), FetchedTranscriptSnippet(text='vo vocabulary size', start=965.12, duration=5.88), FetchedTranscriptSnippet(text='is shown below', start=967.48, duration=6.2), FetchedTranscriptSnippet(text=\"1,130 so next let's actually build the\", start=971.0, duration=4.959), FetchedTranscriptSnippet(text='vocabulary this is actually quite simple', start=973.68, duration=4.399), FetchedTranscriptSnippet(text='in in this illustration for educational', start=975.959, duration=4.44), FetchedTranscriptSnippet(text='purposes what we can do is we can for', start=978.079, duration=5.401), FetchedTranscriptSnippet(text='each token assign an integer a unique', start=980.399, duration=7.281), FetchedTranscriptSnippet(text='integer for each um integer token in', start=983.48, duration=7.0), FetchedTranscriptSnippet(text='enumerate', start=987.68, duration=6.8), FetchedTranscriptSnippet(text='all oops all words so what it will do is', start=990.48, duration=7.359), FetchedTranscriptSnippet(text='it will oh yeah iterate over all the', start=994.48, duration=5.479), FetchedTranscriptSnippet(text='unique words and then assign in', start=997.839, duration=4.321), FetchedTranscriptSnippet(text='ascending order these integer labels so', start=999.959, duration=5.0), FetchedTranscriptSnippet(text='we can actually take a look at this so', start=1002.16, duration=4.28), FetchedTranscriptSnippet(text=\"you can see it's yeah essentially\", start=1004.959, duration=4.961), FetchedTranscriptSnippet(text='mapping each word to an integer here um', start=1006.44, duration=5.36), FetchedTranscriptSnippet(text='yeah and this is essentially um yeah', start=1009.92, duration=5.159), FetchedTranscriptSnippet(text=\"what we're doing here so now we have the\", start=1011.8, duration=6.0), FetchedTranscriptSnippet(text='vocabulary so maybe going to the next um', start=1015.079, duration=4.801), FetchedTranscriptSnippet(text='figure so the next Next Step would be', start=1017.8, duration=4.719), FetchedTranscriptSnippet(text='now that we have this vocabulary we use', start=1019.88, duration=6.4), FetchedTranscriptSnippet(text='this vocabulary to actually tokenize um', start=1022.519, duration=7.361), FetchedTranscriptSnippet(text='training data into token IDs you can see', start=1026.28, duration=5.96), FetchedTranscriptSnippet(text='the vocabulary is in a sorted order so', start=1029.88, duration=4.199), FetchedTranscriptSnippet(text='we have them alphabetically sorted with', start=1032.24, duration=4.24), FetchedTranscriptSnippet(text='this mapping and now for any new text', start=1034.079, duration=4.401), FetchedTranscriptSnippet(text='for example the training text but also', start=1036.48, duration=4.16), FetchedTranscriptSnippet(text='later on when we want to use the llm', start=1038.48, duration=4.8), FetchedTranscriptSnippet(text=\"we're going to use this vocabulary to\", start=1040.64, duration=6.679), FetchedTranscriptSnippet(text='convert any text into token IDs here so', start=1043.28, duration=6.04), FetchedTranscriptSnippet(text='basically using the vocabulary mapping', start=1047.319, duration=4.201), FetchedTranscriptSnippet(text='if we have the word the we would find', start=1049.32, duration=4.2), FetchedTranscriptSnippet(text='the word the in the vocabulary and find', start=1051.52, duration=5.0), FetchedTranscriptSnippet(text='the corresponding integer for example so', start=1053.52, duration=5.159), FetchedTranscriptSnippet(text=\"the here it's not shown because it's too\", start=1056.52, duration=4.92), FetchedTranscriptSnippet(text='long would correspond to integer seven', start=1058.679, duration=4.801), FetchedTranscriptSnippet(text='and then the second word Brown would', start=1061.44, duration=5.28), FetchedTranscriptSnippet(text='correspond to integer zero and then doc', start=1063.48, duration=5.079), FetchedTranscriptSnippet(text='would correspond to integer one and so', start=1066.72, duration=3.959), FetchedTranscriptSnippet(text='forth and so this way we are going to', start=1068.559, duration=4.841), FetchedTranscriptSnippet(text='encode um the given text into these', start=1070.679, duration=7.401), FetchedTranscriptSnippet(text=\"token IDs so let's um yeah do this in\", start=1073.4, duration=8.08), FetchedTranscriptSnippet(text='Python so um I will actually uh copy and', start=1078.08, duration=4.719), FetchedTranscriptSnippet(text='paste here because otherwise it will', start=1081.48, duration=2.88), FetchedTranscriptSnippet(text='take too long if I just type it and I', start=1082.799, duration=3.88), FetchedTranscriptSnippet(text='will make errors because typing in front', start=1084.36, duration=5.199), FetchedTranscriptSnippet(text='of a camera is quite different than um', start=1086.679, duration=7.0), FetchedTranscriptSnippet(text='coding in yeah uh silence but um so here', start=1089.559, duration=5.961), FetchedTranscriptSnippet(text=\"what I'm going to show you is a simple\", start=1093.679, duration=4.201), FetchedTranscriptSnippet(text=\"tokenizer class and that's a python\", start=1095.52, duration=5.399), FetchedTranscriptSnippet(text='class and that does the following things', start=1097.88, duration=6.279), FetchedTranscriptSnippet(text='so in it is um usually yeah uh executed', start=1100.919, duration=5.281), FetchedTranscriptSnippet(text='when you are instantiating a new object', start=1104.159, duration=4.321), FetchedTranscriptSnippet(text='which we will be doing later let me just', start=1106.2, duration=4.28), FetchedTranscriptSnippet(text='maybe first explain and then we will', start=1108.48, duration=4.64), FetchedTranscriptSnippet(text='create a new object here but um so in it', start=1110.48, duration=5.4), FetchedTranscriptSnippet(text='is uh the Constructor it takes a', start=1113.12, duration=6.0), FetchedTranscriptSnippet(text='vocabulary and it defines um or will', start=1115.88, duration=5.6), FetchedTranscriptSnippet(text='just save the vocabulary as the string', start=1119.12, duration=5.919), FetchedTranscriptSnippet(text='to integer mapping so string to integer', start=1121.48, duration=5.24), FetchedTranscriptSnippet(text='uh means that we are mapping from', start=1125.039, duration=4.361), FetchedTranscriptSnippet(text='strings to integer representations and', start=1126.72, duration=4.4), FetchedTranscriptSnippet(text='then we also have the same thing', start=1129.4, duration=4.399), FetchedTranscriptSnippet(text='inverted so we are here instead of', start=1131.12, duration=6.52), FetchedTranscriptSnippet(text='having i s we have SI which inverts', start=1133.799, duration=7.24), FetchedTranscriptSnippet(text='string and integer so what it will do is', start=1137.64, duration=5.32), FetchedTranscriptSnippet(text='it will yeah create this mapping from', start=1141.039, duration=5.241), FetchedTranscriptSnippet(text='integer to string now so we have integer', start=1142.96, duration=6.68), FetchedTranscriptSnippet(text='and string inverting this essentially um', start=1146.28, duration=5.6), FetchedTranscriptSnippet(text='yeah so here we have now two', start=1149.64, duration=4.24), FetchedTranscriptSnippet(text='vocabularies the the regular vocabulary', start=1151.88, duration=3.44), FetchedTranscriptSnippet(text='and the inverted', start=1153.88, duration=3.96), FetchedTranscriptSnippet(text='vocabulary and then we have an end code', start=1155.32, duration=5.92), FetchedTranscriptSnippet(text='method that takes any text and uses the', start=1157.84, duration=6.8), FetchedTranscriptSnippet(text='regular expression from section 2.1 uh', start=1161.24, duration=6.52), FetchedTranscriptSnippet(text='sorry 2.2 and breaks down the text into', start=1164.64, duration=6.279), FetchedTranscriptSnippet(text='tokens so this is what we have seen um', start=1167.76, duration=4.52), FetchedTranscriptSnippet(text='earlier this is quite the long', start=1170.919, duration=3.521), FetchedTranscriptSnippet(text='vocabulary if I scroll up so let me', start=1172.28, duration=4.399), FetchedTranscriptSnippet(text='actually maybe comment this', start=1174.44, duration=5.479), FetchedTranscriptSnippet(text='out so earlier in the previous uh video', start=1176.679, duration=5.681), FetchedTranscriptSnippet(text='we have yeah', start=1179.919, duration=5.601), FetchedTranscriptSnippet(text='um broken down text into tokens so we', start=1182.36, duration=5.76), FetchedTranscriptSnippet(text='use this regular expression so here we', start=1185.52, duration=4.279), FetchedTranscriptSnippet(text='are just going to reuse that regular', start=1188.12, duration=4.12), FetchedTranscriptSnippet(text='expression from earlier and then strip', start=1189.799, duration=5.161), FetchedTranscriptSnippet(text='out the white spaces and then here this', start=1192.24, duration=4.84), FetchedTranscriptSnippet(text='is the interesting part this is where we', start=1194.96, duration=4.88), FetchedTranscriptSnippet(text='are creating the token IDs so we are', start=1197.08, duration=5.4), FetchedTranscriptSnippet(text='going to use the vocabulary from here', start=1199.84, duration=6.28), FetchedTranscriptSnippet(text='and for each token or string in', start=1202.48, duration=6.319), FetchedTranscriptSnippet(text='pre-process we are going to yeah convert', start=1206.12, duration=4.919), FetchedTranscriptSnippet(text='it so just to show you what that means', start=1208.799, duration=6.601), FetchedTranscriptSnippet(text=\"so if I just take let's say Jack here\", start=1211.039, duration=8.201), FetchedTranscriptSnippet(text='and I pass it to my w cap so if I have', start=1215.4, duration=6.44), FetchedTranscriptSnippet(text='something like', start=1219.24, duration=2.6), FetchedTranscriptSnippet(text='this so this should give me an ID right', start=1223.72, duration=5.8), FetchedTranscriptSnippet(text='so this is essentially yeah the the', start=1227.48, duration=4.76), FetchedTranscriptSnippet(text='token ID and I can also maybe just show', start=1229.52, duration=8.08), FetchedTranscriptSnippet(text='you to create a reverse mapping', start=1232.24, duration=5.36), FetchedTranscriptSnippet(text='here so if we want to go back to string', start=1238.4, duration=7.92), FetchedTranscriptSnippet(text='and type the 57 this should give us the', start=1243.2, duration=5.959), FetchedTranscriptSnippet(text='original word back so this is basically', start=1246.32, duration=5.599), FetchedTranscriptSnippet(text=\"What's Happening Here we have uh a\", start=1249.159, duration=6.0), FetchedTranscriptSnippet(text='vocabulary inverse vocabulary here we', start=1251.919, duration=6.321), FetchedTranscriptSnippet(text='are just going from string to integer so', start=1255.159, duration=5.841), FetchedTranscriptSnippet(text='string to integer means this one here we', start=1258.24, duration=5.64), FetchedTranscriptSnippet(text='are going from string to this token ID', start=1261.0, duration=6.08), FetchedTranscriptSnippet(text='and we do that for each token ID in the', start=1263.88, duration=4.679), FetchedTranscriptSnippet(text='in the data set', start=1267.08, duration=4.24), FetchedTranscriptSnippet(text='here and the data set is yeah the', start=1268.559, duration=4.641), FetchedTranscriptSnippet(text='tokenized version of the', start=1271.32, duration=4.32), FetchedTranscriptSnippet(text='text yeah and this is really it and then', start=1273.2, duration=5.44), FetchedTranscriptSnippet(text='we have a decode method and the decode', start=1275.64, duration=5.76), FetchedTranscriptSnippet(text='method goes backwards so here we are', start=1278.64, duration=6.519), FetchedTranscriptSnippet(text='going to uh recreate the text by mapping', start=1281.4, duration=5.92), FetchedTranscriptSnippet(text='back from integers to string which is', start=1285.159, duration=3.961), FetchedTranscriptSnippet(text=\"what I've just shown you so integers to\", start=1287.32, duration=4.239), FetchedTranscriptSnippet(text='string means that we go here from the', start=1289.12, duration=4.799), FetchedTranscriptSnippet(text='integer back to the string and I think', start=1291.559, duration=5.161), FetchedTranscriptSnippet(text='yeah this might be relatively yeah I', start=1293.919, duration=5.841), FetchedTranscriptSnippet(text=\"mean complicated maybe not but uh it's a\", start=1296.72, duration=5.64), FetchedTranscriptSnippet(text=\"lot of information so in this case let's\", start=1299.76, duration=4.399), FetchedTranscriptSnippet(text='actually use it and I think it becomes', start=1302.36, duration=4.24), FetchedTranscriptSnippet(text='more clear when we actually use it so', start=1304.159, duration=4.0), FetchedTranscriptSnippet(text=\"let's uh do\", start=1306.6, duration=3.28), FetchedTranscriptSnippet(text=\"simple unfortunately I don't have\", start=1308.159, duration=3.681), FetchedTranscriptSnippet(text='autocomplete here so let me just copy it', start=1309.88, duration=3.56), FetchedTranscriptSnippet(text=\"from here let's actually just\", start=1311.84, duration=3.92), FetchedTranscriptSnippet(text='instantiate a new object um using the', start=1313.44, duration=5.28), FetchedTranscriptSnippet(text='vocabulary', start=1315.76, duration=5.68), FetchedTranscriptSnippet(text='um like this oops not defined I need to', start=1318.72, duration=5.12), FetchedTranscriptSnippet(text='execute this cell first this always', start=1321.44, duration=4.8), FetchedTranscriptSnippet(text='happens when I um yeah talk and code at', start=1323.84, duration=5.12), FetchedTranscriptSnippet(text='the same time I forget things um then', start=1326.24, duration=5.559), FetchedTranscriptSnippet(text=\"let's just use some text here instead of\", start=1328.96, duration=5.36), FetchedTranscriptSnippet(text=\"me typing it I'm just copy and pasting\", start=1331.799, duration=7.12), FetchedTranscriptSnippet(text='it and so the first step is converting', start=1334.32, duration=7.32), FetchedTranscriptSnippet(text=\"text into these IDs so let's do\", start=1338.919, duration=6.481), FetchedTranscriptSnippet(text='tokenizer encode and then text so this', start=1341.64, duration=5.84), FetchedTranscriptSnippet(text='is essentially calling this encode', start=1345.4, duration=4.96), FetchedTranscriptSnippet(text='method and then we will hopefully see', start=1347.48, duration=6.84), FetchedTranscriptSnippet(text='that those are converted into token IDs', start=1350.36, duration=6.16), FetchedTranscriptSnippet(text='so we can see now this would be the', start=1354.32, duration=4.88), FetchedTranscriptSnippet(text='token ID representation for this', start=1356.52, duration=5.0), FetchedTranscriptSnippet(text='particular text um yeah and we could', start=1359.2, duration=4.32), FetchedTranscriptSnippet(text='also go backwards so if we have these', start=1361.52, duration=4.92), FetchedTranscriptSnippet(text='token IDs we can do tokenizer', start=1363.52, duration=6.32), FetchedTranscriptSnippet(text='decode and IDs and this would correspond', start=1366.44, duration=6.32), FetchedTranscriptSnippet(text='to this code this will be converting', start=1369.84, duration=5.52), FetchedTranscriptSnippet(text='this back into text essentially and yeah', start=1372.76, duration=4.64), FetchedTranscriptSnippet(text=\"what's nice is you can do also the round\", start=1375.36, duration=5.76), FetchedTranscriptSnippet(text='trip which means means um what we can do', start=1377.4, duration=8.12), FetchedTranscriptSnippet(text=\"is let's do it like this decode and then\", start=1381.12, duration=7.4), FetchedTranscriptSnippet(text='put this encoding inside here and this', start=1385.52, duration=5.36), FetchedTranscriptSnippet(text='should give us the original text or', start=1388.52, duration=4.08), FetchedTranscriptSnippet(text='something that is close to the original', start=1390.88, duration=3.96), FetchedTranscriptSnippet(text='text you can see there are few slight', start=1392.6, duration=5.28), FetchedTranscriptSnippet(text='differences for example here we have', start=1394.84, duration=5.24), FetchedTranscriptSnippet(text='this additional white space for example', start=1397.88, duration=4.96), FetchedTranscriptSnippet(text='but um overall this works quite well we', start=1400.08, duration=5.8), FetchedTranscriptSnippet(text='have a way of yeah um converting text', start=1402.84, duration=6.4), FetchedTranscriptSnippet(text='back into token from token back into', start=1405.88, duration=6.08), FetchedTranscriptSnippet(text='text um so this would be a very simple', start=1409.24, duration=5.96), FetchedTranscriptSnippet(text=\"tokenizer that's not the real tokenizer\", start=1411.96, duration=4.88), FetchedTranscriptSnippet(text=\"that is used by GPT models that's why\", start=1415.2, duration=4.2), FetchedTranscriptSnippet(text=\"I'm calling it simple tokenizer but I\", start=1416.84, duration=5.48), FetchedTranscriptSnippet(text='think um this illustrates the concept of', start=1419.4, duration=6.6), FetchedTranscriptSnippet(text='tokenization quite nicely how we go from', start=1422.32, duration=7.16), FetchedTranscriptSnippet(text='text into integer presentations um and', start=1426.0, duration=5.4), FetchedTranscriptSnippet(text='now in the next video we will talk a bit', start=1429.48, duration=4.24), FetchedTranscriptSnippet(text='about yeah Special coding uh special', start=1431.4, duration=6.759), FetchedTranscriptSnippet(text='tokens and how we handle those', start=1433.72, duration=4.439), FetchedTranscriptSnippet(text=\"all right let's now talk about adding\", start=1439.84, duration=5.319), FetchedTranscriptSnippet(text='special context tokens so previously we', start=1441.76, duration=5.68), FetchedTranscriptSnippet(text='talked about creating the so-called', start=1445.159, duration=5.081), FetchedTranscriptSnippet(text='vocabulary from all the different tokens', start=1447.44, duration=4.64), FetchedTranscriptSnippet(text='in the training set and then we use that', start=1450.24, duration=5.08), FetchedTranscriptSnippet(text='vocabulary to create the token IDs um', start=1452.08, duration=5.839), FetchedTranscriptSnippet(text='now we are going to talk about extending', start=1455.32, duration=6.28), FetchedTranscriptSnippet(text='this vocabulary with special tokens so', start=1457.919, duration=6.24), FetchedTranscriptSnippet(text='for example um we might want to add', start=1461.6, duration=5.319), FetchedTranscriptSnippet(text='depending on the tokenizer a token for', start=1464.159, duration=4.4), FetchedTranscriptSnippet(text='unknown words that are not in the', start=1466.919, duration=3.961), FetchedTranscriptSnippet(text='training set or for example we could', start=1468.559, duration=4.961), FetchedTranscriptSnippet(text=\"indicate when the there's a end of the\", start=1470.88, duration=5.159), FetchedTranscriptSnippet(text='text by using a so-called end of text', start=1473.52, duration=4.56), FetchedTranscriptSnippet(text='token and we will talk more about this', start=1476.039, duration=4.441), FetchedTranscriptSnippet(text='end of text token later on when we are', start=1478.08, duration=4.28), FetchedTranscriptSnippet(text='actually um pre-training and fine-tuning', start=1480.48, duration=4.64), FetchedTranscriptSnippet(text='the llm so here though the goal is', start=1482.36, duration=4.559), FetchedTranscriptSnippet(text='essentially to extend our simple', start=1485.12, duration=4.48), FetchedTranscriptSnippet(text='tokenizer a little bit to handle these', start=1486.919, duration=4.64), FetchedTranscriptSnippet(text='special tokens and I also wanted to just', start=1489.6, duration=3.959), FetchedTranscriptSnippet(text='show you one shortcoming of our', start=1491.559, duration=4.561), FetchedTranscriptSnippet(text='tokenizer so if I go up here we defined', start=1493.559, duration=5.401), FetchedTranscriptSnippet(text='our simple tokenizer here', start=1496.12, duration=5.0), FetchedTranscriptSnippet(text='based on the vocabulary if I have some', start=1498.96, duration=4.88), FetchedTranscriptSnippet(text='text now that is slightly more I I would', start=1501.12, duration=5.279), FetchedTranscriptSnippet(text='say sophisticated so before we used this', start=1503.84, duration=6.199), FetchedTranscriptSnippet(text=\"text here if I have text let's say um\", start=1506.399, duration=5.64), FetchedTranscriptSnippet(text=\"let's let's do something not super\", start=1510.039, duration=3.721), FetchedTranscriptSnippet(text='sophisticated but something different', start=1512.039, duration=7.961), FetchedTranscriptSnippet(text='like hello maybe do you like T um', start=1513.76, duration=10.919), FetchedTranscriptSnippet(text=\"is I don't know this a test for example\", start=1520.0, duration=7.799), FetchedTranscriptSnippet(text=\"and let's try to see what our tokenizer\", start=1524.679, duration=7.921), FetchedTranscriptSnippet(text='does when we actually um try to encode', start=1527.799, duration=4.801), FetchedTranscriptSnippet(text='this so as you can see there is an error', start=1533.12, duration=6.52), FetchedTranscriptSnippet(text='now it says key error hello and hello is', start=1536.039, duration=5.561), FetchedTranscriptSnippet(text='a very normal word so why would there be', start=1539.64, duration=5.159), FetchedTranscriptSnippet(text='an error and the reason is because the', start=1541.6, duration=6.199), FetchedTranscriptSnippet(text='text that we processed apparently so the', start=1544.799, duration=5.6), FetchedTranscriptSnippet(text='the verdict here apparently it does not', start=1547.799, duration=5.081), FetchedTranscriptSnippet(text='contain the word hello now if you are', start=1550.399, duration=6.561), FetchedTranscriptSnippet(text='creating U yeah an llm and train it on', start=1552.88, duration=6.56), FetchedTranscriptSnippet(text='trillions of tokens that chances are', start=1556.96, duration=4.24), FetchedTranscriptSnippet(text='that this word would be in the training', start=1559.44, duration=4.88), FetchedTranscriptSnippet(text='set um for example here we have only', start=1561.2, duration=4.839), FetchedTranscriptSnippet(text='this very small data set it might not', start=1564.32, duration=3.959), FetchedTranscriptSnippet(text='even contain simple words as um hello', start=1566.039, duration=4.561), FetchedTranscriptSnippet(text='for example and by the way later on I', start=1568.279, duration=4.0), FetchedTranscriptSnippet(text='will also tell you more about an', start=1570.6, duration=4.88), FetchedTranscriptSnippet(text='algorithm that can naturally handle um', start=1572.279, duration=6.081), FetchedTranscriptSnippet(text='unknown words so for example if the word', start=1575.48, duration=4.559), FetchedTranscriptSnippet(text='is unknown it will just break it down', start=1578.36, duration=4.08), FetchedTranscriptSnippet(text='into individual characters so maybe to', start=1580.039, duration=4.76), FetchedTranscriptSnippet(text='show you what I mean so if I type just', start=1582.44, duration=5.08), FetchedTranscriptSnippet(text='some something like this you can see if', start=1584.799, duration=5.0), FetchedTranscriptSnippet(text=\"if it doesn't know word it will um break\", start=1587.52, duration=4.0), FetchedTranscriptSnippet(text='it down into individual characters but', start=1589.799, duration=4.36), FetchedTranscriptSnippet(text='this is a more advanced algorithm for', start=1591.52, duration=6.08), FetchedTranscriptSnippet(text='now how can we actually um you know make', start=1594.159, duration=6.281), FetchedTranscriptSnippet(text='our simple tokenizer slightly more', start=1597.6, duration=4.84), FetchedTranscriptSnippet(text='sophisticated by handling um yeah', start=1600.44, duration=4.64), FetchedTranscriptSnippet(text='unknown words so what we could do is we', start=1602.44, duration=5.44), FetchedTranscriptSnippet(text='can uh extend our tokens so if we have', start=1605.08, duration=5.479), FetchedTranscriptSnippet(text='all tokens uh which were', start=1607.88, duration=5.48), FetchedTranscriptSnippet(text='the list of', start=1610.559, duration=6.84), FetchedTranscriptSnippet(text='the pre-processed text tokens or birds', start=1613.36, duration=6.24), FetchedTranscriptSnippet(text='so this would essent entally recreate', start=1617.399, duration=5.0), FetchedTranscriptSnippet(text='our uh yeah unique tokens from the data', start=1619.6, duration=4.799), FetchedTranscriptSnippet(text='set and now what we could do is we could', start=1622.399, duration=4.041), FetchedTranscriptSnippet(text='actually extend it so if', start=1624.399, duration=4.16), FetchedTranscriptSnippet(text='we', start=1626.44, duration=5.839), FetchedTranscriptSnippet(text='use the extent method on on python lists', start=1628.559, duration=5.401), FetchedTranscriptSnippet(text='we can actually add additional tokens', start=1632.279, duration=4.52), FetchedTranscriptSnippet(text='for example we could end add this end of', start=1633.96, duration=4.64), FetchedTranscriptSnippet(text='text token that I mentioned', start=1636.799, duration=5.401), FetchedTranscriptSnippet(text='earlier or we could also add a token for', start=1638.6, duration=5.199), FetchedTranscriptSnippet(text='unknown words so we can really add', start=1642.2, duration=4.52), FetchedTranscriptSnippet(text='anything we like here so just extending', start=1643.799, duration=5.201), FetchedTranscriptSnippet(text='it and so the difference is to what we', start=1646.72, duration=5.04), FetchedTranscriptSnippet(text='did before is we are yeah creating it', start=1649.0, duration=5.08), FetchedTranscriptSnippet(text='similar to before but now we are adding', start=1651.76, duration=4.799), FetchedTranscriptSnippet(text='tokens that are not already in the data', start=1654.08, duration=5.28), FetchedTranscriptSnippet(text='set and then yeah we would um generate', start=1656.559, duration=6.161), FetchedTranscriptSnippet(text='the vocabulary similar to before so this', start=1659.36, duration=6.0), FetchedTranscriptSnippet(text='is the same um that we used before like', start=1662.72, duration=7.88), FetchedTranscriptSnippet(text='this token um to integer mapping for the', start=1665.36, duration=7.319), FetchedTranscriptSnippet(text=\"vocabulary um and then yeah let's\", start=1670.6, duration=5.16), FetchedTranscriptSnippet(text='actually uh take a look at um the length', start=1672.679, duration=6.281), FetchedTranscriptSnippet(text='of the vocabulary I think before we had', start=1675.76, duration=6.159), FetchedTranscriptSnippet(text='let me scroll up a bit', start=1678.96, duration=7.079), FetchedTranscriptSnippet(text='1,130 now we should have 100', start=1681.919, duration=7.081), FetchedTranscriptSnippet(text=\"1,132 tokens let's double check yep so\", start=1686.039, duration=7.041), FetchedTranscriptSnippet(text='we have these two new tokens added and', start=1689.0, duration=7.96), FetchedTranscriptSnippet(text=\"let's actually print the first um or the\", start=1693.08, duration=5.76), FetchedTranscriptSnippet(text='last in this case the last five entries', start=1696.96, duration=4.12), FetchedTranscriptSnippet(text='we can do the first five they should', start=1698.84, duration=4.92), FetchedTranscriptSnippet(text=\"look similar to what we've seen before\", start=1701.08, duration=5.76), FetchedTranscriptSnippet(text='so just the yeah special characters but', start=1703.76, duration=4.96), FetchedTranscriptSnippet(text=\"more interestingly let's\", start=1706.84, duration=5.079), FetchedTranscriptSnippet(text='actually print the last five tokens in', start=1708.72, duration=5.679), FetchedTranscriptSnippet(text='the vocabulary and we can see we now', start=1711.919, duration=6.201), FetchedTranscriptSnippet(text='have these two new special tokens in the', start=1714.399, duration=6.361), FetchedTranscriptSnippet(text='vocabulary in a similar way we can now', start=1718.12, duration=6.24), FetchedTranscriptSnippet(text='modify our simple tokenizer to actually', start=1720.76, duration=7.32), FetchedTranscriptSnippet(text='use these um special tokens so what we', start=1724.36, duration=6.039), FetchedTranscriptSnippet(text='have to do now is we have to essentially', start=1728.08, duration=5.52), FetchedTranscriptSnippet(text='add a Special Rule yeah to to process', start=1730.399, duration=5.601), FetchedTranscriptSnippet(text='those so I will just um copy and paste', start=1733.6, duration=6.24), FetchedTranscriptSnippet(text='it here so we return this string if the', start=1736.0, duration=8.2), FetchedTranscriptSnippet(text=\"string is um not empty and if it's in\", start=1739.84, duration=6.959), FetchedTranscriptSnippet(text=\"this vocabulary and otherwise if it's\", start=1744.2, duration=5.199), FetchedTranscriptSnippet(text='not in the vocabulary so else we will', start=1746.799, duration=4.841), FetchedTranscriptSnippet(text='return unknown for this unknown token', start=1749.399, duration=4.4), FetchedTranscriptSnippet(text='and this way we will prevent that this', start=1751.64, duration=4.279), FetchedTranscriptSnippet(text='uh modified version of our simple', start=1753.799, duration=6.041), FetchedTranscriptSnippet(text='tokenizer crashes on words such as hello', start=1755.919, duration=5.0), FetchedTranscriptSnippet(text='here for', start=1759.84, duration=3.559), FetchedTranscriptSnippet(text='example yeah and this is all we we do', start=1760.919, duration=4.521), FetchedTranscriptSnippet(text=\"essentially it's a simple modification\", start=1763.399, duration=6.0), FetchedTranscriptSnippet(text='and this should actually help us to run', start=1765.44, duration=8.2), FetchedTranscriptSnippet(text=\"our Simple Text example let's just um\", start=1769.399, duration=6.721), FetchedTranscriptSnippet(text='copy this one for Simplicity so we are', start=1773.64, duration=5.24), FetchedTranscriptSnippet(text='reinitializing the tokenizer now we have', start=1776.12, duration=5.76), FetchedTranscriptSnippet(text=\"tokenizer version two and I will'll make\", start=1778.88, duration=8.12), FetchedTranscriptSnippet(text='a new code cell and then tokenizer', start=1781.88, duration=8.44), FetchedTranscriptSnippet(text='encode text and this hopefully works now', start=1787.0, duration=5.84), FetchedTranscriptSnippet(text='and yeah you can see we now have this', start=1790.32, duration=4.839), FetchedTranscriptSnippet(text='unknown token so actually when we go', start=1792.84, duration=8.28), FetchedTranscriptSnippet(text=\"back let's see what happens so tokenizer\", start=1795.159, duration=5.961), FetchedTranscriptSnippet(text='decode yeah you can see now it handled', start=1801.48, duration=5.799), FetchedTranscriptSnippet(text='uh yeah the unon tokens this is of', start=1805.12, duration=4.64), FetchedTranscriptSnippet(text='course not desirable if we want to', start=1807.279, duration=5.561), FetchedTranscriptSnippet(text='regenerate the original text and later', start=1809.76, duration=5.08), FetchedTranscriptSnippet(text='actually in the next uh sections I will', start=1812.84, duration=4.319), FetchedTranscriptSnippet(text='talk also about an algorithm that will', start=1814.84, duration=5.04), FetchedTranscriptSnippet(text='make this a bit more um sophisticated uh', start=1817.159, duration=4.481), FetchedTranscriptSnippet(text=\"it's called bite pair encoding and I\", start=1819.88, duration=4.56), FetchedTranscriptSnippet(text='will explain how bite pair encoding', start=1821.64, duration=6.8), FetchedTranscriptSnippet(text='handles unknown tokens', start=1824.44, duration=4.0), FetchedTranscriptSnippet(text=\"let's now talk about bite pair encoding\", start=1829.76, duration=4.799), FetchedTranscriptSnippet(text='which is an algorithm that will help us', start=1832.159, duration=5.601), FetchedTranscriptSnippet(text='to take our tokenizer to the next level', start=1834.559, duration=5.12), FetchedTranscriptSnippet(text='so bite pair encoding is an algorithm', start=1837.76, duration=4.279), FetchedTranscriptSnippet(text='that has been around for about 30 years', start=1839.679, duration=4.761), FetchedTranscriptSnippet(text=\"or so but it's yeah it's really popular\", start=1842.039, duration=5.321), FetchedTranscriptSnippet(text='for implementing tokenizers these days', start=1844.44, duration=6.2), FetchedTranscriptSnippet(text='and for example gpt1 gpt2 3 and four', start=1847.36, duration=4.88), FetchedTranscriptSnippet(text='they are all using this bite pair', start=1850.64, duration=4.0), FetchedTranscriptSnippet(text='encoding algorithm for their tokenizer', start=1852.24, duration=4.039), FetchedTranscriptSnippet(text='and even other companies for example', start=1854.64, duration=4.44), FetchedTranscriptSnippet(text='meta AI for their recent 3 models', start=1856.279, duration=5.041), FetchedTranscriptSnippet(text=\"they're using bite par en coding and so\", start=1859.08, duration=4.04), FetchedTranscriptSnippet(text='bite par en coding really helps us', start=1861.32, duration=4.359), FetchedTranscriptSnippet(text='addressing one major shortcoming of our', start=1863.12, duration=5.48), FetchedTranscriptSnippet(text='tokenizer so if I go up again a few um', start=1865.679, duration=5.441), FetchedTranscriptSnippet(text='SS here one of the shortcomings of our', start=1868.6, duration=5.0), FetchedTranscriptSnippet(text=\"simple tokenizer was that that it wasn't\", start=1871.12, duration=5.32), FetchedTranscriptSnippet(text='able to handle unknown words so for', start=1873.6, duration=5.079), FetchedTranscriptSnippet(text='example in this case we had this word', start=1876.44, duration=4.52), FetchedTranscriptSnippet(text='hello and since hello was not', start=1878.679, duration=4.441), FetchedTranscriptSnippet(text='represented in the training data it', start=1880.96, duration=5.16), FetchedTranscriptSnippet(text='would raise a key error and so the way', start=1883.12, duration=6.6), FetchedTranscriptSnippet(text='we handled that was that we yeah', start=1886.12, duration=7.24), FetchedTranscriptSnippet(text='replaced um unknown tokens um by the', start=1889.72, duration=6.0), FetchedTranscriptSnippet(text='unknown placeholder token here so we had', start=1893.36, duration=4.96), FetchedTranscriptSnippet(text='this Special Rule here to replace', start=1895.72, duration=4.48), FetchedTranscriptSnippet(text='anything that is not contained in the', start=1898.32, duration=4.04), FetchedTranscriptSnippet(text=\"vocabulary so anything that it hasn't\", start=1900.2, duration=4.12), FetchedTranscriptSnippet(text='seen during the training by these', start=1902.36, duration=5.0), FetchedTranscriptSnippet(text='so-called unknown special tokens so what', start=1904.32, duration=4.76), FetchedTranscriptSnippet(text='happens is for example if we have some', start=1907.36, duration=5.08), FetchedTranscriptSnippet(text='text like this here so these uh unknown', start=1909.08, duration=6.559), FetchedTranscriptSnippet(text='words get replaced by Unk placeholder', start=1912.44, duration=5.92), FetchedTranscriptSnippet(text='tokens and this one as well but the main', start=1915.639, duration=5.681), FetchedTranscriptSnippet(text='shortcoming of this is now to the llm', start=1918.36, duration=4.72), FetchedTranscriptSnippet(text='this all looks the same so the LM', start=1921.32, duration=3.64), FetchedTranscriptSnippet(text=\"doesn't really know what's here and\", start=1923.08, duration=4.12), FetchedTranscriptSnippet(text=\"what's here because it receives the same\", start=1924.96, duration=5.439), FetchedTranscriptSnippet(text='token um yeah ID for both of these cases', start=1927.2, duration=5.599), FetchedTranscriptSnippet(text='and in many many um real world task', start=1930.399, duration=4.16), FetchedTranscriptSnippet(text='where you have um specific names or', start=1932.799, duration=3.161), FetchedTranscriptSnippet(text='anything that were not included in the', start=1934.559, duration=3.521), FetchedTranscriptSnippet(text=\"training data the LM wouldn't have any\", start=1935.96, duration=3.959), FetchedTranscriptSnippet(text='idea what you are talking about or what', start=1938.08, duration=3.839), FetchedTranscriptSnippet(text=\"you're referring to if you have multiple\", start=1939.919, duration=3.801), FetchedTranscriptSnippet(text='of these unknown tokens in your text and', start=1941.919, duration=4.401), FetchedTranscriptSnippet(text='so forth and yeah this is not ideal so', start=1943.72, duration=4.64), FetchedTranscriptSnippet(text='the bite pair encoding algorithm', start=1946.32, duration=4.64), FetchedTranscriptSnippet(text='is a way that it can always break down', start=1948.36, duration=6.84), FetchedTranscriptSnippet(text='any type of um yeah word into sub tokens', start=1950.96, duration=6.24), FetchedTranscriptSnippet(text=\"so to show you an example let's go\", start=1955.2, duration=4.359), FetchedTranscriptSnippet(text='briefly to this um Tik to tokenizer app', start=1957.2, duration=4.24), FetchedTranscriptSnippet(text='again and we will be in this video also', start=1959.559, duration=4.12), FetchedTranscriptSnippet(text='using the Tik token app or sorry python', start=1961.44, duration=4.479), FetchedTranscriptSnippet(text='library in a few moments but if I type', start=1963.679, duration=5.561), FetchedTranscriptSnippet(text=\"some let's say some unknown word and\", start=1965.919, duration=5.961), FetchedTranscriptSnippet(text='then something like this here you can', start=1969.24, duration=5.88), FetchedTranscriptSnippet(text=\"see it's actually breaking it down into\", start=1971.88, duration=6.519), FetchedTranscriptSnippet(text='subword so uh what happens here', start=1975.12, duration=5.799), FetchedTranscriptSnippet(text=\"maybe let's fix this it's even nicer so\", start=1978.399, duration=5.561), FetchedTranscriptSnippet(text='what um what happens here is that I bet', start=1980.919, duration=5.161), FetchedTranscriptSnippet(text='you that this is something that was not', start=1983.96, duration=4.439), FetchedTranscriptSnippet(text='contained in the training data for gpd2', start=1986.08, duration=4.479), FetchedTranscriptSnippet(text=\"but nonetheless it's able to break it\", start=1988.399, duration=4.561), FetchedTranscriptSnippet(text='down into individual tokens without', start=1990.559, duration=4.84), FetchedTranscriptSnippet(text='failing so without raising an error or', start=1992.96, duration=4.719), FetchedTranscriptSnippet(text='without yeah inserting a placeholder', start=1995.399, duration=4.201), FetchedTranscriptSnippet(text='token or anything like that so what', start=1997.679, duration=4.321), FetchedTranscriptSnippet(text=\"happens is it's breaking down longer\", start=1999.6, duration=5.24), FetchedTranscriptSnippet(text='unknown words into known subwords so it', start=2002.0, duration=4.88), FetchedTranscriptSnippet(text='has for example seen something like sum', start=2004.84, duration=4.04), FetchedTranscriptSnippet(text='in its training set or unknown so these', start=2006.88, duration=4.799), FetchedTranscriptSnippet(text='are unknown words so you can see here', start=2008.88, duration=4.519), FetchedTranscriptSnippet(text=\"it's breaking them down successfully\", start=2011.679, duration=3.88), FetchedTranscriptSnippet(text='into individual sub tokens and then if', start=2013.399, duration=4.76), FetchedTranscriptSnippet(text='we have a string like this um this is', start=2015.559, duration=4.161), FetchedTranscriptSnippet(text=\"something that doesn't really correspond\", start=2018.159, duration=3.441), FetchedTranscriptSnippet(text='to anything in the training data it', start=2019.72, duration=3.72), FetchedTranscriptSnippet(text='would then always fall back to', start=2021.6, duration=3.88), FetchedTranscriptSnippet(text='individual characters so in this case it', start=2023.44, duration=4.119), FetchedTranscriptSnippet(text='will just use a single character as a', start=2025.48, duration=4.559), FetchedTranscriptSnippet(text='token so in this way it will never fail', start=2027.559, duration=4.24), FetchedTranscriptSnippet(text='because it will always fall back to', start=2030.039, duration=3.24), FetchedTranscriptSnippet(text=\"these individual characters if there's\", start=2031.799, duration=3.641), FetchedTranscriptSnippet(text='an unknown word now this is maybe a bit', start=2033.279, duration=5.201), FetchedTranscriptSnippet(text='inefficient because now one word becomes', start=2035.44, duration=5.44), FetchedTranscriptSnippet(text=\"uh a lot of tokens uh so it's not the\", start=2038.48, duration=5.039), FetchedTranscriptSnippet(text='most efficient way to encode input text', start=2040.88, duration=4.6), FetchedTranscriptSnippet(text='if you have a lot of unknown words', start=2043.519, duration=4.12), FetchedTranscriptSnippet(text='however it will never fail in this case', start=2045.48, duration=6.24), FetchedTranscriptSnippet(text='so this is one uh improvement over our', start=2047.639, duration=6.24), FetchedTranscriptSnippet(text='previous yeah method which would just', start=2051.72, duration=4.84), FetchedTranscriptSnippet(text='replace unknown tokens with this Unk', start=2053.879, duration=4.561), FetchedTranscriptSnippet(text='placeholder', start=2056.56, duration=5.64), FetchedTranscriptSnippet(text=\"token um so if you're interested in the\", start=2058.44, duration=5.6), FetchedTranscriptSnippet(text='bite pair encoding algorithm how it', start=2062.2, duration=5.0), FetchedTranscriptSnippet(text='works so open AI they actually open', start=2064.04, duration=6.4), FetchedTranscriptSnippet(text='sourced uh yeah the inference method for', start=2067.2, duration=5.32), FetchedTranscriptSnippet(text='this bite pair encoding algorithm so', start=2070.44, duration=5.84), FetchedTranscriptSnippet(text='they have this gpt2 GitHub repository so', start=2072.52, duration=6.52), FetchedTranscriptSnippet(text='um it should be just open gpd2 and then', start=2076.28, duration=5.879), FetchedTranscriptSnippet(text='in this folder SRC and then encoder they', start=2079.04, duration=5.28), FetchedTranscriptSnippet(text='have the bite pair encoding implemented', start=2082.159, duration=5.92), FetchedTranscriptSnippet(text='here to break down yeah uh words or', start=2084.32, duration=6.319), FetchedTranscriptSnippet(text='input text into these sub tokens now', start=2088.079, duration=4.84), FetchedTranscriptSnippet(text=\"they don't share the training method for\", start=2090.639, duration=5.72), FetchedTranscriptSnippet(text='this so they only share um the code that', start=2092.919, duration=5.561), FetchedTranscriptSnippet(text='you use when you have already trained', start=2096.359, duration=4.48), FetchedTranscriptSnippet(text=\"this um yeah tokenizer however if you're\", start=2098.48, duration=5.56), FetchedTranscriptSnippet(text='curious I implemented this from scratch', start=2100.839, duration=6.121), FetchedTranscriptSnippet(text='so if you scroll down here in the bonus', start=2104.04, duration=4.76), FetchedTranscriptSnippet(text='material of the book so this is my', start=2106.96, duration=7.159), FetchedTranscriptSnippet(text='repository rsbt lm- from- scratch um and', start=2108.8, duration=7.48), FetchedTranscriptSnippet(text='then you go to the spite pair encoding', start=2114.119, duration=5.081), FetchedTranscriptSnippet(text='tokenizer from scratch link this will', start=2116.28, duration=4.44), FetchedTranscriptSnippet(text='bring you to a notebook a jupyter', start=2119.2, duration=4.0), FetchedTranscriptSnippet(text='notebook where um this implements the', start=2120.72, duration=4.8), FetchedTranscriptSnippet(text='bite pair encoding um yeah step by step', start=2123.2, duration=5.32), FetchedTranscriptSnippet(text='from scratch including training', start=2125.52, duration=5.599), FetchedTranscriptSnippet(text='um and then also at the end I have a', start=2128.52, duration=5.72), FetchedTranscriptSnippet(text='method for loading the open eii weights', start=2131.119, duration=6.321), FetchedTranscriptSnippet(text='into into this so This should work', start=2134.24, duration=6.76), FetchedTranscriptSnippet(text='similarly to what open AI um used for', start=2137.44, duration=6.44), FetchedTranscriptSnippet(text='yeah for the gpd2 model so if you are', start=2141.0, duration=5.44), FetchedTranscriptSnippet(text='interested in more details um yeah I', start=2143.88, duration=5.0), FetchedTranscriptSnippet(text='recommend checking this out but no know', start=2146.44, duration=3.879), FetchedTranscriptSnippet(text='that this is only optional this is', start=2148.88, duration=3.84), FetchedTranscriptSnippet(text='pretty Advanced and this is also a very', start=2150.319, duration=4.28), FetchedTranscriptSnippet(text=\"long notebook which is why I didn't\", start=2152.72, duration=4.0), FetchedTranscriptSnippet(text='include it in the book itself because', start=2154.599, duration=3.641), FetchedTranscriptSnippet(text='yeah in in practice the the book is', start=2156.72, duration=3.76), FetchedTranscriptSnippet(text='about llms and this is a whole different', start=2158.24, duration=4.0), FetchedTranscriptSnippet(text='topic this could be actually the topic', start=2160.48, duration=3.28), FetchedTranscriptSnippet(text=\"of a whole other book because it's\", start=2162.24, duration=4.28), FetchedTranscriptSnippet(text='actually quite complex so the main', start=2163.76, duration=5.599), FetchedTranscriptSnippet(text='message here really is that we are going', start=2166.52, duration=6.0), FetchedTranscriptSnippet(text='to take text un unknown tokens and', start=2169.359, duration=5.641), FetchedTranscriptSnippet(text='everything and we have now a way to', start=2172.52, duration=4.68), FetchedTranscriptSnippet(text='break it down into individual sub tokens', start=2175.0, duration=5.44), FetchedTranscriptSnippet(text='with this bpe method and in practice um', start=2177.2, duration=5.399), FetchedTranscriptSnippet(text='we are going to use the library called', start=2180.44, duration=3.84), FetchedTranscriptSnippet(text='tick token which is an open source', start=2182.599, duration=5.161), FetchedTranscriptSnippet(text='library from openi so the book um the', start=2184.28, duration=5.52), FetchedTranscriptSnippet(text='chapters will Implement everything from', start=2187.76, duration=3.64), FetchedTranscriptSnippet(text=\"scratch because yeah that's the title of\", start=2189.8, duration=3.48), FetchedTranscriptSnippet(text='the book build a large language model', start=2191.4, duration=3.64), FetchedTranscriptSnippet(text='from scratch but I would consider the', start=2193.28, duration=4.76), FetchedTranscriptSnippet(text='tokenizer not really part of the llm', start=2195.04, duration=6.079), FetchedTranscriptSnippet(text='itself so in this case um the tokenizer', start=2198.04, duration=5.52), FetchedTranscriptSnippet(text='here we are going to use this library', start=2201.119, duration=3.881), FetchedTranscriptSnippet(text=\"from Tik token because it's actually\", start=2203.56, duration=3.2), FetchedTranscriptSnippet(text=\"also very nice and efficient and it's\", start=2205.0, duration=4.52), FetchedTranscriptSnippet(text='what most people use in practice um so', start=2206.76, duration=4.24), FetchedTranscriptSnippet(text=\"the reason why it's efficient for\", start=2209.52, duration=3.0), FetchedTranscriptSnippet(text='example more efficient than this', start=2211.0, duration=3.839), FetchedTranscriptSnippet(text='implementation here um this other', start=2212.52, duration=5.28), FetchedTranscriptSnippet(text='implementation here from openi is that', start=2214.839, duration=4.921), FetchedTranscriptSnippet(text='yeah the the most expensive function', start=2217.8, duration=4.799), FetchedTranscriptSnippet(text='called or code is implemented in Rust', start=2219.76, duration=4.8), FetchedTranscriptSnippet(text='where rust is a different um yeah', start=2222.599, duration=4.24), FetchedTranscriptSnippet(text='programming language that is often used', start=2224.56, duration=4.96), FetchedTranscriptSnippet(text='to implement high performance code and', start=2226.839, duration=4.641), FetchedTranscriptSnippet(text='then to make it um yeah easy to use', start=2229.52, duration=4.599), FetchedTranscriptSnippet(text='people usually add a python API around', start=2231.48, duration=4.4), FetchedTranscriptSnippet(text=\"this so we're going to use this library\", start=2234.119, duration=4.681), FetchedTranscriptSnippet(text='in Python but most of the core parts are', start=2235.88, duration=5.12), FetchedTranscriptSnippet(text='implemented in Rust to make it really', start=2238.8, duration=5.08), FetchedTranscriptSnippet(text=\"fast and inference so what we're going\", start=2241.0, duration=4.839), FetchedTranscriptSnippet(text=\"to do is we're going to implement or\", start=2243.88, duration=6.239), FetchedTranscriptSnippet(text='sorry to import the tick token', start=2245.839, duration=8.0), FetchedTranscriptSnippet(text='Library oh here we go so the Tik token', start=2250.119, duration=5.761), FetchedTranscriptSnippet(text='library is um yeah helping us then to', start=2253.839, duration=5.081), FetchedTranscriptSnippet(text='use this gpt2 tokenizer and by the way', start=2255.88, duration=5.8), FetchedTranscriptSnippet(text=\"why gpt2 and not gpt3 or four that's\", start=2258.92, duration=4.76), FetchedTranscriptSnippet(text='because later in chapter 5 we will be', start=2261.68, duration=5.36), FetchedTranscriptSnippet(text='training a small gpd2 style model and', start=2263.68, duration=5.6), FetchedTranscriptSnippet(text='then we will also later see how we can', start=2267.04, duration=5.0), FetchedTranscriptSnippet(text='laot the pre-trained weights from OPI', start=2269.28, duration=4.559), FetchedTranscriptSnippet(text='who shared the gpd2 model weights', start=2272.04, duration=3.96), FetchedTranscriptSnippet(text=\"Unfortunately they didn't share the gpt3\", start=2273.839, duration=4.161), FetchedTranscriptSnippet(text='or gp4 model weights yet', start=2276.0, duration=3.92), FetchedTranscriptSnippet(text='um but the architecture of GPD 3 is the', start=2278.0, duration=4.8), FetchedTranscriptSnippet(text='same as two and so forth in in any case', start=2279.92, duration=5.679), FetchedTranscriptSnippet(text='so we are going to use the gpd2', start=2282.8, duration=5.16), FetchedTranscriptSnippet(text='tokenizer to make it compatible to the', start=2285.599, duration=4.841), FetchedTranscriptSnippet(text='model we are going to implement later in', start=2287.96, duration=5.76), FetchedTranscriptSnippet(text='chapter 5 and um so yeah we can import', start=2290.44, duration=5.84), FetchedTranscriptSnippet(text='The Tick token Library like this if you', start=2293.72, duration=4.68), FetchedTranscriptSnippet(text='have yeah um set up your python', start=2296.28, duration=3.96), FetchedTranscriptSnippet(text='environment as described in the first', start=2298.4, duration=4.719), FetchedTranscriptSnippet(text='video with this requirements. text file', start=2300.24, duration=5.04), FetchedTranscriptSnippet(text='this should um automatically work if you', start=2303.119, duration=4.441), FetchedTranscriptSnippet(text='get an import error that means you', start=2305.28, duration=4.48), FetchedTranscriptSnippet(text=\"probably haven't installed the Tik token\", start=2307.56, duration=4.4), FetchedTranscriptSnippet(text='python Library so you could either use', start=2309.76, duration=3.4), FetchedTranscriptSnippet(text='UV pip', start=2311.96, duration=4.92), FetchedTranscriptSnippet(text='install or just um pip install uh Tik', start=2313.16, duration=6.199), FetchedTranscriptSnippet(text='token to install um yeah this Library so', start=2316.88, duration=4.6), FetchedTranscriptSnippet(text='if you would execute this code cell here', start=2319.359, duration=4.72), FetchedTranscriptSnippet(text='this would install the Tik token Library', start=2321.48, duration=4.52), FetchedTranscriptSnippet(text=\"so but I'm commenting it out because I\", start=2324.079, duration=5.361), FetchedTranscriptSnippet(text='have it already installed and so next um', start=2326.0, duration=5.72), FetchedTranscriptSnippet(text='let me also just briefly double check', start=2329.44, duration=4.08), FetchedTranscriptSnippet(text=\"which version I'm using\", start=2331.72, duration=4.599), FetchedTranscriptSnippet(text=\"so it's always a habit of mine to double\", start=2333.52, duration=4.44), FetchedTranscriptSnippet(text='check CU yeah you never never know if', start=2336.319, duration=3.361), FetchedTranscriptSnippet(text='you run the notebook in a few years in', start=2337.96, duration=3.96), FetchedTranscriptSnippet(text='the future maybe if you get a different', start=2339.68, duration=3.88), FetchedTranscriptSnippet(text='result it could be due to a different', start=2341.92, duration=4.8), FetchedTranscriptSnippet(text=\"version of the library so I'm using 0.9\", start=2343.56, duration=4.559), FetchedTranscriptSnippet(text='but it should also be fine if you have', start=2346.72, duration=4.24), FetchedTranscriptSnippet(text='an older version like 0.7 or 6 or', start=2348.119, duration=4.801), FetchedTranscriptSnippet(text='something like that but yeah if you get', start=2350.96, duration=3.639), FetchedTranscriptSnippet(text=\"some weird results that don't really\", start=2352.92, duration=4.399), FetchedTranscriptSnippet(text='match what you expect um making sure you', start=2354.599, duration=4.24), FetchedTranscriptSnippet(text='have the correct version is never a bad', start=2357.319, duration=3.76), FetchedTranscriptSnippet(text='idea so you could then also U fix this', start=2358.839, duration=4.601), FetchedTranscriptSnippet(text='version number like this for', start=2361.079, duration=4.961), FetchedTranscriptSnippet(text='example okay so how are we going to use', start=2363.44, duration=4.48), FetchedTranscriptSnippet(text='the Tik tokenizer now so we we going to', start=2366.04, duration=5.12), FetchedTranscriptSnippet(text='instantiate a new tokenizer object and', start=2367.92, duration=5.679), FetchedTranscriptSnippet(text='um we are going to use the gpt2', start=2371.16, duration=4.439), FetchedTranscriptSnippet(text='tokenizer like I mentioned before so we', start=2373.599, duration=7.041), FetchedTranscriptSnippet(text='do um get encoding and then gpd2 like', start=2375.599, duration=8.24), FetchedTranscriptSnippet(text='this and this would instantiate the gpd2', start=2380.64, duration=6.439), FetchedTranscriptSnippet(text='tokenizer with the full vocabulary the', start=2383.839, duration=4.721), FetchedTranscriptSnippet(text='way it was trained and everything so', start=2387.079, duration=3.28), FetchedTranscriptSnippet(text=\"this is already ready to use we don't\", start=2388.56, duration=4.68), FetchedTranscriptSnippet(text='have to train anything and then the', start=2390.359, duration=8.041), FetchedTranscriptSnippet(text='usage is is exactly the same as our own', start=2393.24, duration=7.599), FetchedTranscriptSnippet(text='simple tokenizer this is why we went', start=2398.4, duration=4.56), FetchedTranscriptSnippet(text='through all the work here um', start=2400.839, duration=4.201), FetchedTranscriptSnippet(text='implementing this because it helps I', start=2402.96, duration=4.56), FetchedTranscriptSnippet(text='think illustrate how encode and decode', start=2405.04, duration=5.24), FetchedTranscriptSnippet(text='work like these methods and this yeah', start=2407.52, duration=5.96), FetchedTranscriptSnippet(text='tick tokenizer is using exactly the same', start=2410.28, duration=4.559), FetchedTranscriptSnippet(text='um yeah the', start=2413.48, duration=4.639), FetchedTranscriptSnippet(text='same API so you can see we are encoding', start=2414.839, duration=5.881), FetchedTranscriptSnippet(text='this now and then there is', start=2418.119, duration=5.041), FetchedTranscriptSnippet(text='also um', start=2420.72, duration=5.2), FetchedTranscriptSnippet(text='tokenizer', start=2423.16, duration=5.32), FetchedTranscriptSnippet(text='decode like this and this gives us back', start=2425.92, duration=4.159), FetchedTranscriptSnippet(text='the original text which is exactly what', start=2428.48, duration=4.92), FetchedTranscriptSnippet(text='we had before now let me actually show', start=2430.079, duration=4.721), FetchedTranscriptSnippet(text='you', start=2433.4, duration=3.88), FetchedTranscriptSnippet(text='um a more advanced case where I have', start=2434.8, duration=4.84), FetchedTranscriptSnippet(text='some copy pasted some text here and', start=2437.28, duration=4.6), FetchedTranscriptSnippet(text=\"let's\", start=2439.64, duration=5.0), FetchedTranscriptSnippet(text='actually try to encode this text so', start=2441.88, duration=5.479), FetchedTranscriptSnippet(text='before I mentioned to you that it can', start=2444.64, duration=5.719), FetchedTranscriptSnippet(text='handle um arbitrary words so it is', start=2447.359, duration=5.681), FetchedTranscriptSnippet(text='actually uh able to you know break down', start=2450.359, duration=5.121), FetchedTranscriptSnippet(text=\"any words into sub tokens so let's see\", start=2453.04, duration=5.92), FetchedTranscriptSnippet(text='actually what happens', start=2455.48, duration=3.48), FetchedTranscriptSnippet(text='so you may notice now that actually it', start=2459.0, duration=5.24), FetchedTranscriptSnippet(text='raises an error and why is that so there', start=2461.4, duration=4.679), FetchedTranscriptSnippet(text='is one interesting thing about the gbd', start=2464.24, duration=4.72), FetchedTranscriptSnippet(text='to tokenizer and so one is that they use', start=2466.079, duration=5.801), FetchedTranscriptSnippet(text='the so-called special end of text token', start=2468.96, duration=5.0), FetchedTranscriptSnippet(text='and I think I glanced over this in the', start=2471.88, duration=5.76), FetchedTranscriptSnippet(text='previous videos but the end of text', start=2473.96, duration=6.6), FetchedTranscriptSnippet(text='token is usually used if I can find the', start=2477.64, duration=5.479), FetchedTranscriptSnippet(text='figure here um it is usually used to', start=2480.56, duration=8.08), FetchedTranscriptSnippet(text='concatenate texts um let me double check', start=2483.119, duration=7.921), FetchedTranscriptSnippet(text='yeah here so when you are preparing the', start=2488.64, duration=5.04), FetchedTranscriptSnippet(text='data set for training an llm you usually', start=2491.04, duration=6.2), FetchedTranscriptSnippet(text='have multiple documents and to yeah to', start=2493.68, duration=5.76), FetchedTranscriptSnippet(text='signal to the llm where one document', start=2497.24, duration=4.48), FetchedTranscriptSnippet(text='ends and where the next document starts', start=2499.44, duration=4.639), FetchedTranscriptSnippet(text='you usually add this end of text token', start=2501.72, duration=4.44), FetchedTranscriptSnippet(text='you can either add it at the beginning', start=2504.079, duration=4.121), FetchedTranscriptSnippet(text='of the next document or at the end of', start=2506.16, duration=3.32), FetchedTranscriptSnippet(text=\"the previous document that doesn't\", start=2508.2, duration=3.119), FetchedTranscriptSnippet(text='matter but you usually have this end of', start=2509.48, duration=4.72), FetchedTranscriptSnippet(text='text the limiter to show where a new', start=2511.319, duration=5.241), FetchedTranscriptSnippet(text='document starts so that the llm knows', start=2514.2, duration=3.879), FetchedTranscriptSnippet(text=\"okay this is a new new document now it's\", start=2516.56, duration=4.16), FetchedTranscriptSnippet(text='not part of the previous document and', start=2518.079, duration=6.361), FetchedTranscriptSnippet(text='this is also used in gbt2 for example uh', start=2520.72, duration=5.879), FetchedTranscriptSnippet(text=\"and it's a special token that they used\", start=2524.44, duration=4.32), FetchedTranscriptSnippet(text=\"but it's not by default enabled in this\", start=2526.599, duration=4.52), FetchedTranscriptSnippet(text=\"tokenizer which is why it's complaining\", start=2528.76, duration=4.72), FetchedTranscriptSnippet(text='so what you have to do is you have to', start=2531.119, duration=4.921), FetchedTranscriptSnippet(text='unfortunately um yeah add this as a', start=2533.48, duration=4.879), FetchedTranscriptSnippet(text='special token so you have to be very', start=2536.04, duration=5.64), FetchedTranscriptSnippet(text='explicit here and then add this um', start=2538.359, duration=5.72), FetchedTranscriptSnippet(text='special', start=2541.68, duration=2.399), FetchedTranscriptSnippet(text='token like this and yeah you can see', start=2544.319, duration=5.641), FetchedTranscriptSnippet(text='this one works now and so this um end of', start=2547.319, duration=7.321), FetchedTranscriptSnippet(text='text um corresponds to 50,2 56 which is', start=2549.96, duration=7.639), FetchedTranscriptSnippet(text='essentially the vocabulary size of this', start=2554.64, duration=6.64), FetchedTranscriptSnippet(text='um tokenizer so the vocabulary has 50,2', start=2557.599, duration=6.96), FetchedTranscriptSnippet(text='56 entries in this case um and so one', start=2561.28, duration=5.079), FetchedTranscriptSnippet(text='more thing I wanted to mention is it', start=2564.559, duration=4.56), FetchedTranscriptSnippet(text=\"might look like it's yeah not working\", start=2566.359, duration=5.521), FetchedTranscriptSnippet(text='well with things that is not denoted as', start=2569.119, duration=4.561), FetchedTranscriptSnippet(text='a load special but it would work with', start=2571.88, duration=3.64), FetchedTranscriptSnippet(text='any other token so I can just you know', start=2573.68, duration=3.919), FetchedTranscriptSnippet(text='add arbitrary text here and it would', start=2575.52, duration=4.839), FetchedTranscriptSnippet(text='work like this so uh like we said before', start=2577.599, duration=5.081), FetchedTranscriptSnippet(text='it can break down arbitrary text it was', start=2580.359, duration=4.48), FetchedTranscriptSnippet(text='just an exception with this um end of', start=2582.68, duration=4.399), FetchedTranscriptSnippet(text='text token and yeah and so this is in a', start=2584.839, duration=4.561), FetchedTranscriptSnippet(text='nutshell how the bite pair encoding', start=2587.079, duration=3.881), FetchedTranscriptSnippet(text=\"algorithm works so it's essentially a\", start=2589.4, duration=4.439), FetchedTranscriptSnippet(text='method for breaking down arbitrary text', start=2590.96, duration=4.599), FetchedTranscriptSnippet(text='into subware tokens and if you are', start=2593.839, duration=4.24), FetchedTranscriptSnippet(text=\"curious about more details how it's\", start=2595.559, duration=5.8), FetchedTranscriptSnippet(text='implemented I highly recommend um not', start=2598.079, duration=5.76), FetchedTranscriptSnippet(text='highly but I suggest checking out this', start=2601.359, duration=4.401), FetchedTranscriptSnippet(text='source code I would say I would not', start=2603.839, duration=3.361), FetchedTranscriptSnippet(text='highly recommend it at this point', start=2605.76, duration=3.799), FetchedTranscriptSnippet(text=\"because it's pretty Advanced and it can\", start=2607.2, duration=4.44), FetchedTranscriptSnippet(text='you know it can be a bit confusing and', start=2609.559, duration=4.0), FetchedTranscriptSnippet(text='yeah if you like you can also check out', start=2611.64, duration=4.64), FetchedTranscriptSnippet(text='my my implementation here where I added', start=2613.559, duration=4.56), FetchedTranscriptSnippet(text='a few more comments and hopefully making', start=2616.28, duration=3.92), FetchedTranscriptSnippet(text='this a bit more readable but yeah again', start=2618.119, duration=4.801), FetchedTranscriptSnippet(text='this is Advanced material um I would say', start=2620.2, duration=4.28), FetchedTranscriptSnippet(text='this would be almost topic for another', start=2622.92, duration=3.76), FetchedTranscriptSnippet(text=\"book it's not really anything you have\", start=2624.48, duration=4.079), FetchedTranscriptSnippet(text='to necessarily understand for', start=2626.68, duration=4.399), FetchedTranscriptSnippet(text='implementing an llm the core message is', start=2628.559, duration=4.681), FetchedTranscriptSnippet(text='we have now an algorithm that breaks', start=2631.079, duration=5.641), FetchedTranscriptSnippet(text='down text into tokens and in the next um', start=2633.24, duration=6.319), FetchedTranscriptSnippet(text='yeah we will talk about um implementing', start=2636.72, duration=5.28), FetchedTranscriptSnippet(text='the data', start=2639.559, duration=2.441), FetchedTranscriptSnippet(text=\"sample let's now talk about data\", start=2642.4, duration=5.159), FetchedTranscriptSnippet(text='sampling with a sliding window so what', start=2645.079, duration=5.28), FetchedTranscriptSnippet(text='that means is previously we talked about', start=2647.559, duration=5.401), FetchedTranscriptSnippet(text='creating these subw word tokens from a', start=2650.359, duration=5.161), FetchedTranscriptSnippet(text='given text and then we converted the', start=2652.96, duration=5.08), FetchedTranscriptSnippet(text='subtext tokens into token IDs and later', start=2655.52, duration=5.0), FetchedTranscriptSnippet(text='we will learn how to convert them into', start=2658.04, duration=4.72), FetchedTranscriptSnippet(text='vectors embedding vectors and then pass', start=2660.52, duration=4.88), FetchedTranscriptSnippet(text='them to the L&M but for now the focus is', start=2662.76, duration=5.24), FetchedTranscriptSnippet(text='really still on this part here how can', start=2665.4, duration=5.08), FetchedTranscriptSnippet(text='we do this efficiently for instance the', start=2668.0, duration=5.319), FetchedTranscriptSnippet(text=\"llm it can't receive all the tokens all\", start=2670.48, duration=6.0), FetchedTranscriptSnippet(text='at once as input so the the topic of the', start=2673.319, duration=6.201), FetchedTranscriptSnippet(text='section is how do we provide smaller', start=2676.48, duration=6.079), FetchedTranscriptSnippet(text='chunks of these token IDs to the llm', start=2679.52, duration=5.52), FetchedTranscriptSnippet(text='efficiently so that we can later train', start=2682.559, duration=6.081), FetchedTranscriptSnippet(text=\"the llm efficiently so let's go to\", start=2685.04, duration=6.559), FetchedTranscriptSnippet(text=\"section 2.6 here um there's one more\", start=2688.64, duration=4.679), FetchedTranscriptSnippet(text='thing I wanted to mention before we go', start=2691.599, duration=4.76), FetchedTranscriptSnippet(text='to the code examples llms essentially', start=2693.319, duration=5.201), FetchedTranscriptSnippet(text='one of the home marks of llms is that', start=2696.359, duration=4.48), FetchedTranscriptSnippet(text='they are predicting one token at a time', start=2698.52, duration=4.799), FetchedTranscriptSnippet(text='so if we have an input text for example', start=2700.839, duration=5.0), FetchedTranscriptSnippet(text='llms learn to predict one word at a time', start=2703.319, duration=5.0), FetchedTranscriptSnippet(text=\"if that is our input text let's consider\", start=2705.839, duration=4.48), FetchedTranscriptSnippet(text='the first token if we pass only this', start=2708.319, duration=4.361), FetchedTranscriptSnippet(text='token ideally the nlm should yeah', start=2710.319, duration=4.561), FetchedTranscriptSnippet(text='predict the next token and here in the', start=2712.68, duration=5.6), FetchedTranscriptSnippet(text='next row LMS learn the next token is two', start=2714.88, duration=6.32), FetchedTranscriptSnippet(text='and so forth so the goal is um yeah to', start=2718.28, duration=5.12), FetchedTranscriptSnippet(text='teach the llm to predict one work at a', start=2721.2, duration=4.6), FetchedTranscriptSnippet(text='time and um this is one of the', start=2723.4, duration=4.52), FetchedTranscriptSnippet(text='interesting things about LMS this makes', start=2725.8, duration=5.64), FetchedTranscriptSnippet(text='them so efficient at scale in terms of', start=2727.92, duration=6.159), FetchedTranscriptSnippet(text='being able to train them on very large', start=2731.44, duration=5.28), FetchedTranscriptSnippet(text='data sets in traditional machine', start=2734.079, duration=5.801), FetchedTranscriptSnippet(text='learning we usually as humans had to', start=2736.72, duration=6.599), FetchedTranscriptSnippet(text='find ways to create labeled data so here', start=2739.88, duration=5.16), FetchedTranscriptSnippet(text=\"it's actually very easy because we can\", start=2743.319, duration=4.881), FetchedTranscriptSnippet(text='use raw text and the label creates', start=2745.04, duration=5.96), FetchedTranscriptSnippet(text='Itself by just being the next token so', start=2748.2, duration=5.919), FetchedTranscriptSnippet(text='we are just taking text we hide part of', start=2751.0, duration=6.079), FetchedTranscriptSnippet(text='the text um for example here we hide', start=2754.119, duration=5.2), FetchedTranscriptSnippet(text='this text we provide one token as input', start=2757.079, duration=4.121), FetchedTranscriptSnippet(text='and then the next token is the target', start=2759.319, duration=4.681), FetchedTranscriptSnippet(text='label that the llm learns to predict we', start=2761.2, duration=5.28), FetchedTranscriptSnippet(text='will learn more about how this works for', start=2764.0, duration=4.319), FetchedTranscriptSnippet(text='example optimizing a loss function', start=2766.48, duration=4.04), FetchedTranscriptSnippet(text='predicting next tokens and so forth in', start=2768.319, duration=5.561), FetchedTranscriptSnippet(text='later chapters here the goal is to just', start=2770.52, duration=6.039), FetchedTranscriptSnippet(text='prepare the data in in a similar format', start=2773.88, duration=5.6), FetchedTranscriptSnippet(text='so that when we pass an input to the llm', start=2776.559, duration=5.081), FetchedTranscriptSnippet(text='the llm has then the next Target that it', start=2779.48, duration=3.52), FetchedTranscriptSnippet(text='can', start=2781.64, duration=4.28), FetchedTranscriptSnippet(text='predict so if I go to my code here so', start=2783.0, duration=4.4), FetchedTranscriptSnippet(text='what we are going to do is we are going', start=2785.92, duration=4.52), FetchedTranscriptSnippet(text='to yeah do this efficiently for our the', start=2787.4, duration=6.04), FetchedTranscriptSnippet(text='verdict data set so just to', start=2790.44, duration=5.72), FetchedTranscriptSnippet(text='um provide it here again the verdict was', start=2793.44, duration=6.159), FetchedTranscriptSnippet(text='the short story by Edith Wharton that we', start=2796.16, duration=5.64), FetchedTranscriptSnippet(text=\"downloaded earlier and what we're going\", start=2799.599, duration=3.76), FetchedTranscriptSnippet(text=\"to do now is we're going to use the tick\", start=2801.8, duration=4.2), FetchedTranscriptSnippet(text='token tokenizer to encode this into', start=2803.359, duration=5.641), FetchedTranscriptSnippet(text='token IDs and yeah if we do that we can', start=2806.0, duration=4.64), FetchedTranscriptSnippet(text='see we have', start=2809.0, duration=5.28), FetchedTranscriptSnippet(text='5,000 145 um token IDs so just maybe to', start=2810.64, duration=6.28), FetchedTranscriptSnippet(text='visualize them can see them so these are', start=2814.28, duration=5.0), FetchedTranscriptSnippet(text='all the yeah token IDs in our text and', start=2816.92, duration=5.159), FetchedTranscriptSnippet(text='now the goal is how can we provide them', start=2819.28, duration=5.12), FetchedTranscriptSnippet(text='efficiently um yeah to the llm in in the', start=2822.079, duration=4.441), FetchedTranscriptSnippet(text=\"form of sub chunks so you can see it's a\", start=2824.4, duration=4.199), FetchedTranscriptSnippet(text=\"lot of tokens and the LM wouldn't be\", start=2826.52, duration=4.839), FetchedTranscriptSnippet(text='able yeah to pass them all so we want to', start=2828.599, duration=5.72), FetchedTranscriptSnippet(text='for example provide chunks of four four', start=2831.359, duration=5.0), FetchedTranscriptSnippet(text=\"because yeah it's nice and small and\", start=2834.319, duration=3.601), FetchedTranscriptSnippet(text='fits into this Jupiter lab and we', start=2836.359, duration=3.801), FetchedTranscriptSnippet(text='visualize it and I can make nice figures', start=2837.92, duration=4.52), FetchedTranscriptSnippet(text='but in practice the chunks are usually', start=2840.16, duration=5.199), FetchedTranscriptSnippet(text=\"much larger they're usually th000 4,000\", start=2842.44, duration=5.679), FetchedTranscriptSnippet(text='8,000 tokens at a time depending on the', start=2845.359, duration=6.401), FetchedTranscriptSnippet(text='llm here um in the case of gpd2 I think', start=2848.119, duration=6.44), FetchedTranscriptSnippet(text='it was 1024 tokens that were used at a', start=2851.76, duration=5.28), FetchedTranscriptSnippet(text='time during pre-training we are in this', start=2854.559, duration=4.961), FetchedTranscriptSnippet(text='particular notebook only using four for', start=2857.04, duration=4.76), FetchedTranscriptSnippet(text='visualization purposes just so that I', start=2859.52, duration=3.559), FetchedTranscriptSnippet(text='can show you some things more', start=2861.8, duration=4.0), FetchedTranscriptSnippet(text='efficiently in a figure um so yeah that', start=2863.079, duration=5.04), FetchedTranscriptSnippet(text='is what I wanted to say here and we are', start=2865.8, duration=4.92), FetchedTranscriptSnippet(text='also going to use um a particular subset', start=2868.119, duration=5.401), FetchedTranscriptSnippet(text='um we are truncating the first 50 tokens', start=2870.72, duration=5.0), FetchedTranscriptSnippet(text='um just to have some nicer examples and', start=2873.52, duration=5.079), FetchedTranscriptSnippet(text='some nicer text but this is just like an', start=2875.72, duration=5.56), FetchedTranscriptSnippet(text='implementation detail to yeah to uh to', start=2878.599, duration=6.48), FetchedTranscriptSnippet(text='make this visually a bit nicer now what', start=2881.28, duration=7.52), FetchedTranscriptSnippet(text='I wanted to show you now is the way we', start=2885.079, duration=7.401), FetchedTranscriptSnippet(text='yeah prepare the X the inputs and the Y', start=2888.8, duration=6.279), FetchedTranscriptSnippet(text='the targets I kind of showed it to you', start=2892.48, duration=4.72), FetchedTranscriptSnippet(text='here already that we are concerned', start=2895.079, duration=4.641), FetchedTranscriptSnippet(text='always with the next um the next Target', start=2897.2, duration=6.48), FetchedTranscriptSnippet(text='token here so we um we are creating a', start=2899.72, duration=7.2), FetchedTranscriptSnippet(text='data set here where the targets are the', start=2903.68, duration=5.72), FetchedTranscriptSnippet(text='puts shifted by one position which then', start=2906.92, duration=5.8), FetchedTranscriptSnippet(text='helps us to have the targets as the next', start=2909.4, duration=6.919), FetchedTranscriptSnippet(text='token to the llm um so for instance here', start=2912.72, duration=6.24), FetchedTranscriptSnippet(text=\"what I'm doing is I have this sample\", start=2916.319, duration=6.481), FetchedTranscriptSnippet(text='text so the sample text is um it should', start=2918.96, duration=6.0), FetchedTranscriptSnippet(text='be about 5,000', start=2922.8, duration=5.16), FetchedTranscriptSnippet(text='something', start=2924.96, duration=3.0), FetchedTranscriptSnippet(text='oops tokens so 5,95 tokens because yeah', start=2928.24, duration=6.359), FetchedTranscriptSnippet(text='I just truncated some tokens so you have', start=2931.72, duration=6.639), FetchedTranscriptSnippet(text='5,95 tokens and we have a context size', start=2934.599, duration=5.72), FetchedTranscriptSnippet(text='of four so we are only considering four', start=2938.359, duration=4.24), FetchedTranscriptSnippet(text='tokens so here we are going to take the', start=2940.319, duration=5.881), FetchedTranscriptSnippet(text='first four tokens and then we shift it', start=2942.599, duration=6.041), FetchedTranscriptSnippet(text='by one position um so that the targets', start=2946.2, duration=4.48), FetchedTranscriptSnippet(text='are yeah shifted by one position so you', start=2948.64, duration=4.24), FetchedTranscriptSnippet(text='can see it here in the output we have', start=2950.68, duration=4.439), FetchedTranscriptSnippet(text='the inputs and then the targets are', start=2952.88, duration=5.239), FetchedTranscriptSnippet(text='shifted by one position so if I show it', start=2955.119, duration=4.72), FetchedTranscriptSnippet(text='like this this is how it would look like', start=2958.119, duration=3.0), FetchedTranscriptSnippet(text=\"but it's a bit hard to read so I\", start=2959.839, duration=3.52), FetchedTranscriptSnippet(text='inserted this white space so you can see', start=2961.119, duration=4.641), FetchedTranscriptSnippet(text='where um the overlaps are here so you', start=2963.359, duration=3.96), FetchedTranscriptSnippet(text='can see those', start=2965.76, duration=3.48), FetchedTranscriptSnippet(text='three tokens overlap and this would be', start=2967.319, duration=3.961), FetchedTranscriptSnippet(text='the next token and this would be the', start=2969.24, duration=4.559), FetchedTranscriptSnippet(text='previous token that is not um presented', start=2971.28, duration=6.039), FetchedTranscriptSnippet(text='in the in the targets so if the llm', start=2973.799, duration=5.56), FetchedTranscriptSnippet(text=\"receives this token it's supposed to\", start=2977.319, duration=4.561), FetchedTranscriptSnippet(text='predict this token if the llm receives', start=2979.359, duration=4.281), FetchedTranscriptSnippet(text=\"those tokens it's supposed to predict\", start=2981.88, duration=4.8), FetchedTranscriptSnippet(text='this token if the llm receives those', start=2983.64, duration=5.32), FetchedTranscriptSnippet(text='tokens it should predict this one and if', start=2986.68, duration=4.56), FetchedTranscriptSnippet(text='it receives those it should predict this', start=2988.96, duration=4.159), FetchedTranscriptSnippet(text=\"one and so forth so that's the whole\", start=2991.24, duration=4.28), FetchedTranscriptSnippet(text='idea here and maybe to make this a bit', start=2993.119, duration=4.361), FetchedTranscriptSnippet(text='more clear what have just shown you', start=2995.52, duration=4.76), FetchedTranscriptSnippet(text=\"visually so here's some other code\", start=2997.48, duration=5.359), FetchedTranscriptSnippet(text='example which is exactly doing what I', start=3000.28, duration=4.839), FetchedTranscriptSnippet(text=\"just showed you where it's showing the\", start=3002.839, duration=4.881), FetchedTranscriptSnippet(text='next token and since this might be a bit', start=3005.119, duration=4.521), FetchedTranscriptSnippet(text='abstract with these numbers we could', start=3007.72, duration=4.639), FetchedTranscriptSnippet(text='also just use our tokenizer um for', start=3009.64, duration=6.0), FetchedTranscriptSnippet(text=\"example let me copy this so it's easier\", start=3012.359, duration=6.841), FetchedTranscriptSnippet(text='than just retyping it so we can use our', start=3015.64, duration=7.84), FetchedTranscriptSnippet(text='tokenizer to decode the token IDs back', start=3019.2, duration=8.52), FetchedTranscriptSnippet(text='into yeah text and do this for both the', start=3023.48, duration=8.16), FetchedTranscriptSnippet(text='targets and the input context and so', start=3027.72, duration=6.639), FetchedTranscriptSnippet(text='this is the same oops I made a mistake I', start=3031.64, duration=6.08), FetchedTranscriptSnippet(text='probably forgot to close parenthesis so', start=3034.359, duration=5.48), FetchedTranscriptSnippet(text='you can see what it would look like so', start=3037.72, duration=4.48), FetchedTranscriptSnippet(text='we always predicting the next word in', start=3039.839, duration=5.081), FetchedTranscriptSnippet(text='the sentence and so to make this really', start=3042.2, duration=4.919), FetchedTranscriptSnippet(text='efficient we are going to use pytorch', start=3044.92, duration=4.08), FetchedTranscriptSnippet(text='where pytorch is a very popular deep', start=3047.119, duration=3.24), FetchedTranscriptSnippet(text=\"learning framework it's probably the\", start=3049.0, duration=4.839), FetchedTranscriptSnippet(text='most Wily used deep learning framework', start=3050.359, duration=6.801), FetchedTranscriptSnippet(text=\"today um and so we're going to to import\", start=3053.839, duration=7.641), FetchedTranscriptSnippet(text='this and use the data Luder and data set', start=3057.16, duration=5.72), FetchedTranscriptSnippet(text='classes from there because they', start=3061.48, duration=2.92), FetchedTranscriptSnippet(text='implemented this very efficiently and', start=3062.88, duration=3.36), FetchedTranscriptSnippet(text=\"we're going to reuse these parts because\", start=3064.4, duration=3.32), FetchedTranscriptSnippet(text=\"they're not really part of the llm we\", start=3066.24, duration=3.4), FetchedTranscriptSnippet(text='are still talking about setting up the', start=3067.72, duration=3.599), FetchedTranscriptSnippet(text='data set and it would be a bit you know', start=3069.64, duration=4.12), FetchedTranscriptSnippet(text='tedious to reinvent the wheel and write', start=3071.319, duration=4.921), FetchedTranscriptSnippet(text='our own data lers and so forth and so', start=3073.76, duration=3.799), FetchedTranscriptSnippet(text='yeah for that we are going to use', start=3076.24, duration=5.079), FetchedTranscriptSnippet(text='pytorch and so pytorch is um imported as', start=3077.559, duration=5.841), FetchedTranscriptSnippet(text='torch even though the library is called', start=3081.319, duration=4.361), FetchedTranscriptSnippet(text=\"pytorch it's just a convention we have\", start=3083.4, duration=5.84), FetchedTranscriptSnippet(text='to remember um and if you followed the', start=3085.68, duration=6.879), FetchedTranscriptSnippet(text='video uh in in the first chapter I', start=3089.24, duration=5.48), FetchedTranscriptSnippet(text='showed you some setup tutorial if you', start=3092.559, duration=3.401), FetchedTranscriptSnippet(text='followed this and installed the', start=3094.72, duration=3.92), FetchedTranscriptSnippet(text='requirements. text this should already', start=3095.96, duration=5.359), FetchedTranscriptSnippet(text=\"work in case it doesn't work it means\", start=3098.64, duration=6.08), FetchedTranscriptSnippet(text=\"you don't have py toch installed and so\", start=3101.319, duration=5.48), FetchedTranscriptSnippet(text='one way to install it would be UV pip', start=3104.72, duration=4.48), FetchedTranscriptSnippet(text='install or pip install depending on how', start=3106.799, duration=5.0), FetchedTranscriptSnippet(text='your system is set up and just typing', start=3109.2, duration=4.32), FetchedTranscriptSnippet(text='this command and it should automatically', start=3111.799, duration=4.401), FetchedTranscriptSnippet(text='get a good version for your computer', start=3113.52, duration=4.68), FetchedTranscriptSnippet(text='however if you want to be more specific', start=3116.2, duration=4.76), FetchedTranscriptSnippet(text='for example you can go to py do.org and', start=3118.2, duration=5.0), FetchedTranscriptSnippet(text='on this website they have like this', start=3120.96, duration=4.52), FetchedTranscriptSnippet(text='installer menu that has different', start=3123.2, duration=4.599), FetchedTranscriptSnippet(text='commands for different computers so for', start=3125.48, duration=4.72), FetchedTranscriptSnippet(text=\"example I'm running this on a Mac um\", start=3127.799, duration=4.601), FetchedTranscriptSnippet(text='with Pip and then it would yeah give me', start=3130.2, duration=6.879), FetchedTranscriptSnippet(text='that um this command if you are on Linux', start=3132.4, duration=7.959), FetchedTranscriptSnippet(text='by default um here this would be the CPU', start=3137.079, duration=5.561), FetchedTranscriptSnippet(text='code it would be a bit different if you', start=3140.359, duration=3.72), FetchedTranscriptSnippet(text='have a', start=3142.64, duration=4.12), FetchedTranscriptSnippet(text='GPU by default if you type a pip', start=3144.079, duration=6.24), FetchedTranscriptSnippet(text='installed torch it would come with Cuda', start=3146.76, duration=6.24), FetchedTranscriptSnippet(text='12.4 libraries so it would support the', start=3150.319, duration=5.52), FetchedTranscriptSnippet(text='GPU with Cuda 12.4 but if you want a', start=3153.0, duration=4.559), FetchedTranscriptSnippet(text='specific version you can also change it', start=3155.839, duration=3.96), FetchedTranscriptSnippet(text='and there are different URLs and by the', start=3157.559, duration=3.921), FetchedTranscriptSnippet(text='time you are watching it there might be', start=3159.799, duration=4.401), FetchedTranscriptSnippet(text='a whole new set of um Cuda versions so', start=3161.48, duration=5.2), FetchedTranscriptSnippet(text='this is really specific um in terms of', start=3164.2, duration=4.919), FetchedTranscriptSnippet(text='What GPU drivers you support but you', start=3166.68, duration=4.76), FetchedTranscriptSnippet(text='know by default um you can also just do', start=3169.119, duration=4.2), FetchedTranscriptSnippet(text='pip install torch and it should', start=3171.44, duration=6.119), FetchedTranscriptSnippet(text='technically work now back to this one', start=3173.319, duration=6.921), FetchedTranscriptSnippet(text=\"here I'm also just double checking which\", start=3177.559, duration=4.401), FetchedTranscriptSnippet(text=\"version I'm using\", start=3180.24, duration=5.04), FetchedTranscriptSnippet(text=\"here so here I'm using version\", start=3181.96, duration=5.639), FetchedTranscriptSnippet(text='2.6 and I should say when I started', start=3185.28, duration=4.76), FetchedTranscriptSnippet(text='writing this book it was 2.0 because', start=3187.599, duration=4.161), FetchedTranscriptSnippet(text='that was the most recent version two', start=3190.04, duration=3.079), FetchedTranscriptSnippet(text='years ago when I started writing the', start=3191.76, duration=4.319), FetchedTranscriptSnippet(text='book um but I tested the book on all', start=3193.119, duration=5.041), FetchedTranscriptSnippet(text='subsequent subsequent versions of', start=3196.079, duration=3.601), FetchedTranscriptSnippet(text=\"pytorch and I couldn't see any\", start=3198.16, duration=3.12), FetchedTranscriptSnippet(text='difference in any of the code on the', start=3199.68, duration=3.639), FetchedTranscriptSnippet(text='GitHub repository I also have automated', start=3201.28, duration=4.36), FetchedTranscriptSnippet(text='tests so each time a new P version comes', start=3203.319, duration=4.121), FetchedTranscriptSnippet(text=\"out I'm just double- checking it still\", start=3205.64, duration=5.08), FetchedTranscriptSnippet(text=\"works and so far I haven't seen any uh\", start=3207.44, duration=5.119), FetchedTranscriptSnippet(text='difference in pyo that would make any of', start=3210.72, duration=4.72), FetchedTranscriptSnippet(text='the code um different so in this case um', start=3212.559, duration=5.201), FetchedTranscriptSnippet(text='you can feel free to also use older', start=3215.44, duration=5.44), FetchedTranscriptSnippet(text='versions of pytorch for example 2.0 2.1', start=3217.76, duration=6.16), FetchedTranscriptSnippet(text='2.2 3 4 and five they should work', start=3220.88, duration=5.56), FetchedTranscriptSnippet(text='equally fine however if you notice', start=3223.92, duration=5.399), FetchedTranscriptSnippet(text=\"anything that doesn't work for\", start=3226.44, duration=5.52), FetchedTranscriptSnippet(text='example the results look different you', start=3229.319, duration=4.961), FetchedTranscriptSnippet(text='could just try also to install this', start=3231.96, duration=5.96), FetchedTranscriptSnippet(text='specific version of pytorch by doing pip', start=3234.28, duration=6.64), FetchedTranscriptSnippet(text='install torch and then', start=3237.92, duration=5.919), FetchedTranscriptSnippet(text='2.6.0 and so this would then install', start=3240.92, duration=4.919), FetchedTranscriptSnippet(text=\"this specific version of pytorch I'm\", start=3243.839, duration=5.401), FetchedTranscriptSnippet(text='using right now at the moment okay so', start=3245.839, duration=5.841), FetchedTranscriptSnippet(text='this was a little detour maybe one more', start=3249.24, duration=4.359), FetchedTranscriptSnippet(text='thing I wanted to mention is um if', start=3251.68, duration=3.84), FetchedTranscriptSnippet(text=\"you're new to pytorch it's essentially a\", start=3253.599, duration=4.361), FetchedTranscriptSnippet(text=\"deep learning framework uh it's very\", start=3255.52, duration=4.68), FetchedTranscriptSnippet(text='relatively comprehensive it has a lot of', start=3257.96, duration=3.879), FetchedTranscriptSnippet(text='things but we are only going to use a', start=3260.2, duration=4.2), FetchedTranscriptSnippet(text=\"subset here in this book um it's\", start=3261.839, duration=5.321), FetchedTranscriptSnippet(text='essentially more of a linear algebra', start=3264.4, duration=4.719), FetchedTranscriptSnippet(text='library with a few deep learning', start=3267.16, duration=4.159), FetchedTranscriptSnippet(text='convenience functions there are whole', start=3269.119, duration=3.96), FetchedTranscriptSnippet(text='courses and books about pytorch if you', start=3271.319, duration=3.8), FetchedTranscriptSnippet(text='want to learn about it U but if yeah if', start=3273.079, duration=4.24), FetchedTranscriptSnippet(text='you want to learn about it efficiently I', start=3275.119, duration=4.801), FetchedTranscriptSnippet(text=\"also have appendix a here so I've been\", start=3277.319, duration=7.161), FetchedTranscriptSnippet(text='using pytorch I would say since 2018 for', start=3279.92, duration=7.48), FetchedTranscriptSnippet(text=\"seven years now and I've taught many pyo\", start=3284.48, duration=5.2), FetchedTranscriptSnippet(text='workshops and um yeah I wrote another', start=3287.4, duration=5.679), FetchedTranscriptSnippet(text='book about pytor and so forth so uh what', start=3289.68, duration=6.36), FetchedTranscriptSnippet(text='I want to say here is over the years I', start=3293.079, duration=4.921), FetchedTranscriptSnippet(text='used pyro quite a lot and I was thinking', start=3296.04, duration=4.519), FetchedTranscriptSnippet(text='really hard what are actually the parts', start=3298.0, duration=6.04), FetchedTranscriptSnippet(text='you need to know for this book so and', start=3300.559, duration=6.24), FetchedTranscriptSnippet(text='based on that I compiled this appendix a', start=3304.04, duration=4.279), FetchedTranscriptSnippet(text='which is essentially a book chapter of', start=3306.799, duration=5.04), FetchedTranscriptSnippet(text='40 pages approximately that just go over', start=3308.319, duration=6.161), FetchedTranscriptSnippet(text='the essentials that you need for yeah', start=3311.839, duration=5.321), FetchedTranscriptSnippet(text='basic llm training and development where', start=3314.48, duration=4.8), FetchedTranscriptSnippet(text='this should get you up to speed', start=3317.16, duration=4.159), FetchedTranscriptSnippet(text=\"relatively quickly of course if you're\", start=3319.28, duration=4.559), FetchedTranscriptSnippet(text='curious I recommend um yeah considering', start=3321.319, duration=5.28), FetchedTranscriptSnippet(text='a whole course or book on pytorch at', start=3323.839, duration=4.841), FetchedTranscriptSnippet(text='some point but if you just want to get', start=3326.599, duration=4.52), FetchedTranscriptSnippet(text='up to speed with pytorch for this', start=3328.68, duration=5.679), FetchedTranscriptSnippet(text='particular um yeah book try to read', start=3331.119, duration=5.2), FetchedTranscriptSnippet(text=\"appendix a and maybe it's already\", start=3334.359, duration=3.68), FetchedTranscriptSnippet(text='sufficient it might be a steeper', start=3336.319, duration=3.321), FetchedTranscriptSnippet(text=\"learning curve because I'm trying to\", start=3338.039, duration=3.921), FetchedTranscriptSnippet(text='keep things efficient and compact but', start=3339.64, duration=4.52), FetchedTranscriptSnippet(text='maybe this is all you need at that time', start=3341.96, duration=4.399), FetchedTranscriptSnippet(text=\"to really get up to speed so you don't\", start=3344.16, duration=4.24), FetchedTranscriptSnippet(text=\"um let's say go on a detour for a few\", start=3346.359, duration=5.0), FetchedTranscriptSnippet(text='months to try to learn pytorch and then', start=3348.4, duration=5.439), FetchedTranscriptSnippet(text='yeah um yeah then you get maybe', start=3351.359, duration=5.2), FetchedTranscriptSnippet(text='frustrated or get bored so I would', start=3353.839, duration=4.601), FetchedTranscriptSnippet(text='suggest maybe trying appendix a and', start=3356.559, duration=3.921), FetchedTranscriptSnippet(text=\"maybe it's already sufficient to get you\", start=3358.44, duration=4.52), FetchedTranscriptSnippet(text='up to speed here now back to the data', start=3360.48, duration=4.92), FetchedTranscriptSnippet(text='loading so we talked about pytorch and', start=3362.96, duration=4.92), FetchedTranscriptSnippet(text=\"importing pytorch let's actually go back\", start=3365.4, duration=5.76), FetchedTranscriptSnippet(text='to section 2.6 data sampling with a', start=3367.88, duration=4.6), FetchedTranscriptSnippet(text='sliding', start=3371.16, duration=4.399), FetchedTranscriptSnippet(text='window so we talked about the target', start=3372.48, duration=5.48), FetchedTranscriptSnippet(text='word predicting the next word and so now', start=3375.559, duration=4.681), FetchedTranscriptSnippet(text='we are going to prepare the data set', start=3377.96, duration=4.92), FetchedTranscriptSnippet(text='accordingly so we are going to take the', start=3380.24, duration=4.359), FetchedTranscriptSnippet(text='input data and we are creating these', start=3382.88, duration=3.84), FetchedTranscriptSnippet(text=\"chunks and I mentioned before we're\", start=3384.599, duration=4.24), FetchedTranscriptSnippet(text='using a chunk size of four or context', start=3386.72, duration=4.68), FetchedTranscriptSnippet(text='size of four for visualization purposes', start=3388.839, duration=4.361), FetchedTranscriptSnippet(text='so it nicely fits into this figure and', start=3391.4, duration=3.88), FetchedTranscriptSnippet(text='so forth but yeah in reality these are', start=3393.2, duration=5.68), FetchedTranscriptSnippet(text='usually larger like 1,24 in the case of', start=3395.28, duration=7.839), FetchedTranscriptSnippet(text='the gpt2 model or newer models use 8,', start=3398.88, duration=6.719), FetchedTranscriptSnippet(text='16,000 uh tokens in each row but it', start=3403.119, duration=4.96), FetchedTranscriptSnippet(text='would be hard of course to visualize', start=3405.599, duration=5.96), FetchedTranscriptSnippet(text='here and so as you can see the one box', start=3408.079, duration=6.601), FetchedTranscriptSnippet(text=\"um here the left box that's the input X\", start=3411.559, duration=4.841), FetchedTranscriptSnippet(text='that we are creating and then the the', start=3414.68, duration=3.8), FetchedTranscriptSnippet(text='targets are shifted by one position as', start=3416.4, duration=4.399), FetchedTranscriptSnippet(text=\"you can see here it's the same idea um\", start=3418.48, duration=4.0), FetchedTranscriptSnippet(text=\"what I showed you earlier we're just\", start=3420.799, duration=3.841), FetchedTranscriptSnippet(text='creating inputs and targets', start=3422.48, duration=4.48), FetchedTranscriptSnippet(text='simultaneously from this text so that we', start=3424.64, duration=5.159), FetchedTranscriptSnippet(text='can feed both to the llm later when we', start=3426.96, duration=4.48), FetchedTranscriptSnippet(text='are training the', start=3429.799, duration=4.681), FetchedTranscriptSnippet(text='llm so um let me actually do this in', start=3431.44, duration=5.599), FetchedTranscriptSnippet(text='code and for this we are going to use a', start=3434.48, duration=6.079), FetchedTranscriptSnippet(text='pytorch data set class and so the reason', start=3437.039, duration=6.201), FetchedTranscriptSnippet(text=\"why we're using a pyro data set class is\", start=3440.559, duration=5.641), FetchedTranscriptSnippet(text='it will allow us to use the py data', start=3443.24, duration=5.28), FetchedTranscriptSnippet(text='orders um later and they are essentially', start=3446.2, duration=4.56), FetchedTranscriptSnippet(text='a very nice and efficient way to create', start=3448.52, duration=4.799), FetchedTranscriptSnippet(text='bches and to shuffle the data and to use', start=3450.76, duration=5.0), FetchedTranscriptSnippet(text='multiple uh background processes to', start=3453.319, duration=4.72), FetchedTranscriptSnippet(text='process the data faster it has a lot of', start=3455.76, duration=5.48), FetchedTranscriptSnippet(text='convenience um added to it so how a data', start=3458.039, duration=5.441), FetchedTranscriptSnippet(text='set looks like is um we have an', start=3461.24, duration=4.28), FetchedTranscriptSnippet(text='Constructor here we are setting them to', start=3463.48, duration=4.8), FetchedTranscriptSnippet(text='empty lists then we are going to', start=3465.52, duration=5.039), FetchedTranscriptSnippet(text='tokenize the text so text is the input', start=3468.28, duration=4.2), FetchedTranscriptSnippet(text=\"text we're going to pass and so we get\", start=3470.559, duration=5.04), FetchedTranscriptSnippet(text='the token IDs and here this is the part', start=3472.48, duration=5.44), FetchedTranscriptSnippet(text='where we creating these so-called chunks', start=3475.599, duration=6.0), FetchedTranscriptSnippet(text='so these chunks are uh these two chunks', start=3477.92, duration=6.08), FetchedTranscriptSnippet(text='basically and instead of I should go', start=3481.599, duration=4.081), FetchedTranscriptSnippet(text='back maybe one more time instead of just', start=3484.0, duration=3.799), FetchedTranscriptSnippet(text='creating one chunk for the inputs and', start=3485.68, duration=3.52), FetchedTranscriptSnippet(text=\"targets we're doing this for the whole\", start=3487.799, duration=3.76), FetchedTranscriptSnippet(text='data set so we are storing these chunks', start=3489.2, duration=6.399), FetchedTranscriptSnippet(text=\"here um we're adding them here for the\", start=3491.559, duration=6.401), FetchedTranscriptSnippet(text='inputs and for the targets so we are', start=3495.599, duration=4.881), FetchedTranscriptSnippet(text='appending them to these list we creating', start=3497.96, duration=4.96), FetchedTranscriptSnippet(text=\"lists we created earlier and it's the\", start=3500.48, duration=5.96), FetchedTranscriptSnippet(text='same concept as before we are yeah um', start=3502.92, duration=6.439), FetchedTranscriptSnippet(text='sliding over the input data set and then', start=3506.44, duration=6.0), FetchedTranscriptSnippet(text='the targets are shifted here by one', start=3509.359, duration=5.24), FetchedTranscriptSnippet(text='position so what we are going to do is', start=3512.44, duration=4.32), FetchedTranscriptSnippet(text=\"we're taking this left box and we are\", start=3514.599, duration=3.76), FetchedTranscriptSnippet(text='going to slide it over the input so you', start=3516.76, duration=4.48), FetchedTranscriptSnippet(text='can see um the first row is in the heart', start=3518.359, duration=5.68), FetchedTranscriptSnippet(text='off and then when we slide it over the', start=3521.24, duration=5.48), FetchedTranscriptSnippet(text='next one is the city stood the and then', start=3524.039, duration=5.361), FetchedTranscriptSnippet(text=\"old library and so forth so it's yeah\", start=3526.72, duration=4.92), FetchedTranscriptSnippet(text='sliding it over by one', start=3529.4, duration=5.56), FetchedTranscriptSnippet(text=\"position and um it's if you have a very\", start=3531.64, duration=5.52), FetchedTranscriptSnippet(text=\"large data set it's maybe not the way to\", start=3534.96, duration=4.76), FetchedTranscriptSnippet(text=\"do it because you can't store 15\", start=3537.16, duration=5.76), FetchedTranscriptSnippet(text='trillion tokens in in memory so this', start=3539.72, duration=5.359), FetchedTranscriptSnippet(text='would run out of memory but if you have', start=3542.92, duration=4.399), FetchedTranscriptSnippet(text='just a few books a few gigabytes or', start=3545.079, duration=4.361), FetchedTranscriptSnippet(text='megabytes of data I mean if you have I', start=3547.319, duration=4.321), FetchedTranscriptSnippet(text=\"don't know thousand books or something\", start=3549.44, duration=4.2), FetchedTranscriptSnippet(text='like this this would conveniently fit so', start=3551.64, duration=3.959), FetchedTranscriptSnippet(text=\"you don't have to worry about it and\", start=3553.64, duration=4.24), FetchedTranscriptSnippet(text='over optimize at this point there are a', start=3555.599, duration=5.0), FetchedTranscriptSnippet(text='few more advanced um ways and tools if', start=3557.88, duration=5.64), FetchedTranscriptSnippet(text='you are really training llms on', start=3560.599, duration=4.921), FetchedTranscriptSnippet(text='trillions of tokens but for your', start=3563.52, duration=3.96), FetchedTranscriptSnippet(text='educational purposes we are keeping', start=3565.52, duration=4.4), FetchedTranscriptSnippet(text='things relatively simple and readable', start=3567.48, duration=5.559), FetchedTranscriptSnippet(text='because this will this is the uh basic', start=3569.92, duration=6.48), FetchedTranscriptSnippet(text='approach and uh yeah so to understand', start=3573.039, duration=6.52), FetchedTranscriptSnippet(text='also how things work under the hood now', start=3576.4, duration=4.959), FetchedTranscriptSnippet(text='we have this data set now and I', start=3579.559, duration=3.76), FetchedTranscriptSnippet(text='mentioned the data laer that we want to', start=3581.359, duration=4.401), FetchedTranscriptSnippet(text='use so let me instead of typing all the', start=3583.319, duration=4.48), FetchedTranscriptSnippet(text='code just insert it', start=3585.76, duration=5.559), FetchedTranscriptSnippet(text='below so the data laer um is created', start=3587.799, duration=5.681), FetchedTranscriptSnippet(text=\"here and I have a function I'm defining\", start=3591.319, duration=5.081), FetchedTranscriptSnippet(text='that returns this data Lo and so here we', start=3593.48, duration=5.44), FetchedTranscriptSnippet(text='are defining the the text you can set a', start=3596.4, duration=5.719), FetchedTranscriptSnippet(text='batch size um a max length of each', start=3598.92, duration=6.199), FetchedTranscriptSnippet(text='context length so before we set four is', start=3602.119, duration=6.121), FetchedTranscriptSnippet(text='our length here um we can also this is', start=3605.119, duration=4.281), FetchedTranscriptSnippet(text='just a default parameter we can', start=3608.24, duration=4.119), FetchedTranscriptSnippet(text='overwrite it later um we set the stride', start=3609.4, duration=5.76), FetchedTranscriptSnippet(text=\"it's by how many positions we are moving\", start=3612.359, duration=4.68), FetchedTranscriptSnippet(text='the Box we will get back to that in a', start=3615.16, duration=4.36), FetchedTranscriptSnippet(text='few moments whether we want to shuffle', start=3617.039, duration=5.0), FetchedTranscriptSnippet(text='the data set whether we want to drop the', start=3619.52, duration=4.48), FetchedTranscriptSnippet(text='last batch or not and this is actually', start=3622.039, duration=4.04), FetchedTranscriptSnippet(text='an interesting one um this is really to', start=3624.0, duration=5.24), FetchedTranscriptSnippet(text='avoid loss spikes so for example if we', start=3626.079, duration=6.641), FetchedTranscriptSnippet(text=\"have a data set let's say example\", start=3629.24, duration=7.04), FetchedTranscriptSnippet(text='one example two and so forth if we have', start=3632.72, duration=7.399), FetchedTranscriptSnippet(text='a data set like that let me just do it', start=3636.28, duration=6.079), FetchedTranscriptSnippet(text='like', start=3640.119, duration=2.24), FetchedTranscriptSnippet(text='this and suppose we have a batch size of', start=3645.24, duration=5.879), FetchedTranscriptSnippet(text='two what the data Lo will do is it will', start=3648.359, duration=5.561), FetchedTranscriptSnippet(text='divide the data set into batch sizes of', start=3651.119, duration=6.601), FetchedTranscriptSnippet(text='two but um if the data set size is not', start=3653.92, duration=6.199), FetchedTranscriptSnippet(text='divisible by the batch size you often', start=3657.72, duration=4.839), FetchedTranscriptSnippet(text='end up with very small batches at the', start=3660.119, duration=4.68), FetchedTranscriptSnippet(text=\"end the last batch because it's the\", start=3662.559, duration=5.161), FetchedTranscriptSnippet(text='remainder and this often results in loss', start=3664.799, duration=5.481), FetchedTranscriptSnippet(text=\"spikes like unstable training so it's\", start=3667.72, duration=5.16), FetchedTranscriptSnippet(text='actually a good idea to always remove', start=3670.28, duration=5.039), FetchedTranscriptSnippet(text='the last batch if you do multiple EPO', start=3672.88, duration=4.36), FetchedTranscriptSnippet(text=\"training you also don't have to worry\", start=3675.319, duration=4.681), FetchedTranscriptSnippet(text='about missing certain data points by', start=3677.24, duration=4.52), FetchedTranscriptSnippet(text='removing them but also in general', start=3680.0, duration=4.0), FetchedTranscriptSnippet(text='usually data sets are so large that you', start=3681.76, duration=4.68), FetchedTranscriptSnippet(text=\"know if you don't use the last batch\", start=3684.0, duration=4.799), FetchedTranscriptSnippet(text=\"doesn't really matter so it's usually a\", start=3686.44, duration=4.48), FetchedTranscriptSnippet(text='good idea to have batches of the same', start=3688.799, duration=4.121), FetchedTranscriptSnippet(text=\"size so you don't have these\", start=3690.92, duration=3.679), FetchedTranscriptSnippet(text='instabilities during training this is', start=3692.92, duration=4.679), FetchedTranscriptSnippet(text=\"why we are why I'm recommending dropping\", start=3694.599, duration=6.041), FetchedTranscriptSnippet(text='the last batch um always and then num', start=3697.599, duration=5.841), FetchedTranscriptSnippet(text='workers is for using multiple background', start=3700.64, duration=5.24), FetchedTranscriptSnippet(text='processes I have it here set to zero', start=3703.44, duration=4.84), FetchedTranscriptSnippet(text=\"because I'm executing this in a notebook\", start=3705.88, duration=5.6), FetchedTranscriptSnippet(text='and in some cases this can be yeah might', start=3708.28, duration=5.799), FetchedTranscriptSnippet(text='not work because uh notebooks are a bit', start=3711.48, duration=5.0), FetchedTranscriptSnippet(text='more restricted than python script', start=3714.079, duration=4.081), FetchedTranscriptSnippet(text='in terms of spawning multiple', start=3716.48, duration=4.0), FetchedTranscriptSnippet(text=\"subprocesses or if you're using Windows\", start=3718.16, duration=4.28), FetchedTranscriptSnippet(text='I think Windows generally has some', start=3720.48, duration=4.359), FetchedTranscriptSnippet(text='problems with multiple background', start=3722.44, duration=4.48), FetchedTranscriptSnippet(text='processes in Python so this is just like', start=3724.839, duration=4.0), FetchedTranscriptSnippet(text='a safe choice but yeah you can try also', start=3726.92, duration=4.6), FetchedTranscriptSnippet(text='to set it to a larger number for example', start=3728.839, duration=4.361), FetchedTranscriptSnippet(text='but yeah for for these purposes in a', start=3731.52, duration=3.72), FetchedTranscriptSnippet(text='notebook this should be fine to set it', start=3733.2, duration=3.2), FetchedTranscriptSnippet(text='to', start=3735.24, duration=4.2), FetchedTranscriptSnippet(text='zero and um yeah this was very', start=3736.4, duration=5.84), FetchedTranscriptSnippet(text='conceptual few more things here we are', start=3739.44, duration=4.96), FetchedTranscriptSnippet(text='initializing the tokenizer this is the', start=3742.24, duration=4.64), FetchedTranscriptSnippet(text='tick token token token we talked about', start=3744.4, duration=5.12), FetchedTranscriptSnippet(text='earlier um then we are creating the data', start=3746.88, duration=5.76), FetchedTranscriptSnippet(text='set that is the data set we have up here', start=3749.52, duration=6.079), FetchedTranscriptSnippet(text='and we are creating it with um yeah text', start=3752.64, duration=5.52), FetchedTranscriptSnippet(text='and text will be our training data set', start=3755.599, duration=4.2), FetchedTranscriptSnippet(text='and here is where we are creating the', start=3758.16, duration=4.639), FetchedTranscriptSnippet(text='data ler using the settings I defined', start=3759.799, duration=6.361), FetchedTranscriptSnippet(text='earlier and we are returning it so all', start=3762.799, duration=6.48), FetchedTranscriptSnippet(text='very very um I would say conceptual so', start=3766.16, duration=5.119), FetchedTranscriptSnippet(text=\"let's actually see it in action and for\", start=3769.279, duration=4.32), FetchedTranscriptSnippet(text='that we are going to use the verdict', start=3771.279, duration=4.921), FetchedTranscriptSnippet(text='again which is our short story story', start=3773.599, duration=4.2), FetchedTranscriptSnippet(text='that we downloaded earlier the short', start=3776.2, duration=3.24), FetchedTranscriptSnippet(text='story by Edith', start=3777.799, duration=6.48), FetchedTranscriptSnippet(text='Wharton um raw text so this is how it', start=3779.44, duration=6.839), FetchedTranscriptSnippet(text='looks like we have seen it earlier and', start=3784.279, duration=4.721), FetchedTranscriptSnippet(text='now we are going to pass it to our data', start=3786.279, duration=5.721), FetchedTranscriptSnippet(text='set and data loer so let me just uh', start=3789.0, duration=4.599), FetchedTranscriptSnippet(text='because otherwise it will end up a very', start=3792.0, duration=3.559), FetchedTranscriptSnippet(text='long video let me just copy and paste', start=3793.599, duration=4.121), FetchedTranscriptSnippet(text='the code here we are going to start with', start=3795.559, duration=5.121), FetchedTranscriptSnippet(text='a batch size of one and a context L', start=3797.72, duration=6.119), FetchedTranscriptSnippet(text='length of one and a stride of one and so', start=3800.68, duration=6.2), FetchedTranscriptSnippet(text='the batch size is how many samples are', start=3803.839, duration=7.161), FetchedTranscriptSnippet(text='in each batch essentially the max length', start=3806.88, duration=7.239), FetchedTranscriptSnippet(text=\"is our context length it's um how long\", start=3811.0, duration=4.88), FetchedTranscriptSnippet(text='the context here is like the size of the', start=3814.119, duration=5.561), FetchedTranscriptSnippet(text='Box how many um yeah tokens are in there', start=3815.88, duration=6.04), FetchedTranscriptSnippet(text='and then the stride is by how many um', start=3819.68, duration=4.76), FetchedTranscriptSnippet(text='places we are moving it so for', start=3821.92, duration=4.639), FetchedTranscriptSnippet(text='Simplicity I will show you stride of one', start=3824.44, duration=6.159), FetchedTranscriptSnippet(text='and then we will change it so if this is', start=3826.559, duration=6.921), FetchedTranscriptSnippet(text='unclear wait for a few moments we will', start=3830.599, duration=4.601), FetchedTranscriptSnippet(text='we will get to it and whether we um', start=3833.48, duration=3.839), FetchedTranscriptSnippet(text='Shuffle the data or not and then we are', start=3835.2, duration=4.72), FetchedTranscriptSnippet(text='iterating here just for demonstration', start=3837.319, duration=5.201), FetchedTranscriptSnippet(text='purposes over the data set manually and', start=3839.92, duration=4.48), FetchedTranscriptSnippet(text='later I will will show you a different', start=3842.52, duration=4.48), FetchedTranscriptSnippet(text='way to do that so here we are creating', start=3844.4, duration=4.919), FetchedTranscriptSnippet(text='an iteration object an iter object and', start=3847.0, duration=4.96), FetchedTranscriptSnippet(text='then we are taking the next um sample', start=3849.319, duration=4.8), FetchedTranscriptSnippet(text='from our data set so when we are', start=3851.96, duration=5.119), FetchedTranscriptSnippet(text='executing this we can see the the next', start=3854.119, duration=5.881), FetchedTranscriptSnippet(text='batch here so this is the first input it', start=3857.079, duration=6.801), FetchedTranscriptSnippet(text='only has um yeah a size of one and this', start=3860.0, duration=6.119), FetchedTranscriptSnippet(text='is then the the targets so you can see', start=3863.88, duration=5.12), FetchedTranscriptSnippet(text='the targets are shifted by one position', start=3866.119, duration=6.68), FetchedTranscriptSnippet(text=\"so and then let's draw another\", start=3869.0, duration=3.799), FetchedTranscriptSnippet(text='batch now what you can see is that the', start=3873.079, duration=6.72), FetchedTranscriptSnippet(text='targets here become those inputs so the', start=3876.76, duration=4.64), FetchedTranscriptSnippet(text='the first one is always the inputs the', start=3879.799, duration=3.961), FetchedTranscriptSnippet(text='second one is always the targets because', start=3881.4, duration=5.52), FetchedTranscriptSnippet(text=\"that's how we set up our data set so the\", start=3883.76, duration=6.359), FetchedTranscriptSnippet(text='first is the inputs the second is the', start=3886.92, duration=7.0), FetchedTranscriptSnippet(text='targets now you can see um this is', start=3890.119, duration=6.24), FetchedTranscriptSnippet(text=\"something where it's maybe not ideal\", start=3893.92, duration=6.0), FetchedTranscriptSnippet(text='because when we have this input and then', start=3896.359, duration=6.0), FetchedTranscriptSnippet(text='the second uh iteration is this input', start=3899.92, duration=5.76), FetchedTranscriptSnippet(text='there is an overlap so these tokens are', start=3902.359, duration=5.881), FetchedTranscriptSnippet(text='the same and if you do it like this the', start=3905.68, duration=5.679), FetchedTranscriptSnippet(text='llm sees certain tokens many many times', start=3908.24, duration=4.599), FetchedTranscriptSnippet(text=\"and then it's kind of leading to\", start=3911.359, duration=4.0), FetchedTranscriptSnippet(text=\"overfitting so what's actually better is\", start=3912.839, duration=5.321), FetchedTranscriptSnippet(text='to increase the stride by four so if we', start=3915.359, duration=6.521), FetchedTranscriptSnippet(text='do that you can see each of the tokens', start=3918.16, duration=6.8), FetchedTranscriptSnippet(text='are unique so you can see they are not', start=3921.88, duration=6.239), FetchedTranscriptSnippet(text='overlapping anymore more and so what it', start=3924.96, duration=6.079), FetchedTranscriptSnippet(text=\"does is it's moving this box by stri of\", start=3928.119, duration=6.081), FetchedTranscriptSnippet(text='four by four positions so 1 2 3 four', start=3931.039, duration=5.441), FetchedTranscriptSnippet(text='moving it by four positions so we are at', start=3934.2, duration=4.56), FetchedTranscriptSnippet(text='the city stood the and you can see this', start=3936.48, duration=5.119), FetchedTranscriptSnippet(text=\"is what's shown here in the second row\", start=3938.76, duration=4.12), FetchedTranscriptSnippet(text='the reason why I showed you just the', start=3941.599, duration=4.561), FetchedTranscriptSnippet(text='stride of one was so you can see that', start=3942.88, duration=5.239), FetchedTranscriptSnippet(text=\"this actually works and it's not missing\", start=3946.16, duration=6.52), FetchedTranscriptSnippet(text='any data because you can see um this one', start=3948.119, duration=6.48), FetchedTranscriptSnippet(text='picks up right here if we have a stride', start=3952.68, duration=3.24), FetchedTranscriptSnippet(text='of one I thought this might be more', start=3954.599, duration=3.96), FetchedTranscriptSnippet(text=\"intuitive but if it is confusing don't\", start=3955.92, duration=4.639), FetchedTranscriptSnippet(text='worry about it just uh go to the stride', start=3958.559, duration=4.321), FetchedTranscriptSnippet(text='of four and then you have exactly what', start=3960.559, duration=5.601), FetchedTranscriptSnippet(text='you can see here in the figure now this', start=3962.88, duration=6.76), FetchedTranscriptSnippet(text='is um for a batch size of one when we', start=3966.16, duration=5.399), FetchedTranscriptSnippet(text=\"are training deep do networks it's\", start=3969.64, duration=4.36), FetchedTranscriptSnippet(text='usually efficient to use a larger batch', start=3971.559, duration=5.321), FetchedTranscriptSnippet(text='size so what we can do is um we can use', start=3974.0, duration=4.359), FetchedTranscriptSnippet(text=\"the batch size of eight so I'm just\", start=3976.88, duration=3.959), FetchedTranscriptSnippet(text='showing you same thing here instead of', start=3978.359, duration=4.72), FetchedTranscriptSnippet(text='bch size of one I have a bch size of', start=3980.839, duration=5.72), FetchedTranscriptSnippet(text='eight max length of four and A stri four', start=3983.079, duration=5.681), FetchedTranscriptSnippet(text=\"and then yeah some labeling so it's a\", start=3986.559, duration=4.52), FetchedTranscriptSnippet(text='bit more clear and yeah you can see now', start=3988.76, duration=5.12), FetchedTranscriptSnippet(text='we have the the inputs here and the', start=3991.079, duration=6.24), FetchedTranscriptSnippet(text='targets and this is similar uh to what', start=3993.88, duration=5.32), FetchedTranscriptSnippet(text='we are seeing here where we have', start=3997.319, duration=5.48), FetchedTranscriptSnippet(text='multiple rows where each row is one', start=3999.2, duration=5.72), FetchedTranscriptSnippet(text='training example and so we have a batch', start=4002.799, duration=6.401), FetchedTranscriptSnippet(text='size of 1 2 3 4 5 6 7 eight and then the', start=4004.92, duration=7.159), FetchedTranscriptSnippet(text='corresponding targets yeah and this is', start=4009.2, duration=6.56), FetchedTranscriptSnippet(text='really um how we implement this data la', start=4012.079, duration=7.0), FetchedTranscriptSnippet(text='and then um later we can also iterate', start=4015.76, duration=5.64), FetchedTranscriptSnippet(text='over it more efficiently so what this is', start=4019.079, duration=4.72), FetchedTranscriptSnippet(text='a topic for later I just wanted to set', start=4021.4, duration=4.28), FetchedTranscriptSnippet(text='up this data loer so that we have one', start=4023.799, duration=4.841), FetchedTranscriptSnippet(text='way later on to load the data into the', start=4025.68, duration=5.639), FetchedTranscriptSnippet(text='llm', start=4028.64, duration=2.679), FetchedTranscriptSnippet(text='efficiently we talked a lot about', start=4033.359, duration=4.561), FetchedTranscriptSnippet(text=\"creating token IDs but now let's take it\", start=4035.52, duration=5.079), FetchedTranscriptSnippet(text='to the next step and talk about creating', start=4037.92, duration=5.0), FetchedTranscriptSnippet(text='token embeddings so if I go back to an', start=4040.599, duration=5.161), FetchedTranscriptSnippet(text='earlier figure from section 2.2 we', start=4042.92, duration=5.199), FetchedTranscriptSnippet(text='looked at this figure where I showed you', start=4045.76, duration=4.72), FetchedTranscriptSnippet(text='that input text gets converted into', start=4048.119, duration=5.121), FetchedTranscriptSnippet(text='tokenized text and the tokenized text', start=4050.48, duration=5.96), FetchedTranscriptSnippet(text='gets converted to token IDs now the next', start=4053.24, duration=5.359), FetchedTranscriptSnippet(text='step is creating these token embeddings', start=4056.44, duration=4.48), FetchedTranscriptSnippet(text='from the token IDs so essentially taking', start=4058.599, duration=5.52), FetchedTranscriptSnippet(text='an ID an integer value and converting it', start=4060.92, duration=5.359), FetchedTranscriptSnippet(text='to an embedding vector and so this', start=4064.119, duration=4.761), FetchedTranscriptSnippet(text='embedding Vector contains real numbers', start=4066.279, duration=4.8), FetchedTranscriptSnippet(text='like a floating Point numbers that can', start=4068.88, duration=3.52), FetchedTranscriptSnippet(text='then be', start=4071.079, duration=4.081), FetchedTranscriptSnippet(text='optimized so um yeah to show you how it', start=4072.4, duration=3.679), FetchedTranscriptSnippet(text='works', start=4075.16, duration=3.959), FetchedTranscriptSnippet(text='we are going to use um some token IDs as', start=4076.079, duration=4.601), FetchedTranscriptSnippet(text='an example just', start=4079.119, duration=5.48), FetchedTranscriptSnippet(text='like um as a pyd tensor so that it is a', start=4080.68, duration=6.32), FetchedTranscriptSnippet(text=\"bit more easy to see what's going on so\", start=4084.599, duration=6.081), FetchedTranscriptSnippet(text=\"let's do torch tensor and then um insert\", start=4087.0, duration=5.72), FetchedTranscriptSnippet(text='those and I should say these are', start=4090.68, duration=4.76), FetchedTranscriptSnippet(text=\"relatively large numbers so let's\", start=4092.72, duration=5.16), FetchedTranscriptSnippet(text='actually use yeah some smaller ones so', start=4095.44, duration=5.279), FetchedTranscriptSnippet(text='we can have nicer visualization so these', start=4097.88, duration=6.279), FetchedTranscriptSnippet(text='are just like uh some words or tokens in', start=4100.719, duration=6.6), FetchedTranscriptSnippet(text='our input so we have this input tensor', start=4104.159, duration=5.841), FetchedTranscriptSnippet(text='now and then the next step would be', start=4107.319, duration=5.321), FetchedTranscriptSnippet(text='creating a so-called embedding layer', start=4110.0, duration=5.88), FetchedTranscriptSnippet(text='usually this embedding layer is um part', start=4112.64, duration=5.639), FetchedTranscriptSnippet(text='of the llm', start=4115.88, duration=5.879), FetchedTranscriptSnippet(text='itself we will revisit this many times', start=4118.279, duration=5.96), FetchedTranscriptSnippet(text='later in this book but this would be a', start=4121.759, duration=4.881), FetchedTranscriptSnippet(text='standalone um embedding layer and there', start=4124.239, duration=4.681), FetchedTranscriptSnippet(text='usually something called the vocabulary', start=4126.64, duration=4.599), FetchedTranscriptSnippet(text='size so this is the size of the', start=4128.92, duration=4.799), FetchedTranscriptSnippet(text='tokenizer the vocabulary', start=4131.239, duration=5.641), FetchedTranscriptSnippet(text='tokenizer and then the output Dimension', start=4133.719, duration=5.681), FetchedTranscriptSnippet(text='so this is the vector embedding size so', start=4136.88, duration=6.52), FetchedTranscriptSnippet(text='the vocabulary size is in our case um', start=4139.4, duration=7.08), FetchedTranscriptSnippet(text=\"for the tick token tokenizer it's 5,00\", start=4143.4, duration=4.959), FetchedTranscriptSnippet(text='uh 50,', start=4146.48, duration=4.0), FetchedTranscriptSnippet(text='257 sorry that is probably not the', start=4148.359, duration=4.681), FetchedTranscriptSnippet(text='correct number uh let me double check', start=4150.48, duration=5.12), FetchedTranscriptSnippet(text='actually', start=4153.04, duration=2.56), FetchedTranscriptSnippet(text='tokenizer one way if you forget like me', start=4157.4, duration=5.56), FetchedTranscriptSnippet(text='what the the name of the vocabulary is', start=4160.44, duration=5.48), FetchedTranscriptSnippet(text='you can use in Python this uh deer', start=4162.96, duration=6.279), FetchedTranscriptSnippet(text='and it should actually show you so n', start=4165.92, duration=6.12), FetchedTranscriptSnippet(text='voap so if I go up', start=4169.239, duration=6.201), FetchedTranscriptSnippet(text='here Type n WAP and then we do the', start=4172.04, duration=7.6), FetchedTranscriptSnippet(text='length you can see oops uh', start=4175.44, duration=6.96), FetchedTranscriptSnippet(text=\"it's not a list it's just an integer you\", start=4179.64, duration=6.96), FetchedTranscriptSnippet(text='can see it has 50,2 57 unique tokens so', start=4182.4, duration=6.759), FetchedTranscriptSnippet(text='this would be one argument for our', start=4186.6, duration=4.44), FetchedTranscriptSnippet(text='embedding layer so it supports all these', start=4189.159, duration=3.68), FetchedTranscriptSnippet(text='different tokens and the other one would', start=4191.04, duration=4.56), FetchedTranscriptSnippet(text='be the output Dimension so this would be', start=4192.839, duration=7.161), FetchedTranscriptSnippet(text='the desired size for these vectors we', start=4195.6, duration=7.76), FetchedTranscriptSnippet(text='will actually use the original GPT um', start=4200.0, duration=6.36), FetchedTranscriptSnippet(text='embedding size later on for Simplicity', start=4203.36, duration=6.0), FetchedTranscriptSnippet(text=\"let's actually use simpler and smaller\", start=4206.36, duration=5.64), FetchedTranscriptSnippet(text=\"values so it's then easier to visualize\", start=4209.36, duration=7.52), FetchedTranscriptSnippet(text='so let me use V cap size six and output', start=4212.0, duration=8.8), FetchedTranscriptSnippet(text='Dimension um let me just copy', start=4216.88, duration=3.92), FetchedTranscriptSnippet(text=\"it uh let's do three because uh three is\", start=4221.84, duration=4.56), FetchedTranscriptSnippet(text='also so what we are seeing here like', start=4225.0, duration=3.719), FetchedTranscriptSnippet(text='three little boxes per Vector so we have', start=4226.4, duration=4.52), FetchedTranscriptSnippet(text='a very small um example that is', start=4228.719, duration=4.921), FetchedTranscriptSnippet(text='hopefully easy to see and then', start=4230.92, duration=5.96), FetchedTranscriptSnippet(text=\"um one more thing is I'm adding a random\", start=4233.64, duration=4.96), FetchedTranscriptSnippet(text='seat', start=4236.88, duration=5.6), FetchedTranscriptSnippet(text=\"here why am I doing that uh it's because\", start=4238.6, duration=6.2), FetchedTranscriptSnippet(text='this is a neuron Network layer with', start=4242.48, duration=4.44), FetchedTranscriptSnippet(text='random weights and so the random weights', start=4244.8, duration=4.16), FetchedTranscriptSnippet(text='are different each time you instantiate', start=4246.92, duration=4.44), FetchedTranscriptSnippet(text='a new layer and so I just want to make', start=4248.96, duration=4.6), FetchedTranscriptSnippet(text='sure you get the same results that I do', start=4251.36, duration=4.2), FetchedTranscriptSnippet(text=\"so I'm initializing this right random\", start=4253.56, duration=4.28), FetchedTranscriptSnippet(text='seat and by the way if random seeds are', start=4255.56, duration=4.52), FetchedTranscriptSnippet(text='something um you are not familiar with', start=4257.84, duration=4.879), FetchedTranscriptSnippet(text='let me know I also have a nice video on', start=4260.08, duration=5.44), FetchedTranscriptSnippet(text='random seeds I can share with you but', start=4262.719, duration=4.801), FetchedTranscriptSnippet(text='yeah so this is um now this embedding', start=4265.52, duration=3.88), FetchedTranscriptSnippet(text='layer and the embedding', start=4267.52, duration=5.56), FetchedTranscriptSnippet(text='layer has a weight', start=4269.4, duration=3.68), FetchedTranscriptSnippet(text='Matrix um wait and so this is something', start=4274.12, duration=6.599), FetchedTranscriptSnippet(text='that is later optimized during the', start=4278.239, duration=4.641), FetchedTranscriptSnippet(text='neuron Network training like the llm', start=4280.719, duration=4.801), FetchedTranscriptSnippet(text='training and um yeah they are called', start=4282.88, duration=5.56), FetchedTranscriptSnippet(text='parameters or parameters and these are', start=4285.52, duration=5.44), FetchedTranscriptSnippet(text='things that are going to be adjusted and', start=4288.44, duration=5.4), FetchedTranscriptSnippet(text='by the way if you are familiar already', start=4290.96, duration=4.84), FetchedTranscriptSnippet(text='with linear layers or Matrix', start=4293.84, duration=4.28), FetchedTranscriptSnippet(text='multiplications and everything um an', start=4295.8, duration=4.439), FetchedTranscriptSnippet(text='embedding layer is just um an', start=4298.12, duration=5.2), FetchedTranscriptSnippet(text='implementation detail to make a certain', start=4300.239, duration=6.721), FetchedTranscriptSnippet(text='lookup easier or more efficient this is', start=4303.32, duration=5.399), FetchedTranscriptSnippet(text=\"more advanced you don't have to read\", start=4306.96, duration=3.32), FetchedTranscriptSnippet(text=\"this if you're not familiar with this\", start=4308.719, duration=4.401), FetchedTranscriptSnippet(text='but for those who are curious I do', start=4310.28, duration=6.12), FetchedTranscriptSnippet(text='actually have some bonus material here', start=4313.12, duration=6.68), FetchedTranscriptSnippet(text=\"um where it's about understanding the\", start=4316.4, duration=4.96), FetchedTranscriptSnippet(text='difference', start=4319.8, duration=4.56), FetchedTranscriptSnippet(text='between one moment uh the difference', start=4321.36, duration=6.16), FetchedTranscriptSnippet(text='between embedding layers and linear', start=4324.36, duration=5.48), FetchedTranscriptSnippet(text=\"layers so um if you're curious how this\", start=4327.52, duration=4.119), FetchedTranscriptSnippet(text='relates to Matrix multiplications and', start=4329.84, duration=4.319), FetchedTranscriptSnippet(text=\"linear layers it's like a short notebook\", start=4331.639, duration=5.0), FetchedTranscriptSnippet(text='for those who are curious but if you are', start=4334.159, duration=4.241), FetchedTranscriptSnippet(text=\"not familiar with it don't worry about\", start=4336.639, duration=4.6), FetchedTranscriptSnippet(text='it I will try to explain it in a more', start=4338.4, duration=4.96), FetchedTranscriptSnippet(text=\"let's say intuitive sense so let me show\", start=4341.239, duration=5.92), FetchedTranscriptSnippet(text='you something so if I do embed', start=4343.36, duration=8.879), FetchedTranscriptSnippet(text=\"layer and I call it on let's say this\", start=4347.159, duration=6.921), FetchedTranscriptSnippet(text=\"input here let's just pick one of these\", start=4352.239, duration=4.681), FetchedTranscriptSnippet(text=\"numbers um torch tensor let's make a\", start=4354.08, duration=5.04), FetchedTranscriptSnippet(text='smaller tensor and', start=4356.92, duration=6.96), FetchedTranscriptSnippet(text=\"use let's say the the three from\", start=4359.12, duration=4.76), FetchedTranscriptSnippet(text=\"here let's see what happens so you can\", start=4363.96, duration=5.759), FetchedTranscriptSnippet(text=\"see it's pulling out a vector here so\", start=4367.36, duration=5.4), FetchedTranscriptSnippet(text='there a um tensor of three different', start=4369.719, duration=6.041), FetchedTranscriptSnippet(text='numbers and if you um have a sharp I you', start=4372.76, duration=5.2), FetchedTranscriptSnippet(text=\"might notice it's actually this row here\", start=4375.76, duration=5.24), FetchedTranscriptSnippet(text='from this embedding Matrix so if we are', start=4377.96, duration=5.84), FetchedTranscriptSnippet(text='uh typing three here py uh python is a', start=4381.0, duration=8.52), FetchedTranscriptSnippet(text=\"zero index so it's 0 1 2 3 it's giving\", start=4383.8, duration=8.96), FetchedTranscriptSnippet(text='us this Vector from this Matrix and', start=4389.52, duration=6.639), FetchedTranscriptSnippet(text=\"similarly if I go here and I do let's\", start=4392.76, duration=6.28), FetchedTranscriptSnippet(text='use the first one and I do this one you', start=4396.159, duration=7.641), FetchedTranscriptSnippet(text='can see this one is the first um sorry', start=4399.04, duration=9.56), FetchedTranscriptSnippet(text='not quite the Third row sorry 0 1 2', start=4403.8, duration=6.72), FetchedTranscriptSnippet(text=\"because that's the number two and this\", start=4408.6, duration=4.24), FetchedTranscriptSnippet(text='is yeah the index two corresponding to', start=4410.52, duration=4.6), FetchedTranscriptSnippet(text='this row and similarly if I now use the', start=4412.84, duration=4.879), FetchedTranscriptSnippet(text='whole input', start=4415.12, duration=2.599), FetchedTranscriptSnippet(text=\"IDs I'm pulling out values from this\", start=4420.159, duration=5.601), FetchedTranscriptSnippet(text='Matrix', start=4424.08, duration=4.72), FetchedTranscriptSnippet(text='so um I probably should show them here', start=4425.76, duration=7.16), FetchedTranscriptSnippet(text=\"for reference so the first one that's\", start=4428.8, duration=6.879), FetchedTranscriptSnippet(text='the two it corresponds to 0 one two to', start=4432.92, duration=5.239), FetchedTranscriptSnippet(text='this one here the second one corresponds', start=4435.679, duration=4.921), FetchedTranscriptSnippet(text='to this three so this would be this row', start=4438.159, duration=4.401), FetchedTranscriptSnippet(text='here the', start=4440.6, duration=8.079), FetchedTranscriptSnippet(text='five this one 0 1 2 3 four five and so', start=4442.56, duration=9.119), FetchedTranscriptSnippet(text='forth so we are essentially when we are', start=4448.679, duration=4.56), FetchedTranscriptSnippet(text='calling this embedding layer on the', start=4451.679, duration=4.0), FetchedTranscriptSnippet(text='input IDs we are pulling out vectors', start=4453.239, duration=5.561), FetchedTranscriptSnippet(text='from this Matrix and this Matrix', start=4455.679, duration=6.921), FetchedTranscriptSnippet(text='technically has the size of our um', start=4458.8, duration=5.96), FetchedTranscriptSnippet(text='tokenizer so I can just show you this', start=4462.6, duration=7.039), FetchedTranscriptSnippet(text='example Le tokenizer n wab right so in', start=4464.76, duration=7.08), FetchedTranscriptSnippet(text='this case this might be a very large one', start=4469.639, duration=4.04), FetchedTranscriptSnippet(text='should I print it maybe let try and see', start=4471.84, duration=3.72), FetchedTranscriptSnippet(text=\"what happens it's actually dotting it\", start=4473.679, duration=6.121), FetchedTranscriptSnippet(text='out but yeah this would be then 50,2 57', start=4475.56, duration=6.119), FetchedTranscriptSnippet(text='um rows essentially and yeah this is', start=4479.8, duration=3.839), FetchedTranscriptSnippet(text='essentially the embedding layer that is', start=4481.679, duration=5.801), FetchedTranscriptSnippet(text='used to convert the token IDs here into', start=4483.639, duration=6.0), FetchedTranscriptSnippet(text='these vectors and this is usually part', start=4487.48, duration=5.32), FetchedTranscriptSnippet(text='of the llm and yeah these numbers I', start=4489.639, duration=4.961), FetchedTranscriptSnippet(text='mentioned before they are random so if I', start=4492.8, duration=5.2), FetchedTranscriptSnippet(text='actually comment this out you might see', start=4494.6, duration=6.48), FetchedTranscriptSnippet(text='um these numbers change because the', start=4498.0, duration=5.52), FetchedTranscriptSnippet(text='weight Matrix uh yeah the weight Matrix', start=4501.08, duration=5.079), FetchedTranscriptSnippet(text='changes and so usually what we want to', start=4503.52, duration=4.719), FetchedTranscriptSnippet(text='do is we want to start with small random', start=4506.159, duration=3.721), FetchedTranscriptSnippet(text=\"numbers it doesn't really matter what\", start=4508.239, duration=3.761), FetchedTranscriptSnippet(text='these numbers are but later on during', start=4509.88, duration=5.04), FetchedTranscriptSnippet(text='the training these numbers get optimized', start=4512.0, duration=6.199), FetchedTranscriptSnippet(text='uh in a sense that the llm is optimized', start=4514.92, duration=6.56), FetchedTranscriptSnippet(text='end to end to produce yeah the desired', start=4518.199, duration=6.121), FetchedTranscriptSnippet(text='text the next word in the training text', start=4521.48, duration=4.92), FetchedTranscriptSnippet(text=\"and so don't worry about why these\", start=4524.32, duration=3.96), FetchedTranscriptSnippet(text=\"numbers are random yet because they're\", start=4526.4, duration=3.64), FetchedTranscriptSnippet(text=\"not trained yet they're just starting\", start=4528.28, duration=3.879), FetchedTranscriptSnippet(text=\"values we're going to train them later\", start=4530.04, duration=4.52), FetchedTranscriptSnippet(text='in chapter 5 but now yeah you have a', start=4532.159, duration=4.161), FetchedTranscriptSnippet(text='hopefully good understanding what an', start=4534.56, duration=5.159), FetchedTranscriptSnippet(text='embedding is so an embedding is a vector', start=4536.32, duration=5.08), FetchedTranscriptSnippet(text='from this embedding Matrix that gets', start=4539.719, duration=4.121), FetchedTranscriptSnippet(text='later', start=4541.4, duration=2.44), FetchedTranscriptSnippet(text='trained so now that we understand word', start=4545.239, duration=5.44), FetchedTranscriptSnippet(text=\"embeddings let's take it a step further\", start=4548.199, duration=5.04), FetchedTranscriptSnippet(text='and add positional information I will', start=4550.679, duration=4.361), FetchedTranscriptSnippet(text='tell you in a few moments what the', start=4553.239, duration=3.641), FetchedTranscriptSnippet(text='positional information means but yeah', start=4555.04, duration=5.52), FetchedTranscriptSnippet(text='just to recap we had input text we', start=4556.88, duration=6.2), FetchedTranscriptSnippet(text='tokenized the input text created token', start=4560.56, duration=4.92), FetchedTranscriptSnippet(text='IDs and then the previous section was', start=4563.08, duration=4.24), FetchedTranscriptSnippet(text='focused on creating these so-called', start=4565.48, duration=3.92), FetchedTranscriptSnippet(text='token embeddings and we only had a very', start=4567.32, duration=5.24), FetchedTranscriptSnippet(text='small toy example and then just to use', start=4569.4, duration=7.04), FetchedTranscriptSnippet(text='the actual sizes um for our tokenizer we', start=4572.56, duration=7.04), FetchedTranscriptSnippet(text='mentioned our tokenizer has 50,2 57', start=4576.44, duration=6.52), FetchedTranscriptSnippet(text='words and so we pass this vocab size to', start=4579.6, duration=6.119), FetchedTranscriptSnippet(text='our embedding layer to to create it in', start=4582.96, duration=5.279), FetchedTranscriptSnippet(text='in this size and now we also scale this', start=4585.719, duration=4.681), FetchedTranscriptSnippet(text='a little bit up cuz before you know we', start=4588.239, duration=5.201), FetchedTranscriptSnippet(text='had like a output dimension of one two', start=4590.4, duration=5.0), FetchedTranscriptSnippet(text='three tokens which is just for a toy', start=4593.44, duration=5.56), FetchedTranscriptSnippet(text='example now we are increasing it to 256', start=4595.4, duration=5.64), FetchedTranscriptSnippet(text='which is what we are going to use to', start=4599.0, duration=3.92), FetchedTranscriptSnippet(text=\"train the llm later it's a more\", start=4601.04, duration=4.88), FetchedTranscriptSnippet(text='realistic size um gpd2 was originally', start=4602.92, duration=5.319), FetchedTranscriptSnippet(text='trained with u output dimension of I', start=4605.92, duration=3.48), FetchedTranscriptSnippet(text='think', start=4608.239, duration=3.44), FetchedTranscriptSnippet(text=\"1,24 I'm not quite sure I would have to\", start=4609.4, duration=5.12), FetchedTranscriptSnippet(text='double check um but yeah this would be', start=4611.679, duration=4.681), FetchedTranscriptSnippet(text='something that works also well on a', start=4614.52, duration=4.119), FetchedTranscriptSnippet(text='small computer like a laptop for example', start=4616.36, duration=4.68), FetchedTranscriptSnippet(text='you can always you know try larger', start=4618.639, duration=4.361), FetchedTranscriptSnippet(text='values if your computer supports it but', start=4621.04, duration=3.639), FetchedTranscriptSnippet(text='I think this should be supported on most', start=4623.0, duration=4.44), FetchedTranscriptSnippet(text='hardware for yeah demonstration purposes', start=4624.679, duration=5.04), FetchedTranscriptSnippet(text='but also to make it a bit more realistic', start=4627.44, duration=4.239), FetchedTranscriptSnippet(text='than having an embedding size of just', start=4629.719, duration=5.121), FetchedTranscriptSnippet(text='three dimensions so we have now 256', start=4631.679, duration=6.201), FetchedTranscriptSnippet(text='Dimensions so if we do that um so we', start=4634.84, duration=5.92), FetchedTranscriptSnippet(text='have our embedding Matrix that has', start=4637.88, duration=6.56), FetchedTranscriptSnippet(text='50,000 uh rows and about 200 um columns', start=4640.76, duration=6.68), FetchedTranscriptSnippet(text=\"so that's our weight Matrix similar to\", start=4644.44, duration=6.64), FetchedTranscriptSnippet(text='what we did before now taking it a step', start=4647.44, duration=5.759), FetchedTranscriptSnippet(text='further we had also the data ler how', start=4651.08, duration=3.84), FetchedTranscriptSnippet(text='does it now come together with these', start=4653.199, duration=4.681), FetchedTranscriptSnippet(text='embeddings previously we created these', start=4654.92, duration=6.48), FetchedTranscriptSnippet(text='um uh token IDs so if I just show you', start=4657.88, duration=5.24), FetchedTranscriptSnippet(text=\"this it's the same that we've covered\", start=4661.4, duration=6.0), FetchedTranscriptSnippet(text='earlier so we have um a data loer that', start=4663.12, duration=6.72), FetchedTranscriptSnippet(text='creates batches of size eight with a max', start=4667.4, duration=6.0), FetchedTranscriptSnippet(text='length of four so four token IDs um per', start=4669.84, duration=5.399), FetchedTranscriptSnippet(text='training example and then we we have', start=4673.4, duration=3.2), FetchedTranscriptSnippet(text='eight', start=4675.239, duration=4.681), FetchedTranscriptSnippet(text='examples and so now how does it actually', start=4676.6, duration=5.36), FetchedTranscriptSnippet(text='relate to the Token embedding that we', start=4679.92, duration=4.04), FetchedTranscriptSnippet(text='have seen in the previous section so', start=4681.96, duration=4.239), FetchedTranscriptSnippet(text='what we do now is we actually convert', start=4683.96, duration=3.8), FetchedTranscriptSnippet(text='these token IDs into these embedding', start=4686.199, duration=4.0), FetchedTranscriptSnippet(text='vectors so if I do that um what I showed', start=4687.76, duration=5.879), FetchedTranscriptSnippet(text='you earlier is if I take this and I', start=4690.199, duration=6.841), FetchedTranscriptSnippet(text='type input IDs it would create the', start=4693.639, duration=6.08), FetchedTranscriptSnippet(text='sample uh vectors for the simple input', start=4697.04, duration=4.76), FetchedTranscriptSnippet(text='IDs that we talked about earlier but now', start=4699.719, duration=3.561), FetchedTranscriptSnippet(text='we are going to use actually the data', start=4701.8, duration=4.48), FetchedTranscriptSnippet(text='badge here so this would be the data', start=4703.28, duration=7.08), FetchedTranscriptSnippet(text=\"badge that we convert and let's maybe\", start=4706.28, duration=6.399), FetchedTranscriptSnippet(text='call this token', start=4710.36, duration=4.48), FetchedTranscriptSnippet(text=\"embeddings and let's just see what\", start=4712.679, duration=5.0), FetchedTranscriptSnippet(text=\"happens and let's actually look at the\", start=4714.84, duration=5.68), FetchedTranscriptSnippet(text='size of the output', start=4717.679, duration=6.321), FetchedTranscriptSnippet(text='here so you can see now we have still', start=4720.52, duration=5.44), FetchedTranscriptSnippet(text=\"the batch size of eight so it's now a\", start=4724.0, duration=4.239), FetchedTranscriptSnippet(text='three-dimensional tensor and the numbers', start=4725.96, duration=4.0), FetchedTranscriptSnippet(text='correspond to the batch size so we have', start=4728.239, duration=5.281), FetchedTranscriptSnippet(text='1 2 3 4 5 6 seven eight rows eight', start=4729.96, duration=7.08), FetchedTranscriptSnippet(text='examples each example has four tokens', start=4733.52, duration=7.8), FetchedTranscriptSnippet(text='like one two whoops one two three four', start=4737.04, duration=7.24), FetchedTranscriptSnippet(text=\"but now it's not just the token ID each\", start=4741.32, duration=5.399), FetchedTranscriptSnippet(text='one of those is actually a 256', start=4744.28, duration=6.24), FetchedTranscriptSnippet(text='dimensional Vector so here each token ID', start=4746.719, duration=5.92), FetchedTranscriptSnippet(text='gets converted to a three-dimensional', start=4750.52, duration=6.6), FetchedTranscriptSnippet(text=\"Vector now it's a 256 dimensional Vector\", start=4752.639, duration=7.121), FetchedTranscriptSnippet(text=\"it's hard to visualize vectors in 256\", start=4757.12, duration=5.84), FetchedTranscriptSnippet(text='Dimensions but uh we can actually uh', start=4759.76, duration=6.24), FetchedTranscriptSnippet(text='maybe print one of those out so if we', start=4762.96, duration=6.32), FetchedTranscriptSnippet(text='take a look at the first one first batch', start=4766.0, duration=6.48), FetchedTranscriptSnippet(text='the first token this should give us um', start=4769.28, duration=6.68), FetchedTranscriptSnippet(text='yeah the the vector the 256 dimensional', start=4772.48, duration=6.04), FetchedTranscriptSnippet(text='Vector corresponding to the first token', start=4775.96, duration=5.4), FetchedTranscriptSnippet(text='in the first batch basically this one', start=4778.52, duration=7.36), FetchedTranscriptSnippet(text='okay so now we converted token IDs', start=4781.36, duration=7.68), FetchedTranscriptSnippet(text='into uh token embeddings the next um', start=4785.88, duration=7.48), FetchedTranscriptSnippet(text='step is adding positional information so', start=4789.04, duration=6.679), FetchedTranscriptSnippet(text='let me see if I have a nice figure for', start=4793.36, duration=5.839), FetchedTranscriptSnippet(text=\"this so previously what we've done is we\", start=4795.719, duration=5.281), FetchedTranscriptSnippet(text='pulled out these embedding vectors from', start=4799.199, duration=4.361), FetchedTranscriptSnippet(text='the weight Matrix for each input example', start=4801.0, duration=5.28), FetchedTranscriptSnippet(text='and so I previously chose an input that', start=4803.56, duration=5.599), FetchedTranscriptSnippet(text='was yeah I would say um relatively nice', start=4806.28, duration=4.959), FetchedTranscriptSnippet(text='for explanation purposes where each', start=4809.159, duration=4.921), FetchedTranscriptSnippet(text='output row looked different so we had', start=4811.239, duration=5.44), FetchedTranscriptSnippet(text='fox jumps over doc these are token IDs', start=4814.08, duration=4.48), FetchedTranscriptSnippet(text='corresponding to an example input like', start=4816.679, duration=4.321), FetchedTranscriptSnippet(text='this and then we pulled out these um', start=4818.56, duration=4.8), FetchedTranscriptSnippet(text='embedding vectors corresponding to the', start=4821.0, duration=5.719), FetchedTranscriptSnippet(text='Token IDs from this weight Matrix now', start=4823.36, duration=6.6), FetchedTranscriptSnippet(text='what happens if we have the same token', start=4826.719, duration=6.52), FetchedTranscriptSnippet(text='ID for example fox jumps over Fox so', start=4829.96, duration=7.239), FetchedTranscriptSnippet(text='here this um token index 2 is the third', start=4833.239, duration=7.44), FetchedTranscriptSnippet(text='row in the weight Matrix and so we get', start=4837.199, duration=6.44), FetchedTranscriptSnippet(text='as the result this row here but since in', start=4840.679, duration=6.321), FetchedTranscriptSnippet(text='the last token ID we have the same yeah', start=4843.639, duration=6.161), FetchedTranscriptSnippet(text='the same word the same token ID it has', start=4847.0, duration=4.679), FetchedTranscriptSnippet(text='the same Vector I mean it makes sense', start=4849.8, duration=3.399), FetchedTranscriptSnippet(text='right we are looking up the position', start=4851.679, duration=4.441), FetchedTranscriptSnippet(text='here we pulling out the the vector and', start=4853.199, duration=6.201), FetchedTranscriptSnippet(text='create the output like that now this is', start=4856.12, duration=5.44), FetchedTranscriptSnippet(text='fine it would work but we could actually', start=4859.4, duration=4.319), FetchedTranscriptSnippet(text='add some more information to it so to', start=4861.56, duration=4.4), FetchedTranscriptSnippet(text='provide more information to the llm', start=4863.719, duration=6.48), FetchedTranscriptSnippet(text='depending on where this word is located', start=4865.96, duration=7.199), FetchedTranscriptSnippet(text='in the input we can create so-called', start=4870.199, duration=4.921), FetchedTranscriptSnippet(text='positional embedding information there', start=4873.159, duration=4.56), FetchedTranscriptSnippet(text='are different ways you can do that um so', start=4875.12, duration=5.32), FetchedTranscriptSnippet(text='gpt2 used a very simple way it used the', start=4877.719, duration=5.081), FetchedTranscriptSnippet(text='second embedding layer there are', start=4880.44, duration=4.48), FetchedTranscriptSnippet(text='nowadays other ways for example using', start=4882.8, duration=4.12), FetchedTranscriptSnippet(text='rotational positional embeddings if', start=4884.92, duration=4.239), FetchedTranscriptSnippet(text=\"you're curious about those in the bonus\", start=4886.92, duration=4.88), FetchedTranscriptSnippet(text='material um on my GitHub repository I', start=4889.159, duration=5.56), FetchedTranscriptSnippet(text='implemented also llama 3.2 from scratch', start=4891.8, duration=5.24), FetchedTranscriptSnippet(text=\"it's more advanced and more complicated\", start=4894.719, duration=4.321), FetchedTranscriptSnippet(text=\"and it's not as nicely described as in\", start=4897.04, duration=4.119), FetchedTranscriptSnippet(text='the book but they use something called', start=4899.04, duration=3.92), FetchedTranscriptSnippet(text='rope embeddings that I also implemented', start=4901.159, duration=4.401), FetchedTranscriptSnippet(text='here but gbt is yeah fortunately much', start=4902.96, duration=4.48), FetchedTranscriptSnippet(text='simpler in this case with the embeddings', start=4905.56, duration=4.56), FetchedTranscriptSnippet(text=\"it's using simply a second embedding\", start=4907.44, duration=5.04), FetchedTranscriptSnippet(text='layer so let me actually um show you how', start=4910.12, duration=6.24), FetchedTranscriptSnippet(text='that would work so so um we are using', start=4912.48, duration=6.52), FetchedTranscriptSnippet(text='the same embedding class we used before', start=4916.36, duration=5.359), FetchedTranscriptSnippet(text='but now we have an token embedding layer', start=4919.0, duration=4.4), FetchedTranscriptSnippet(text='in addition we have now a position', start=4921.719, duration=3.801), FetchedTranscriptSnippet(text='embedding layer where we add positional', start=4923.4, duration=4.36), FetchedTranscriptSnippet(text='information so the goal will be maybe I', start=4925.52, duration=3.8), FetchedTranscriptSnippet(text='have a figure yeah I do have a figure on', start=4927.76, duration=3.68), FetchedTranscriptSnippet(text='that if we have input embeddings like', start=4929.32, duration=4.6), FetchedTranscriptSnippet(text='this the goal is to have a second set of', start=4931.44, duration=6.239), FetchedTranscriptSnippet(text='values that we add here so um positional', start=4933.92, duration=6.96), FetchedTranscriptSnippet(text='embeddings that we add so we have sorry', start=4937.679, duration=6.361), FetchedTranscriptSnippet(text='token embeddings so to step through this', start=4940.88, duration=5.88), FetchedTranscriptSnippet(text='figure most slowly I chose the same', start=4944.04, duration=5.24), FetchedTranscriptSnippet(text='token embeddings for each um example', start=4946.76, duration=6.16), FetchedTranscriptSnippet(text='here just to keep things simple so um if', start=4949.28, duration=5.48), FetchedTranscriptSnippet(text='we have just input that consists of the', start=4952.92, duration=5.2), FetchedTranscriptSnippet(text='same word like um fox fox fox fox for', start=4954.76, duration=5.36), FetchedTranscriptSnippet(text='example the token embeddings would be', start=4958.12, duration=4.119), FetchedTranscriptSnippet(text='the same in all the cases but then we', start=4960.12, duration=4.24), FetchedTranscriptSnippet(text='add positional information on top of', start=4962.239, duration=4.721), FetchedTranscriptSnippet(text='that so that the actual input embeddings', start=4964.36, duration=5.64), FetchedTranscriptSnippet(text='look different depending on on where', start=4966.96, duration=5.719), FetchedTranscriptSnippet(text=\"it's located now I'm using these um very\", start=4970.0, duration=4.76), FetchedTranscriptSnippet(text=\"nice numbers where it's easy to read\", start=4972.679, duration=4.48), FetchedTranscriptSnippet(text='those numbers in reality these are all', start=4974.76, duration=3.84), FetchedTranscriptSnippet(text='starting with random numbers so it', start=4977.159, duration=3.281), FetchedTranscriptSnippet(text=\"doesn't look quite as nice but these\", start=4978.6, duration=3.44), FetchedTranscriptSnippet(text='numbers the position embedding numbers', start=4980.44, duration=3.92), FetchedTranscriptSnippet(text='they also get optimized automatically', start=4982.04, duration=4.56), FetchedTranscriptSnippet(text='later during the llm training so they', start=4984.36, duration=4.2), FetchedTranscriptSnippet(text='are random numbers to start with and', start=4986.6, duration=4.32), FetchedTranscriptSnippet(text='later they will get optimized similar to', start=4988.56, duration=3.4), FetchedTranscriptSnippet(text='the Token', start=4990.92, duration=3.799), FetchedTranscriptSnippet(text=\"embeddings so here um yeah I'm creating\", start=4991.96, duration=5.36), FetchedTranscriptSnippet(text='a second uh position embedding layer', start=4994.719, duration=5.561), FetchedTranscriptSnippet(text=\"essentially and then um what I'm doing\", start=4997.32, duration=6.6), FetchedTranscriptSnippet(text='so here is the max length um so the max', start=5000.28, duration=6.439), FetchedTranscriptSnippet(text='length is is yeah how many inputs are', start=5003.92, duration=6.2), FetchedTranscriptSnippet(text='supported in this case we only have four', start=5006.719, duration=6.44), FetchedTranscriptSnippet(text='right so we had a max length of four 1 2', start=5010.12, duration=5.8), FetchedTranscriptSnippet(text='3 four it can only support four input', start=5013.159, duration=5.161), FetchedTranscriptSnippet(text='tokens in practice uh this is a much', start=5015.92, duration=5.0), FetchedTranscriptSnippet(text='bigger layer so for Simplicity we just', start=5018.32, duration=3.56), FetchedTranscriptSnippet(text='keep it', start=5020.92, duration=4.6), FetchedTranscriptSnippet(text='small um for visualization purposes and', start=5021.88, duration=5.759), FetchedTranscriptSnippet(text='just Simplicity later when we train the', start=5025.52, duration=4.4), FetchedTranscriptSnippet(text='llm we will actually use much larger', start=5027.639, duration=5.08), FetchedTranscriptSnippet(text='values and then what we can do is we can', start=5029.92, duration=4.82), FetchedTranscriptSnippet(text='use uh torch a', start=5032.719, duration=3.121), FetchedTranscriptSnippet(text='[Music]', start=5034.74, duration=3.899), FetchedTranscriptSnippet(text='range so a range', start=5035.84, duration=6.56), FetchedTranscriptSnippet(text='is creating uh in sequential order', start=5038.639, duration=6.56), FetchedTranscriptSnippet(text='values here 0 1 2 and', start=5042.4, duration=6.68), FetchedTranscriptSnippet(text=\"three so 0 1 2 3 it's just a tensor it's\", start=5045.199, duration=5.921), FetchedTranscriptSnippet(text='a placeholder tensor for the positions', start=5049.08, duration=4.8), FetchedTranscriptSnippet(text='position zero position one two and three', start=5051.12, duration=5.84), FetchedTranscriptSnippet(text='and we have this position', start=5053.88, duration=6.12), FetchedTranscriptSnippet(text=\"embedding Matrix and it's a similar\", start=5056.96, duration=4.8), FetchedTranscriptSnippet(text='Matrix to what we had', start=5060.0, duration=5.76), FetchedTranscriptSnippet(text='before oops', start=5061.76, duration=4.0), FetchedTranscriptSnippet(text='and then when we are', start=5067.76, duration=3.68), FetchedTranscriptSnippet(text='calling this with our a', start=5072.96, duration=7.84), FetchedTranscriptSnippet(text='range what it will do it will just pull', start=5078.0, duration=5.44), FetchedTranscriptSnippet(text='out the values like the in sequential', start=5080.8, duration=5.28), FetchedTranscriptSnippet(text='order here so you can see it here so', start=5083.44, duration=5.48), FetchedTranscriptSnippet(text=\"it's just um getting me the values here\", start=5086.08, duration=5.44), FetchedTranscriptSnippet(text='and that is all that is really happening', start=5088.92, duration=4.799), FetchedTranscriptSnippet(text='in in this positional embedding layer', start=5091.52, duration=5.28), FetchedTranscriptSnippet(text='and then what we do is we add them on', start=5093.719, duration=6.641), FetchedTranscriptSnippet(text='top of each other so maybe to before I', start=5096.8, duration=10.08), FetchedTranscriptSnippet(text='execute this um this one has shape 8 for', start=5100.36, duration=9.359), FetchedTranscriptSnippet(text='256 and this one should have the same', start=5106.88, duration=5.96), FetchedTranscriptSnippet(text='shape so we can then add them to each', start=5109.719, duration=6.44), FetchedTranscriptSnippet(text='other I have not executed', start=5112.84, duration=7.319), FetchedTranscriptSnippet(text=\"this right here so um I'm sorry this is\", start=5116.159, duration=7.08), FetchedTranscriptSnippet(text='actually only for 256 because uh how', start=5120.159, duration=5.241), FetchedTranscriptSnippet(text='pytorch work I mean for efficiency we', start=5123.239, duration=4.48), FetchedTranscriptSnippet(text=\"don't need to duplicate this over the\", start=5125.4, duration=4.48), FetchedTranscriptSnippet(text='batch Dimension because this one will be', start=5127.719, duration=6.121), FetchedTranscriptSnippet(text='added to each batch um separately so', start=5129.88, duration=7.0), FetchedTranscriptSnippet(text='what happens is essentially um that when', start=5133.84, duration=5.64), FetchedTranscriptSnippet(text='we have something like this um the input', start=5136.88, duration=5.0), FetchedTranscriptSnippet(text='embedding has the same size as the sorry', start=5139.48, duration=3.719), FetchedTranscriptSnippet(text='the token embedding has the same', start=5141.88, duration=3.0), FetchedTranscriptSnippet(text='Dimension as the position embeddings and', start=5143.199, duration=3.241), FetchedTranscriptSnippet(text='then the input embedding will also have', start=5144.88, duration=4.359), FetchedTranscriptSnippet(text=\"the same Dimension so here um there's\", start=5146.44, duration=4.719), FetchedTranscriptSnippet(text='the concept is called broadcasting in', start=5149.239, duration=3.721), FetchedTranscriptSnippet(text='pytorch', start=5151.159, duration=5.281), FetchedTranscriptSnippet(text='so um Let Me Maybe show it to you like', start=5152.96, duration=8.04), FetchedTranscriptSnippet(text='this so when we do', start=5156.44, duration=4.56), FetchedTranscriptSnippet(text='this we can add these values together so', start=5161.44, duration=4.92), FetchedTranscriptSnippet(text='this is the first example in the', start=5164.719, duration=3.44), FetchedTranscriptSnippet(text='training set and then our position', start=5166.36, duration=3.96), FetchedTranscriptSnippet(text='embedding vector and this would be', start=5168.159, duration=6.08), FetchedTranscriptSnippet(text='essentially similar to doing this except', start=5170.32, duration=5.799), FetchedTranscriptSnippet(text='now we are doing it for all the examples', start=5174.239, duration=4.841), FetchedTranscriptSnippet(text='in the batch but the first the first', start=5176.119, duration=5.761), FetchedTranscriptSnippet(text='example maybe we can see it here um', start=5179.08, duration=4.92), FetchedTranscriptSnippet(text='should be similar so you can see this', start=5181.88, duration=3.88), FetchedTranscriptSnippet(text='one is similar to this one I mean this', start=5184.0, duration=3.6), FetchedTranscriptSnippet(text='is like a rounding thing not a rounding', start=5185.76, duration=4.6), FetchedTranscriptSnippet(text='thing a different uh print', start=5187.6, duration=5.88), FetchedTranscriptSnippet(text='representation but this row is the same', start=5190.36, duration=5.56), FetchedTranscriptSnippet(text='as this row essentially and so forth so', start=5193.48, duration=5.36), FetchedTranscriptSnippet(text='this is um just generalizing it over the', start=5195.92, duration=4.759), FetchedTranscriptSnippet(text='whole batch', start=5198.84, duration=4.279), FetchedTranscriptSnippet(text='Dimension oops let me just clean this up', start=5200.679, duration=6.52), FetchedTranscriptSnippet(text='a bit and then when I add them together', start=5203.119, duration=6.881), FetchedTranscriptSnippet(text='like before the shape should be the same', start=5207.199, duration=5.401), FetchedTranscriptSnippet(text='as before so we are just adding this', start=5210.0, duration=4.639), FetchedTranscriptSnippet(text='additional yeah position information to', start=5212.6, duration=6.0), FetchedTranscriptSnippet(text='it and so with that we now', start=5214.639, duration=8.441), FetchedTranscriptSnippet(text='have covered the whole input pipeline so', start=5218.6, duration=6.639), FetchedTranscriptSnippet(text='we have input text that we break down', start=5223.08, duration=4.88), FetchedTranscriptSnippet(text='into tokenized text then we convert it', start=5225.239, duration=5.761), FetchedTranscriptSnippet(text='into token IDs then we create these um', start=5227.96, duration=5.159), FetchedTranscriptSnippet(text='token embeddings with our token', start=5231.0, duration=4.639), FetchedTranscriptSnippet(text='embedding layer we create the position', start=5233.119, duration=5.441), FetchedTranscriptSnippet(text='embeddings add them together to get the', start=5235.639, duration=6.321), FetchedTranscriptSnippet(text='input embeddings and so this is all we', start=5238.56, duration=6.48), FetchedTranscriptSnippet(text='need to know for now for the data', start=5241.96, duration=5.36), FetchedTranscriptSnippet(text=\"preparation step next let's actually\", start=5245.04, duration=5.679), FetchedTranscriptSnippet(text='tackle the inside of the GPT like', start=5247.32, duration=5.839), FetchedTranscriptSnippet(text='decoder only Transformer so what is', start=5250.719, duration=4.721), FetchedTranscriptSnippet(text='actually inside the model one more thing', start=5253.159, duration=6.08), FetchedTranscriptSnippet(text='I should say the embedding layers are', start=5255.44, duration=5.64), FetchedTranscriptSnippet(text='technically part of the GPT model as we', start=5259.239, duration=4.721), FetchedTranscriptSnippet(text='will see later I just draw them outside', start=5261.08, duration=5.039), FetchedTranscriptSnippet(text=\"here because it's a bit easier to talk\", start=5263.96, duration=3.759), FetchedTranscriptSnippet(text='about it since we already talked about', start=5266.119, duration=3.641), FetchedTranscriptSnippet(text='the data laer but yeah in the next in', start=5267.719, duration=4.601), FetchedTranscriptSnippet(text='the next um chapter we will talk more', start=5269.76, duration=4.76), FetchedTranscriptSnippet(text='about the attention mechanism that is', start=5272.32, duration=5.08), FetchedTranscriptSnippet(text='inside of a GPT model so I hope this was', start=5274.52, duration=5.08), FetchedTranscriptSnippet(text='useful and I hope I will see you in the', start=5277.4, duration=5.12), FetchedTranscriptSnippet(text='next video', start=5279.6, duration=2.92)], video_id='341Rb8fJxY0', language='English (auto-generated)', language_code='en', is_generated=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yeah hi and welcome back everyone to thesupplementary coding Along video seriesfor the build a large language modelfrom scratchbook here we are now goingto talk about chapter 2 working withText data which means essentially thatwe are going to prepare the data that weare going to use to train the llm yeahjust to um go to the chapter itselfchapter 2 you can see there are lots ofsections starting at the top the thegoal really here is to prepare the dataset for the llm so there is actually anice figure here that shows you thisstep in the grand scheme of the wholellm building process so here building anllm means setting up the data set codingthe attention mechanism coding the llmarchitecture and so forth this is reallythe first stage of you know training thellm and then the pre-training comes instage two and then later on in stagethree we will be fine-tuning the llm butyou know just to keep things focusedhere we are going to talk aboutpreparing the data set and yeah to giveyou another view here um essentiallywhat we are going to do is we are goingto take text and tokenize it and thenconvert thesetokenized uh subp parts of the text intotoken IDs which we will be then encodinginto vectors I know this is maybe a bitFAS here in this video so I mentionedbefore for the videos are not um yeah asdetailed as the book itself and they'remore focused as supplementary resourcefor the coding examples but yeah just togive you the big picture here so thegoal is really to yeah prepare raw Textdata so that the llm can then process itit's really that the llm works with textbut you know we have to first convert itinto a format that it can be um yeahthat can be read let's say in a formthat the llm understands which in thatsense here is actually vectors likemathematical vectors and if I scroll upa bit it's really like the idea alsothat yeah vectors are numericrepresentations here of this text so sothat is our goal going from letters andwords into a more numeric um yeahrepresentation which can then be usedlater on during the um yeah pre-trainingto optimize the weight parameters ofthat llm but yeah um I don't want to gotoo far ahead and talk aboutpre-training the focus here really is onpreparing here the input data for thellm and specifically in section twotokenizing text the focus is really alsoonly on this subar here at the bottomwhere we take some input text and weyeah we break it down into individualsmaller chunks and um yeah and this is aprocess known as um yeah tokenizationjust to show you an exampleso uh we will be using a library uhcalled tick token later and um there isI think anice nice um yeah virtual visualizationum tick tokenizer uh when we go here andthey do have different llms for examplethere is this um gpd2 llm if we typesome um text let's say um hello worldsomething like this um thisis a test so here you can really see umhow it's broken down into individualtokens so you can see Hello is a tokenthe comma is a token world is a tokendot is a token and these are the tokenIDs we will get to the Token IDs lateron how they are created for now thefocus is really like to just break itdown into individual parts and we arenot going to implement actually theexact same algorithm that is used here Ido have some bonus material on doingthat at will share later um here firstwe want to do the big picture level ofhow we can actually break down text intoindividual chunks so one way would beessentially to yeah use just regularExpressions but um yeah startingstarting sequentially like how how I didin the book Let's actually download thedata set first and we will be using thedata set later in this chapter um toactually yeah implement the data lus inpytor so let's um let's start with adata set here before I go too much aheadand talk about all the other things sowe are going to use a library um OSwhich is just a standard python libraryand U URL lip request which is also astandard python Library this is usuallyfor working with files and this is fordownloading things from the internet sothe data set we are going to work withhere um it's called um the verdict solet me just go here and new tab um andthe verdict is a short story by edit wonWharton soit's available as public domaindata um I think there's a Wikipediaversion that is actually quitenice here so this is a very short storyhere and I picked this one as the dataset because yeah we want to keep thingssimple that it doesn't take too longwhen we actually pre-training the llmbecause it's more for educationalpurposes and it's also you know prettysimple readable and most importantlyit's a public domain um data set or bookso that means we can use it withouthaving to worry about um copyrightrestrictions for example in the UnitedStates at least so in that case this isan open public domain data set there isno yeah concern using this for examplefor Mtraining um okay so we would downloadthis data set so we could um what wecould do is we could go here just copyand pastethis and create a new file a text fileinsert this and save it and so forth umI have a little bit of a quicker wayhere so I actually let me just copy andpaste this I have it also in the GitHubrepository for the book and so this linkis a URL link to the place on GitHubwhere I provided this data set just incase you know in the future the data setmight become unavailable from Wikiumedia or something here you can find aversion in the books GitHub repository Rasbt llms Das from- scratch and inchapter one so sorry chapter two in thefirst folder main chapter code Iincluded the verdict here and if youwant to copy something from uh yeahGitHub is raw text you have to click onthis raw here and then yeah you get thethe raw text which in this case um yeahas you can see issimilar to what you can see on this pageso if I copy this URL now insert it hereit's really the same URL that you haveseen earlier without the line breakswhen I execute this code it willdownload this um data set and save it toa file the- word. text let me do that umand then maybe refresh here and yeah youcan see it's now in this folder so it'slocally now on our computer The NextStep would be opening this file inPython so right now it's just sitting inour computer and the next step would beum yeah opening this so let mejust copythis and we are just reading it so the rhere and then on Windows it's I thinkrequired to specifically say whichencoding we are using so utf8 thestandardencoding um yeah and then we canjust read the raw texthere and then let's double check so wecan double check that the text looksokay so we can see y the pythoninterpreter has successfully loaded thetext hereand we can also just double check youknow how long it isso I maybe recommend running this if youare running the code to check that youget approximately the same number herewhich means you're not missing anycharacter so it should be about2,479 characters if you loaded the dataset correctly so let's now um brieflyintroduce the concept of tokenizationwith regular Expressions which is justlike a warm up um before weuse use thereal let's say tick token library fortokenization so we are going to use apython regular expression and let'sactually use some simpler text let's forthe warmup here let's use the text Ijust typed inhere and um provide it here then we cantype result and then the regularexpression and I I must say that regularexpressions are my weak spot so I'mreally not that good at them buthonestly um nowadays with llms like chatGPT it's actually pretty easy to figureit out so this is essentially just aregular expression that splits on whesSpace characters and by the way youdon't have to really know what regularexpressions are for the bigger contextof training llms because we are later umusing a tokenizer based on bite pairencoding and I also have a from scratchimplementation it's not in the book butit's in the supplementary materials thatI can also maybe show you laterbut yeah for Simplicity our goal here ifyou remember what I mentioned earlierour goal here is first to split up thistext into individualtokens so let's do that with thisregular expression and then maybe Uprint the result and see how it lookslike and so yeah we can see we have nowthe individual words and we have whitesspace characters and so forth um onething um we might want to do is also tohave the punctuationas um separate characters so for that wewould have to make a little bit of amore sophisticated regular expressionand like I told you I'm not very good atregular Expressions so let me just umcopy and paste it here so this is now aregular expression that is slightly moresophisticated um so this one will alsoinclude the punctuation as separatetokens where before they were part ofthe word itself okay so this is oursimplest um way of you know tokenizingnow as you have seen here if I go herethere are actually no whes spacecharactersum in in the output here so one thing weto mimic this could be for example thatwe are also going to strip the whitesspace characters this is just like anoptional thing we could do we can justget rid of the whites space charactersso here I just have a simple listcomprehension that will strip out yeahthe whites space characters so you cansee now that is the result um yeahwithout whites space characters it's abit different from what you see herebecause this is a sophisticated bitepair encoding algorithm and I will talkmore about this later I will share someuh we have a dedicated section on thisactually but so here for now we're justbuilding a very simple version of thatto understand roughly yeah what we aredoing here like um how we are tokenizingtextum doing it a bit more advanced now Imean this is a nice way to um yeahhandle simple cases but now imagine wehave a more complicated case like thatso this this will actually fail reallyit doesn't really take care of specialcharacters like you know the Double Dashand everything so we can actually makeit more sophisticated so I have now likeum more sophisticated regular expressionand also again the white space strippingand that does a good job of you knowseparating words and special tokens sowe did a lot of work here now to yeahprepare our our text um by splitsplitting it into individual tokens andpunctuation characters so we tested iton this simple example but let'sactually use this on our um yeah on ourreal um text so for that what we have todo now is we have to actually use ourraw text where raw text is what we havehave here at the top so that's our rawtext let's actually see what happenswhen we do that so yeah so we tokenizedour raw text and now let's see how manytokens we have so we have uh length oflet me actually save this asa variable because this is a lot of codeum let's just call it preprocessedandthen so yeah we have here4,690 tokens so what we have done so farin section 2.2 is we took raw text andwe tokenized it and now in the next umyeah section we will be talking aboutconverting this raw text into theseso-called tokenIDs yeah let's now talk about convertingthe tokens into token IDs so previouslywe talked about breaking down the textinto individual tokens here and now inthis video we are going to talk aboutconverting the tokens here into tokenIDs which are so-called yeah uh tokenIDs um and they are essentially uniqueintegers so how do we do that the firstFirst Step let's maybe go to section 2.3here the first step would be building aso-called vocabulary so the vocabularyis a unique mapping between yeah eachword that can occur and a unique integerso for example for Simplicity if we havea very small data set here the quickbrown fox jumps over the lazy dog thefirst step would be yeah breaking downthe text into tokens which we have donebefore and now we are going to yeah sortthem alphabetically making sure sure wedon't have duplicates in there and thenwe are going to assign a unique mappinghere to integers so how do we do that inPython um so we already have thepre-processed um from the previoussection so pre-processed is all theunique tokens in in the data set and sowhat we can do now is we can get rid ofthe duplicates first so we can do um setpre-processed which will get rid of theduplicates and then um let's also sortthem with a sorted function in Pythonand we can yeah assign all words avariable to hold them so let's just makesure it worked so we can see these areall the unique words it's a lot ofwords we can actually also look at howhow many we have how long this list isso that'sabout, 130 words so it's yeah four timessmaller than pre-processed so we had abunch of duplicates in there um let'sactually save that for later we callthat the vocabulary size so how manyunique words we have and um yeah so thevo vocabulary sizeis shown below1,130 so next let's actually build thevocabulary this is actually quite simplein in this illustration for educationalpurposes what we can do is we can foreach token assign an integer a uniqueinteger for each um integer token inenumerateall oops all words so what it will do isit will oh yeah iterate over all theunique words and then assign inascending order these integer labels sowe can actually take a look at this soyou can see it's yeah essentiallymapping each word to an integer here umyeah and this is essentially um yeahwhat we're doing here so now we have thevocabulary so maybe going to the next umfigure so the next Next Step would benow that we have this vocabulary we usethis vocabulary to actually tokenize umtraining data into token IDs you can seethe vocabulary is in a sorted order sowe have them alphabetically sorted withthis mapping and now for any new textfor example the training text but alsolater on when we want to use the llmwe're going to use this vocabulary toconvert any text into token IDs here sobasically using the vocabulary mappingif we have the word the we would findthe word the in the vocabulary and findthe corresponding integer for example sothe here it's not shown because it's toolong would correspond to integer sevenand then the second word Brown wouldcorrespond to integer zero and then docwould correspond to integer one and soforth and so this way we are going toencode um the given text into thesetoken IDs so let's um yeah do this inPython so um I will actually uh copy andpaste here because otherwise it willtake too long if I just type it and Iwill make errors because typing in frontof a camera is quite different than umcoding in yeah uh silence but um so herewhat I'm going to show you is a simpletokenizer class and that's a pythonclass and that does the following thingsso in it is um usually yeah uh executedwhen you are instantiating a new objectwhich we will be doing later let me justmaybe first explain and then we willcreate a new object here but um so in itis uh the Constructor it takes avocabulary and it defines um or willjust save the vocabulary as the stringto integer mapping so string to integeruh means that we are mapping fromstrings to integer representations andthen we also have the same thinginverted so we are here instead ofhaving i s we have SI which invertsstring and integer so what it will do isit will yeah create this mapping frominteger to string now so we have integerand string inverting this essentially umyeah so here we have now twovocabularies the the regular vocabularyand the invertedvocabulary and then we have an end codemethod that takes any text and uses theregular expression from section 2.1 uhsorry 2.2 and breaks down the text intotokens so this is what we have seen umearlier this is quite the longvocabulary if I scroll up so let meactually maybe comment thisout so earlier in the previous uh videowe have yeahum broken down text into tokens so weuse this regular expression so here weare just going to reuse that regularexpression from earlier and then stripout the white spaces and then here thisis the interesting part this is where weare creating the token IDs so we aregoing to use the vocabulary from hereand for each token or string inpre-process we are going to yeah convertit so just to show you what that meansso if I just take let's say Jack hereand I pass it to my w cap so if I havesomething likethis so this should give me an ID rightso this is essentially yeah the thetoken ID and I can also maybe just showyou to create a reverse mappinghere so if we want to go back to stringand type the 57 this should give us theoriginal word back so this is basicallyWhat's Happening Here we have uh avocabulary inverse vocabulary here weare just going from string to integer sostring to integer means this one here weare going from string to this token IDand we do that for each token ID in thein the data sethere and the data set is yeah thetokenized version of thetext yeah and this is really it and thenwe have a decode method and the decodemethod goes backwards so here we aregoing to uh recreate the text by mappingback from integers to string which iswhat I've just shown you so integers tostring means that we go here from theinteger back to the string and I thinkyeah this might be relatively yeah Imean complicated maybe not but uh it's alot of information so in this case let'sactually use it and I think it becomesmore clear when we actually use it solet's uh dosimple unfortunately I don't haveautocomplete here so let me just copy itfrom here let's actually justinstantiate a new object um using thevocabularyum like this oops not defined I need toexecute this cell first this alwayshappens when I um yeah talk and code atthe same time I forget things um thenlet's just use some text here instead ofme typing it I'm just copy and pastingit and so the first step is convertingtext into these IDs so let's dotokenizer encode and then text so thisis essentially calling this encodemethod and then we will hopefully seethat those are converted into token IDsso we can see now this would be thetoken ID representation for thisparticular text um yeah and we couldalso go backwards so if we have thesetoken IDs we can do tokenizerdecode and IDs and this would correspondto this code this will be convertingthis back into text essentially and yeahwhat's nice is you can do also the roundtrip which means means um what we can dois let's do it like this decode and thenput this encoding inside here and thisshould give us the original text orsomething that is close to the originaltext you can see there are few slightdifferences for example here we havethis additional white space for examplebut um overall this works quite well wehave a way of yeah um converting textback into token from token back intotext um so this would be a very simpletokenizer that's not the real tokenizerthat is used by GPT models that's whyI'm calling it simple tokenizer but Ithink um this illustrates the concept oftokenization quite nicely how we go fromtext into integer presentations um andnow in the next video we will talk a bitabout yeah Special coding uh specialtokens and how we handle thoseall right let's now talk about addingspecial context tokens so previously wetalked about creating the so-calledvocabulary from all the different tokensin the training set and then we use thatvocabulary to create the token IDs umnow we are going to talk about extendingthis vocabulary with special tokens sofor example um we might want to adddepending on the tokenizer a token forunknown words that are not in thetraining set or for example we couldindicate when the there's a end of thetext by using a so-called end of texttoken and we will talk more about thisend of text token later on when we areactually um pre-training and fine-tuningthe llm so here though the goal isessentially to extend our simpletokenizer a little bit to handle thesespecial tokens and I also wanted to justshow you one shortcoming of ourtokenizer so if I go up here we definedour simple tokenizer herebased on the vocabulary if I have sometext now that is slightly more I I wouldsay sophisticated so before we used thistext here if I have text let's say umlet's let's do something not supersophisticated but something differentlike hello maybe do you like T umis I don't know this a test for exampleand let's try to see what our tokenizerdoes when we actually um try to encodethis so as you can see there is an errornow it says key error hello and hello isa very normal word so why would there bean error and the reason is because thetext that we processed apparently so thethe verdict here apparently it does notcontain the word hello now if you arecreating U yeah an llm and train it ontrillions of tokens that chances arethat this word would be in the trainingset um for example here we have onlythis very small data set it might noteven contain simple words as um hellofor example and by the way later on Iwill also tell you more about analgorithm that can naturally handle umunknown words so for example if the wordis unknown it will just break it downinto individual characters so maybe toshow you what I mean so if I type justsome something like this you can see ifif it doesn't know word it will um breakit down into individual characters butthis is a more advanced algorithm fornow how can we actually um you know makeour simple tokenizer slightly moresophisticated by handling um yeahunknown words so what we could do is wecan uh extend our tokens so if we haveall tokens uh which werethe list ofthe pre-processed text tokens or birdsso this would essent entally recreateour uh yeah unique tokens from the dataset and now what we could do is we couldactually extend it so ifweuse the extent method on on python listswe can actually add additional tokensfor example we could end add this end oftext token that I mentionedearlier or we could also add a token forunknown words so we can really addanything we like here so just extendingit and so the difference is to what wedid before is we are yeah creating itsimilar to before but now we are addingtokens that are not already in the dataset and then yeah we would um generatethe vocabulary similar to before so thisis the same um that we used before likethis token um to integer mapping for thevocabulary um and then yeah let'sactually uh take a look at um the lengthof the vocabulary I think before we hadlet me scroll up a bit1,130 now we should have 1001,132 tokens let's double check yep sowe have these two new tokens added andlet's actually print the first um or thelast in this case the last five entrieswe can do the first five they shouldlook similar to what we've seen beforeso just the yeah special characters butmore interestingly let'sactually print the last five tokens inthe vocabulary and we can see we nowhave these two new special tokens in thevocabulary in a similar way we can nowmodify our simple tokenizer to actuallyuse these um special tokens so what wehave to do now is we have to essentiallyadd a Special Rule yeah to to processthose so I will just um copy and pasteit here so we return this string if thestring is um not empty and if it's inthis vocabulary and otherwise if it'snot in the vocabulary so else we willreturn unknown for this unknown tokenand this way we will prevent that thisuh modified version of our simpletokenizer crashes on words such as hellohere forexample yeah and this is all we we doessentially it's a simple modificationand this should actually help us to runour Simple Text example let's just umcopy this one for Simplicity so we arereinitializing the tokenizer now we havetokenizer version two and I will'll makea new code cell and then tokenizerencode text and this hopefully works nowand yeah you can see we now have thisunknown token so actually when we goback let's see what happens so tokenizerdecode yeah you can see now it handleduh yeah the unon tokens this is ofcourse not desirable if we want toregenerate the original text and lateractually in the next uh sections I willtalk also about an algorithm that willmake this a bit more um sophisticated uhit's called bite pair encoding and Iwill explain how bite pair encodinghandles unknown tokenslet's now talk about bite pair encodingwhich is an algorithm that will help usto take our tokenizer to the next levelso bite pair encoding is an algorithmthat has been around for about 30 yearsor so but it's yeah it's really popularfor implementing tokenizers these daysand for example gpt1 gpt2 3 and fourthey are all using this bite pairencoding algorithm for their tokenizerand even other companies for examplemeta AI for their recent 3 modelsthey're using bite par en coding and sobite par en coding really helps usaddressing one major shortcoming of ourtokenizer so if I go up again a few umSS here one of the shortcomings of oursimple tokenizer was that that it wasn'table to handle unknown words so forexample in this case we had this wordhello and since hello was notrepresented in the training data itwould raise a key error and so the waywe handled that was that we yeahreplaced um unknown tokens um by theunknown placeholder token here so we hadthis Special Rule here to replaceanything that is not contained in thevocabulary so anything that it hasn'tseen during the training by theseso-called unknown special tokens so whathappens is for example if we have sometext like this here so these uh unknownwords get replaced by Unk placeholdertokens and this one as well but the mainshortcoming of this is now to the llmthis all looks the same so the LMdoesn't really know what's here andwhat's here because it receives the sametoken um yeah ID for both of these casesand in many many um real world taskwhere you have um specific names oranything that were not included in thetraining data the LM wouldn't have anyidea what you are talking about or whatyou're referring to if you have multipleof these unknown tokens in your text andso forth and yeah this is not ideal sothe bite pair encoding algorithmis a way that it can always break downany type of um yeah word into sub tokensso to show you an example let's gobriefly to this um Tik to tokenizer appagain and we will be in this video alsousing the Tik token app or sorry pythonlibrary in a few moments but if I typesome let's say some unknown word andthen something like this here you cansee it's actually breaking it down intosubword so uh what happens heremaybe let's fix this it's even nicer sowhat um what happens here is that I betyou that this is something that was notcontained in the training data for gpd2but nonetheless it's able to break itdown into individual tokens withoutfailing so without raising an error orwithout yeah inserting a placeholdertoken or anything like that so whathappens is it's breaking down longerunknown words into known subwords so ithas for example seen something like sumin its training set or unknown so theseare unknown words so you can see hereit's breaking them down successfullyinto individual sub tokens and then ifwe have a string like this um this issomething that doesn't really correspondto anything in the training data itwould then always fall back toindividual characters so in this case itwill just use a single character as atoken so in this way it will never failbecause it will always fall back tothese individual characters if there'san unknown word now this is maybe a bitinefficient because now one word becomesuh a lot of tokens uh so it's not themost efficient way to encode input textif you have a lot of unknown wordshowever it will never fail in this caseso this is one uh improvement over ourprevious yeah method which would justreplace unknown tokens with this Unkplaceholdertoken um so if you're interested in thebite pair encoding algorithm how itworks so open AI they actually opensourced uh yeah the inference method forthis bite pair encoding algorithm sothey have this gpt2 GitHub repository soum it should be just open gpd2 and thenin this folder SRC and then encoder theyhave the bite pair encoding implementedhere to break down yeah uh words orinput text into these sub tokens nowthey don't share the training method forthis so they only share um the code thatyou use when you have already trainedthis um yeah tokenizer however if you'recurious I implemented this from scratchso if you scroll down here in the bonusmaterial of the book so this is myrepository rsbt lm- from- scratch um andthen you go to the spite pair encodingtokenizer from scratch link this willbring you to a notebook a jupyternotebook where um this implements thebite pair encoding um yeah step by stepfrom scratch including trainingum and then also at the end I have amethod for loading the open eii weightsinto into this so This should worksimilarly to what open AI um used foryeah for the gpd2 model so if you areinterested in more details um yeah Irecommend checking this out but no knowthat this is only optional this ispretty Advanced and this is also a verylong notebook which is why I didn'tinclude it in the book itself becauseyeah in in practice the the book isabout llms and this is a whole differenttopic this could be actually the topicof a whole other book because it'sactually quite complex so the mainmessage here really is that we are goingto take text un unknown tokens andeverything and we have now a way tobreak it down into individual sub tokenswith this bpe method and in practice umwe are going to use the library calledtick token which is an open sourcelibrary from openi so the book um thechapters will Implement everything fromscratch because yeah that's the title ofthe book build a large language modelfrom scratch but I would consider thetokenizer not really part of the llmitself so in this case um the tokenizerhere we are going to use this libraryfrom Tik token because it's actuallyalso very nice and efficient and it'swhat most people use in practice um sothe reason why it's efficient forexample more efficient than thisimplementation here um this otherimplementation here from openi is thatyeah the the most expensive functioncalled or code is implemented in Rustwhere rust is a different um yeahprogramming language that is often usedto implement high performance code andthen to make it um yeah easy to usepeople usually add a python API aroundthis so we're going to use this libraryin Python but most of the core parts areimplemented in Rust to make it reallyfast and inference so what we're goingto do is we're going to implement orsorry to import the tick tokenLibrary oh here we go so the Tik tokenlibrary is um yeah helping us then touse this gpt2 tokenizer and by the waywhy gpt2 and not gpt3 or four that'sbecause later in chapter 5 we will betraining a small gpd2 style model andthen we will also later see how we canlaot the pre-trained weights from OPIwho shared the gpd2 model weightsUnfortunately they didn't share the gpt3or gp4 model weights yetum but the architecture of GPD 3 is thesame as two and so forth in in any caseso we are going to use the gpd2tokenizer to make it compatible to themodel we are going to implement later inchapter 5 and um so yeah we can importThe Tick token Library like this if youhave yeah um set up your pythonenvironment as described in the firstvideo with this requirements. text filethis should um automatically work if youget an import error that means youprobably haven't installed the Tik tokenpython Library so you could either useUV pipinstall or just um pip install uh Tiktoken to install um yeah this Library soif you would execute this code cell herethis would install the Tik token Libraryso but I'm commenting it out because Ihave it already installed and so next umlet me also just briefly double checkwhich version I'm usingso it's always a habit of mine to doublecheck CU yeah you never never know ifyou run the notebook in a few years inthe future maybe if you get a differentresult it could be due to a differentversion of the library so I'm using 0.9but it should also be fine if you havean older version like 0.7 or 6 orsomething like that but yeah if you getsome weird results that don't reallymatch what you expect um making sure youhave the correct version is never a badidea so you could then also U fix thisversion number like this forexample okay so how are we going to usethe Tik tokenizer now so we we going toinstantiate a new tokenizer object andum we are going to use the gpt2tokenizer like I mentioned before so wedo um get encoding and then gpd2 likethis and this would instantiate the gpd2tokenizer with the full vocabulary theway it was trained and everything sothis is already ready to use we don'thave to train anything and then theusage is is exactly the same as our ownsimple tokenizer this is why we wentthrough all the work here umimplementing this because it helps Ithink illustrate how encode and decodework like these methods and this yeahtick tokenizer is using exactly the sameum yeah thesame API so you can see we are encodingthis now and then there isalso umtokenizerdecode like this and this gives us backthe original text which is exactly whatwe had before now let me actually showyouum a more advanced case where I havesome copy pasted some text here andlet'sactually try to encode this text sobefore I mentioned to you that it canhandle um arbitrary words so it isactually uh able to you know break downany words into sub tokens so let's seeactually what happensso you may notice now that actually itraises an error and why is that so thereis one interesting thing about the gbdto tokenizer and so one is that they usethe so-called special end of text tokenand I think I glanced over this in theprevious videos but the end of texttoken is usually used if I can find thefigure here um it is usually used toconcatenate texts um let me double checkyeah here so when you are preparing thedata set for training an llm you usuallyhave multiple documents and to yeah tosignal to the llm where one documentends and where the next document startsyou usually add this end of text tokenyou can either add it at the beginningof the next document or at the end ofthe previous document that doesn'tmatter but you usually have this end oftext the limiter to show where a newdocument starts so that the llm knowsokay this is a new new document now it'snot part of the previous document andthis is also used in gbt2 for example uhand it's a special token that they usedbut it's not by default enabled in thistokenizer which is why it's complainingso what you have to do is you have tounfortunately um yeah add this as aspecial token so you have to be veryexplicit here and then add this umspecialtoken like this and yeah you can seethis one works now and so this um end oftext um corresponds to 50,2 56 which isessentially the vocabulary size of thisum tokenizer so the vocabulary has 50,256 entries in this case um and so onemore thing I wanted to mention is itmight look like it's yeah not workingwell with things that is not denoted asa load special but it would work withany other token so I can just you knowadd arbitrary text here and it wouldwork like this so uh like we said beforeit can break down arbitrary text it wasjust an exception with this um end oftext token and yeah and so this is in anutshell how the bite pair encodingalgorithm works so it's essentially amethod for breaking down arbitrary textinto subware tokens and if you arecurious about more details how it'simplemented I highly recommend um nothighly but I suggest checking out thissource code I would say I would nothighly recommend it at this pointbecause it's pretty Advanced and it canyou know it can be a bit confusing andyeah if you like you can also check outmy my implementation here where I addeda few more comments and hopefully makingthis a bit more readable but yeah againthis is Advanced material um I would saythis would be almost topic for anotherbook it's not really anything you haveto necessarily understand forimplementing an llm the core message iswe have now an algorithm that breaksdown text into tokens and in the next umyeah we will talk about um implementingthe datasample let's now talk about datasampling with a sliding window so whatthat means is previously we talked aboutcreating these subw word tokens from agiven text and then we converted thesubtext tokens into token IDs and laterwe will learn how to convert them intovectors embedding vectors and then passthem to the L&M but for now the focus isreally still on this part here how canwe do this efficiently for instance thellm it can't receive all the tokens allat once as input so the the topic of thesection is how do we provide smallerchunks of these token IDs to the llmefficiently so that we can later trainthe llm efficiently so let's go tosection 2.6 here um there's one morething I wanted to mention before we goto the code examples llms essentiallyone of the home marks of llms is thatthey are predicting one token at a timeso if we have an input text for examplellms learn to predict one word at a timeif that is our input text let's considerthe first token if we pass only thistoken ideally the nlm should yeahpredict the next token and here in thenext row LMS learn the next token is twoand so forth so the goal is um yeah toteach the llm to predict one work at atime and um this is one of theinteresting things about LMS this makesthem so efficient at scale in terms ofbeing able to train them on very largedata sets in traditional machinelearning we usually as humans had tofind ways to create labeled data so hereit's actually very easy because we canuse raw text and the label createsItself by just being the next token sowe are just taking text we hide part ofthe text um for example here we hidethis text we provide one token as inputand then the next token is the targetlabel that the llm learns to predict wewill learn more about how this works forexample optimizing a loss functionpredicting next tokens and so forth inlater chapters here the goal is to justprepare the data in in a similar formatso that when we pass an input to the llmthe llm has then the next Target that itcanpredict so if I go to my code here sowhat we are going to do is we are goingto yeah do this efficiently for our theverdict data set so just toum provide it here again the verdict wasthe short story by Edith Wharton that wedownloaded earlier and what we're goingto do now is we're going to use the ticktoken tokenizer to encode this intotoken IDs and yeah if we do that we cansee we have5,000 145 um token IDs so just maybe tovisualize them can see them so these areall the yeah token IDs in our text andnow the goal is how can we provide themefficiently um yeah to the llm in in theform of sub chunks so you can see it's alot of tokens and the LM wouldn't beable yeah to pass them all so we want tofor example provide chunks of four fourbecause yeah it's nice and small andfits into this Jupiter lab and wevisualize it and I can make nice figuresbut in practice the chunks are usuallymuch larger they're usually th000 4,0008,000 tokens at a time depending on thellm here um in the case of gpd2 I thinkit was 1024 tokens that were used at atime during pre-training we are in thisparticular notebook only using four forvisualization purposes just so that Ican show you some things moreefficiently in a figure um so yeah thatis what I wanted to say here and we arealso going to use um a particular subsetum we are truncating the first 50 tokensum just to have some nicer examples andsome nicer text but this is just like animplementation detail to yeah to uh tomake this visually a bit nicer now whatI wanted to show you now is the way weyeah prepare the X the inputs and the Ythe targets I kind of showed it to youhere already that we are concernedalways with the next um the next Targettoken here so we um we are creating adata set here where the targets are theputs shifted by one position which thenhelps us to have the targets as the nexttoken to the llm um so for instance herewhat I'm doing is I have this sampletext so the sample text is um it shouldbe about 5,000somethingoops tokens so 5,95 tokens because yeahI just truncated some tokens so you have5,95 tokens and we have a context sizeof four so we are only considering fourtokens so here we are going to take thefirst four tokens and then we shift itby one position um so that the targetsare yeah shifted by one position so youcan see it here in the output we havethe inputs and then the targets areshifted by one position so if I show itlike this this is how it would look likebut it's a bit hard to read so Iinserted this white space so you can seewhere um the overlaps are here so youcan see thosethree tokens overlap and this would bethe next token and this would be theprevious token that is not um presentedin the in the targets so if the llmreceives this token it's supposed topredict this token if the llm receivesthose tokens it's supposed to predictthis token if the llm receives thosetokens it should predict this one and ifit receives those it should predict thisone and so forth so that's the wholeidea here and maybe to make this a bitmore clear what have just shown youvisually so here's some other codeexample which is exactly doing what Ijust showed you where it's showing thenext token and since this might be a bitabstract with these numbers we couldalso just use our tokenizer um forexample let me copy this so it's easierthan just retyping it so we can use ourtokenizer to decode the token IDs backinto yeah text and do this for both thetargets and the input context and sothis is the same oops I made a mistake Iprobably forgot to close parenthesis soyou can see what it would look like sowe always predicting the next word inthe sentence and so to make this reallyefficient we are going to use pytorchwhere pytorch is a very popular deeplearning framework it's probably themost Wily used deep learning frameworktoday um and so we're going to to importthis and use the data Luder and data setclasses from there because theyimplemented this very efficiently andwe're going to reuse these parts becausethey're not really part of the llm weare still talking about setting up thedata set and it would be a bit you knowtedious to reinvent the wheel and writeour own data lers and so forth and soyeah for that we are going to usepytorch and so pytorch is um imported astorch even though the library is calledpytorch it's just a convention we haveto remember um and if you followed thevideo uh in in the first chapter Ishowed you some setup tutorial if youfollowed this and installed therequirements. text this should alreadywork in case it doesn't work it meansyou don't have py toch installed and soone way to install it would be UV pipinstall or pip install depending on howyour system is set up and just typingthis command and it should automaticallyget a good version for your computerhowever if you want to be more specificfor example you can go to py do.org andon this website they have like thisinstaller menu that has differentcommands for different computers so forexample I'm running this on a Mac umwith Pip and then it would yeah give methat um this command if you are on Linuxby default um here this would be the CPUcode it would be a bit different if youhave aGPU by default if you type a pipinstalled torch it would come with Cuda12.4 libraries so it would support theGPU with Cuda 12.4 but if you want aspecific version you can also change itand there are different URLs and by thetime you are watching it there might bea whole new set of um Cuda versions sothis is really specific um in terms ofWhat GPU drivers you support but youknow by default um you can also just dopip install torch and it shouldtechnically work now back to this onehere I'm also just double checking whichversion I'm usinghere so here I'm using version2.6 and I should say when I startedwriting this book it was 2.0 becausethat was the most recent version twoyears ago when I started writing thebook um but I tested the book on allsubsequent subsequent versions ofpytorch and I couldn't see anydifference in any of the code on theGitHub repository I also have automatedtests so each time a new P version comesout I'm just double- checking it stillworks and so far I haven't seen any uhdifference in pyo that would make any ofthe code um different so in this case umyou can feel free to also use olderversions of pytorch for example 2.0 2.12.2 3 4 and five they should workequally fine however if you noticeanything that doesn't work forexample the results look different youcould just try also to install thisspecific version of pytorch by doing pipinstall torch and then2.6.0 and so this would then installthis specific version of pytorch I'musing right now at the moment okay sothis was a little detour maybe one morething I wanted to mention is um ifyou're new to pytorch it's essentially adeep learning framework uh it's veryrelatively comprehensive it has a lot ofthings but we are only going to use asubset here in this book um it'sessentially more of a linear algebralibrary with a few deep learningconvenience functions there are wholecourses and books about pytorch if youwant to learn about it U but if yeah ifyou want to learn about it efficiently Ialso have appendix a here so I've beenusing pytorch I would say since 2018 forseven years now and I've taught many pyoworkshops and um yeah I wrote anotherbook about pytor and so forth so uh whatI want to say here is over the years Iused pyro quite a lot and I was thinkingreally hard what are actually the partsyou need to know for this book so andbased on that I compiled this appendix awhich is essentially a book chapter of40 pages approximately that just go overthe essentials that you need for yeahbasic llm training and development wherethis should get you up to speedrelatively quickly of course if you'recurious I recommend um yeah consideringa whole course or book on pytorch atsome point but if you just want to getup to speed with pytorch for thisparticular um yeah book try to readappendix a and maybe it's alreadysufficient it might be a steeperlearning curve because I'm trying tokeep things efficient and compact butmaybe this is all you need at that timeto really get up to speed so you don'tum let's say go on a detour for a fewmonths to try to learn pytorch and thenyeah um yeah then you get maybefrustrated or get bored so I wouldsuggest maybe trying appendix a andmaybe it's already sufficient to get youup to speed here now back to the dataloading so we talked about pytorch andimporting pytorch let's actually go backto section 2.6 data sampling with aslidingwindow so we talked about the targetword predicting the next word and so nowwe are going to prepare the data setaccordingly so we are going to take theinput data and we are creating thesechunks and I mentioned before we'reusing a chunk size of four or contextsize of four for visualization purposesso it nicely fits into this figure andso forth but yeah in reality these areusually larger like 1,24 in the case ofthe gpt2 model or newer models use 8,16,000 uh tokens in each row but itwould be hard of course to visualizehere and so as you can see the one boxum here the left box that's the input Xthat we are creating and then the thetargets are shifted by one position asyou can see here it's the same idea umwhat I showed you earlier we're justcreating inputs and targetssimultaneously from this text so that wecan feed both to the llm later when weare training thellm so um let me actually do this incode and for this we are going to use apytorch data set class and so the reasonwhy we're using a pyro data set class isit will allow us to use the py dataorders um later and they are essentiallya very nice and efficient way to createbches and to shuffle the data and to usemultiple uh background processes toprocess the data faster it has a lot ofconvenience um added to it so how a dataset looks like is um we have anConstructor here we are setting them toempty lists then we are going totokenize the text so text is the inputtext we're going to pass and so we getthe token IDs and here this is the partwhere we creating these so-called chunksso these chunks are uh these two chunksbasically and instead of I should goback maybe one more time instead of justcreating one chunk for the inputs andtargets we're doing this for the wholedata set so we are storing these chunkshere um we're adding them here for theinputs and for the targets so we areappending them to these list we creatinglists we created earlier and it's thesame concept as before we are yeah umsliding over the input data set and thenthe targets are shifted here by oneposition so what we are going to do iswe're taking this left box and we aregoing to slide it over the input so youcan see um the first row is in the heartoff and then when we slide it over thenext one is the city stood the and thenold library and so forth so it's yeahsliding it over by oneposition and um it's if you have a verylarge data set it's maybe not the way todo it because you can't store 15trillion tokens in in memory so thiswould run out of memory but if you havejust a few books a few gigabytes ormegabytes of data I mean if you have Idon't know thousand books or somethinglike this this would conveniently fit soyou don't have to worry about it andover optimize at this point there are afew more advanced um ways and tools ifyou are really training llms ontrillions of tokens but for youreducational purposes we are keepingthings relatively simple and readablebecause this will this is the uh basicapproach and uh yeah so to understandalso how things work under the hood nowwe have this data set now and Imentioned the data laer that we want touse so let me instead of typing all thecode just insert itbelow so the data laer um is createdhere and I have a function I'm definingthat returns this data Lo and so here weare defining the the text you can set abatch size um a max length of eachcontext length so before we set four isour length here um we can also this isjust a default parameter we canoverwrite it later um we set the strideit's by how many positions we are movingthe Box we will get back to that in afew moments whether we want to shufflethe data set whether we want to drop thelast batch or not and this is actuallyan interesting one um this is really toavoid loss spikes so for example if wehave a data set let's say exampleone example two and so forth if we havea data set like that let me just do itlikethis and suppose we have a batch size oftwo what the data Lo will do is it willdivide the data set into batch sizes oftwo but um if the data set size is notdivisible by the batch size you oftenend up with very small batches at theend the last batch because it's theremainder and this often results in lossspikes like unstable training so it'sactually a good idea to always removethe last batch if you do multiple EPOtraining you also don't have to worryabout missing certain data points byremoving them but also in generalusually data sets are so large that youknow if you don't use the last batchdoesn't really matter so it's usually agood idea to have batches of the samesize so you don't have theseinstabilities during training this iswhy we are why I'm recommending droppingthe last batch um always and then numworkers is for using multiple backgroundprocesses I have it here set to zerobecause I'm executing this in a notebookand in some cases this can be yeah mightnot work because uh notebooks are a bitmore restricted than python scriptin terms of spawning multiplesubprocesses or if you're using WindowsI think Windows generally has someproblems with multiple backgroundprocesses in Python so this is just likea safe choice but yeah you can try alsoto set it to a larger number for examplebut yeah for for these purposes in anotebook this should be fine to set ittozero and um yeah this was veryconceptual few more things here we areinitializing the tokenizer this is thetick token token token we talked aboutearlier um then we are creating the dataset that is the data set we have up hereand we are creating it with um yeah textand text will be our training data setand here is where we are creating thedata ler using the settings I definedearlier and we are returning it so allvery very um I would say conceptual solet's actually see it in action and forthat we are going to use the verdictagain which is our short story storythat we downloaded earlier the shortstory by EdithWharton um raw text so this is how itlooks like we have seen it earlier andnow we are going to pass it to our dataset and data loer so let me just uhbecause otherwise it will end up a verylong video let me just copy and pastethe code here we are going to start witha batch size of one and a context Llength of one and a stride of one and sothe batch size is how many samples arein each batch essentially the max lengthis our context length it's um how longthe context here is like the size of theBox how many um yeah tokens are in thereand then the stride is by how many umplaces we are moving it so forSimplicity I will show you stride of oneand then we will change it so if this isunclear wait for a few moments we willwe will get to it and whether we umShuffle the data or not and then we areiterating here just for demonstrationpurposes over the data set manually andlater I will will show you a differentway to do that so here we are creatingan iteration object an iter object andthen we are taking the next um samplefrom our data set so when we areexecuting this we can see the the nextbatch here so this is the first input itonly has um yeah a size of one and thisis then the the targets so you can seethe targets are shifted by one positionso and then let's draw anotherbatch now what you can see is that thetargets here become those inputs so thethe first one is always the inputs thesecond one is always the targets becausethat's how we set up our data set so thefirst is the inputs the second is thetargets now you can see um this issomething where it's maybe not idealbecause when we have this input and thenthe second uh iteration is this inputthere is an overlap so these tokens arethe same and if you do it like this thellm sees certain tokens many many timesand then it's kind of leading tooverfitting so what's actually better isto increase the stride by four so if wedo that you can see each of the tokensare unique so you can see they are notoverlapping anymore more and so what itdoes is it's moving this box by stri offour by four positions so 1 2 3 fourmoving it by four positions so we are atthe city stood the and you can see thisis what's shown here in the second rowthe reason why I showed you just thestride of one was so you can see thatthis actually works and it's not missingany data because you can see um this onepicks up right here if we have a strideof one I thought this might be moreintuitive but if it is confusing don'tworry about it just uh go to the strideof four and then you have exactly whatyou can see here in the figure now thisis um for a batch size of one when weare training deep do networks it'susually efficient to use a larger batchsize so what we can do is um we can usethe batch size of eight so I'm justshowing you same thing here instead ofbch size of one I have a bch size ofeight max length of four and A stri fourand then yeah some labeling so it's abit more clear and yeah you can see nowwe have the the inputs here and thetargets and this is similar uh to whatwe are seeing here where we havemultiple rows where each row is onetraining example and so we have a batchsize of 1 2 3 4 5 6 7 eight and then thecorresponding targets yeah and this isreally um how we implement this data laand then um later we can also iterateover it more efficiently so what this isa topic for later I just wanted to setup this data loer so that we have oneway later on to load the data into thellmefficiently we talked a lot aboutcreating token IDs but now let's take itto the next step and talk about creatingtoken embeddings so if I go back to anearlier figure from section 2.2 welooked at this figure where I showed youthat input text gets converted intotokenized text and the tokenized textgets converted to token IDs now the nextstep is creating these token embeddingsfrom the token IDs so essentially takingan ID an integer value and converting itto an embedding vector and so thisembedding Vector contains real numberslike a floating Point numbers that canthen beoptimized so um yeah to show you how itworkswe are going to use um some token IDs asan example justlike um as a pyd tensor so that it is abit more easy to see what's going on solet's do torch tensor and then um insertthose and I should say these arerelatively large numbers so let'sactually use yeah some smaller ones sowe can have nicer visualization so theseare just like uh some words or tokens inour input so we have this input tensornow and then the next step would becreating a so-called embedding layerusually this embedding layer is um partof the llmitself we will revisit this many timeslater in this book but this would be astandalone um embedding layer and thereusually something called the vocabularysize so this is the size of thetokenizer the vocabularytokenizer and then the output Dimensionso this is the vector embedding size sothe vocabulary size is in our case umfor the tick token tokenizer it's 5,00uh 50,257 sorry that is probably not thecorrect number uh let me double checkactuallytokenizer one way if you forget like mewhat the the name of the vocabulary isyou can use in Python this uh deerand it should actually show you so nvoap so if I go uphere Type n WAP and then we do thelength you can see oops uhit's not a list it's just an integer youcan see it has 50,2 57 unique tokens sothis would be one argument for ourembedding layer so it supports all thesedifferent tokens and the other one wouldbe the output Dimension so this would bethe desired size for these vectors wewill actually use the original GPT umembedding size later on for Simplicitylet's actually use simpler and smallervalues so it's then easier to visualizeso let me use V cap size six and outputDimension um let me just copyit uh let's do three because uh three isalso so what we are seeing here likethree little boxes per Vector so we havea very small um example that ishopefully easy to see and thenum one more thing is I'm adding a randomseathere why am I doing that uh it's becausethis is a neuron Network layer withrandom weights and so the random weightsare different each time you instantiatea new layer and so I just want to makesure you get the same results that I doso I'm initializing this right randomseat and by the way if random seeds aresomething um you are not familiar withlet me know I also have a nice video onrandom seeds I can share with you butyeah so this is um now this embeddinglayer and the embeddinglayer has a weightMatrix um wait and so this is somethingthat is later optimized during theneuron Network training like the llmtraining and um yeah they are calledparameters or parameters and these arethings that are going to be adjusted andby the way if you are familiar alreadywith linear layers or Matrixmultiplications and everything um anembedding layer is just um animplementation detail to make a certainlookup easier or more efficient this ismore advanced you don't have to readthis if you're not familiar with thisbut for those who are curious I doactually have some bonus material hereum where it's about understanding thedifferencebetween one moment uh the differencebetween embedding layers and linearlayers so um if you're curious how thisrelates to Matrix multiplications andlinear layers it's like a short notebookfor those who are curious but if you arenot familiar with it don't worry aboutit I will try to explain it in a morelet's say intuitive sense so let me showyou something so if I do embedlayer and I call it on let's say thisinput here let's just pick one of thesenumbers um torch tensor let's make asmaller tensor anduse let's say the the three fromhere let's see what happens so you cansee it's pulling out a vector here sothere a um tensor of three differentnumbers and if you um have a sharp I youmight notice it's actually this row herefrom this embedding Matrix so if we areuh typing three here py uh python is azero index so it's 0 1 2 3 it's givingus this Vector from this Matrix andsimilarly if I go here and I do let'suse the first one and I do this one youcan see this one is the first um sorrynot quite the Third row sorry 0 1 2because that's the number two and thisis yeah the index two corresponding tothis row and similarly if I now use thewhole inputIDs I'm pulling out values from thisMatrixso um I probably should show them herefor reference so the first one that'sthe two it corresponds to 0 one two tothis one here the second one correspondsto this three so this would be this rowhere thefive this one 0 1 2 3 four five and soforth so we are essentially when we arecalling this embedding layer on theinput IDs we are pulling out vectorsfrom this Matrix and this Matrixtechnically has the size of our umtokenizer so I can just show you thisexample Le tokenizer n wab right so inthis case this might be a very large oneshould I print it maybe let try and seewhat happens it's actually dotting itout but yeah this would be then 50,2 57um rows essentially and yeah this isessentially the embedding layer that isused to convert the token IDs here intothese vectors and this is usually partof the llm and yeah these numbers Imentioned before they are random so if Iactually comment this out you might seeum these numbers change because theweight Matrix uh yeah the weight Matrixchanges and so usually what we want todo is we want to start with small randomnumbers it doesn't really matter whatthese numbers are but later on duringthe training these numbers get optimizeduh in a sense that the llm is optimizedend to end to produce yeah the desiredtext the next word in the training textand so don't worry about why thesenumbers are random yet because they'renot trained yet they're just startingvalues we're going to train them laterin chapter 5 but now yeah you have ahopefully good understanding what anembedding is so an embedding is a vectorfrom this embedding Matrix that getslatertrained so now that we understand wordembeddings let's take it a step furtherand add positional information I willtell you in a few moments what thepositional information means but yeahjust to recap we had input text wetokenized the input text created tokenIDs and then the previous section wasfocused on creating these so-calledtoken embeddings and we only had a verysmall toy example and then just to usethe actual sizes um for our tokenizer wementioned our tokenizer has 50,2 57words and so we pass this vocab size toour embedding layer to to create it inin this size and now we also scale thisa little bit up cuz before you know wehad like a output dimension of one twothree tokens which is just for a toyexample now we are increasing it to 256which is what we are going to use totrain the llm later it's a morerealistic size um gpd2 was originallytrained with u output dimension of Ithink1,24 I'm not quite sure I would have todouble check um but yeah this would besomething that works also well on asmall computer like a laptop for exampleyou can always you know try largervalues if your computer supports it butI think this should be supported on mosthardware for yeah demonstration purposesbut also to make it a bit more realisticthan having an embedding size of justthree dimensions so we have now 256Dimensions so if we do that um so wehave our embedding Matrix that has50,000 uh rows and about 200 um columnsso that's our weight Matrix similar towhat we did before now taking it a stepfurther we had also the data ler howdoes it now come together with theseembeddings previously we created theseum uh token IDs so if I just show youthis it's the same that we've coveredearlier so we have um a data loer thatcreates batches of size eight with a maxlength of four so four token IDs um pertraining example and then we we haveeightexamples and so now how does it actuallyrelate to the Token embedding that wehave seen in the previous section sowhat we do now is we actually convertthese token IDs into these embeddingvectors so if I do that um what I showedyou earlier is if I take this and Itype input IDs it would create thesample uh vectors for the simple inputIDs that we talked about earlier but nowwe are going to use actually the databadge here so this would be the databadge that we convert and let's maybecall this tokenembeddings and let's just see whathappens and let's actually look at thesize of the outputhere so you can see now we have stillthe batch size of eight so it's now athree-dimensional tensor and the numberscorrespond to the batch size so we have1 2 3 4 5 6 seven eight rows eightexamples each example has four tokenslike one two whoops one two three fourbut now it's not just the token ID eachone of those is actually a 256dimensional Vector so here each token IDgets converted to a three-dimensionalVector now it's a 256 dimensional Vectorit's hard to visualize vectors in 256Dimensions but uh we can actually uhmaybe print one of those out so if wetake a look at the first one first batchthe first token this should give us umyeah the the vector the 256 dimensionalVector corresponding to the first tokenin the first batch basically this oneokay so now we converted token IDsinto uh token embeddings the next umstep is adding positional information solet me see if I have a nice figure forthis so previously what we've done is wepulled out these embedding vectors fromthe weight Matrix for each input exampleand so I previously chose an input thatwas yeah I would say um relatively nicefor explanation purposes where eachoutput row looked different so we hadfox jumps over doc these are token IDscorresponding to an example input likethis and then we pulled out these umembedding vectors corresponding to theToken IDs from this weight Matrix nowwhat happens if we have the same tokenID for example fox jumps over Fox sohere this um token index 2 is the thirdrow in the weight Matrix and so we getas the result this row here but since inthe last token ID we have the same yeahthe same word the same token ID it hasthe same Vector I mean it makes senseright we are looking up the positionhere we pulling out the the vector andcreate the output like that now this isfine it would work but we could actuallyadd some more information to it so toprovide more information to the llmdepending on where this word is locatedin the input we can create so-calledpositional embedding information thereare different ways you can do that um sogpt2 used a very simple way it used thesecond embedding layer there arenowadays other ways for example usingrotational positional embeddings ifyou're curious about those in the bonusmaterial um on my GitHub repository Iimplemented also llama 3.2 from scratchit's more advanced and more complicatedand it's not as nicely described as inthe book but they use something calledrope embeddings that I also implementedhere but gbt is yeah fortunately muchsimpler in this case with the embeddingsit's using simply a second embeddinglayer so let me actually um show you howthat would work so so um we are usingthe same embedding class we used beforebut now we have an token embedding layerin addition we have now a positionembedding layer where we add positionalinformation so the goal will be maybe Ihave a figure yeah I do have a figure onthat if we have input embeddings likethis the goal is to have a second set ofvalues that we add here so um positionalembeddings that we add so we have sorrytoken embeddings so to step through thisfigure most slowly I chose the sametoken embeddings for each um examplehere just to keep things simple so um ifwe have just input that consists of thesame word like um fox fox fox fox forexample the token embeddings would bethe same in all the cases but then weadd positional information on top ofthat so that the actual input embeddingslook different depending on on whereit's located now I'm using these um verynice numbers where it's easy to readthose numbers in reality these are allstarting with random numbers so itdoesn't look quite as nice but thesenumbers the position embedding numbersthey also get optimized automaticallylater during the llm training so theyare random numbers to start with andlater they will get optimized similar tothe Tokenembeddings so here um yeah I'm creatinga second uh position embedding layeressentially and then um what I'm doingso here is the max length um so the maxlength is is yeah how many inputs aresupported in this case we only have fourright so we had a max length of four 1 23 four it can only support four inputtokens in practice uh this is a muchbigger layer so for Simplicity we justkeep itsmall um for visualization purposes andjust Simplicity later when we train thellm we will actually use much largervalues and then what we can do is we canuse uh torch a[Music]range so a rangeis creating uh in sequential ordervalues here 0 1 2 andthree so 0 1 2 3 it's just a tensor it'sa placeholder tensor for the positionsposition zero position one two and threeand we have this positionembedding Matrix and it's a similarMatrix to what we hadbefore oopsand then when we arecalling this with our arange what it will do it will just pullout the values like the in sequentialorder here so you can see it here soit's just um getting me the values hereand that is all that is really happeningin in this positional embedding layerand then what we do is we add them ontop of each other so maybe to before Iexecute this um this one has shape 8 for256 and this one should have the sameshape so we can then add them to eachother I have not executedthis right here so um I'm sorry this isactually only for 256 because uh howpytorch work I mean for efficiency wedon't need to duplicate this over thebatch Dimension because this one will beadded to each batch um separately sowhat happens is essentially um that whenwe have something like this um the inputembedding has the same size as the sorrythe token embedding has the sameDimension as the position embeddings andthen the input embedding will also havethe same Dimension so here um there'sthe concept is called broadcasting inpytorchso um Let Me Maybe show it to you likethis so when we dothis we can add these values together sothis is the first example in thetraining set and then our positionembedding vector and this would beessentially similar to doing this exceptnow we are doing it for all the examplesin the batch but the first the firstexample maybe we can see it here umshould be similar so you can see thisone is similar to this one I mean thisis like a rounding thing not a roundingthing a different uh printrepresentation but this row is the sameas this row essentially and so forth sothis is um just generalizing it over thewhole batchDimension oops let me just clean this upa bit and then when I add them togetherlike before the shape should be the sameas before so we are just adding thisadditional yeah position information toit and so with that we nowhave covered the whole input pipeline sowe have input text that we break downinto tokenized text then we convert itinto token IDs then we create these umtoken embeddings with our tokenembedding layer we create the positionembeddings add them together to get theinput embeddings and so this is all weneed to know for now for the datapreparation step next let's actuallytackle the inside of the GPT likedecoder only Transformer so what isactually inside the model one more thingI should say the embedding layers aretechnically part of the GPT model as wewill see later I just draw them outsidehere because it's a bit easier to talkabout it since we already talked aboutthe data laer but yeah in the next inthe next um chapter we will talk moreabout the attention mechanism that isinside of a GPT model so I hope this wasuseful and I hope I will see you in thenext video\n"
     ]
    }
   ],
   "source": [
    "result = \"\"\n",
    "for i in transcript:\n",
    "    result += \"\" + i.text\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
